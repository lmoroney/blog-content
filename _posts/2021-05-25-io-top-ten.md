---
title: Top 10 AI and ML Announcements for Developers at Google IO
layout: article
tags: google talks
---

There were so many new announcements for AI and ML developers at Google IO this year. In this video, I'll share my top 10!. I'm excited with how much the AI and ML ecosystems have grown, and  hope these updates will help you solve any future challenges you face. 


Check it out here:
<div>{%- include extensions/youtube.html id='3k37fz9p6_k' -%}</div>

Sunspring was generated by a model called Jetson, and, as you'll see in the video, it was created using a technique called LSTM -- which stands for 'Long Short Term Memory,' and it's a way that a machine learning model can understand context in a sequence, and predict what should come next in that sequence.

This is good for writing text -- given a series of words -- it can predict the next likely word in the corpus. So, for example, if an LSTM was trained on Star Wars, and given the sequence "May the force," it would quite likely predict the next word to be "be". They're a lot of fun, but as you think about it, the reason why they become gibberish quickly is that they really only predict one word at a time.

So, if you were to start a sequence with 

"May the force", it would likely predict "be". I say 'likely,' because it will look at every possible word used in 'Star Wars,' and then calculate the probability of each one being the next word. Say 'be' has an 80% probability. Now you have "May the force be," and you want to get the *next* word. It will do the same -- and calculate the probability of every word used in Star Wars, and the most likely one is 'with,' with, say a 70% probability. Then the next word, of course will be 'you.'

But then what? 'be with you,' would be generated, but what would come next? This is a phrase that happens a lot in that corpus -- so "May the force be with you," is relatively easy to generate. The next word will be calculated based on the probabilities of every word in the corpus...so it's unlikely it will have a high probability like 'be,' 'with,' and 'you' had. And the next one would be even lower and so on. Thus, very quickly, low probability words would be spat out. And very quickly, the script would become gibberish.

For more detail -- check out my tutorial series [here](https://www.youtube.com/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S), where I teach Natural Language Processing (NLP), and show how to create lyrics for traditional Irish songs with an LSTM.

##But Stargate neeeded something better


