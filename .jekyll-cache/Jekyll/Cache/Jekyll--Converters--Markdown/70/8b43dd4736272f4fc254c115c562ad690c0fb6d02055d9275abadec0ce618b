I"  <p>Recently, Iâ€™ve been working on a very exciting project â€“ instigated by an idea from Brad Wright, creator of the â€˜Stargateâ€™ TV shows. Given how many attempts there have been for Machine Learning models to consume scripts and to attempt to come up with stories of their own, why couldnâ€™t we do something similar for Stargate â€“ where there are over 300 episodes of TV, several TV movies, and a lot more in the corpus of work.</p>

<p>The goal: Create an AI that writes a script for Stargate, and then get a number of the actors from the original show(s) together to do a table read.</p>

<p>If youâ€™ve watched any AI-generated media, youâ€™ll immediately see that they rapidly deteriorate into gibberish. The famous â€˜Sunspringâ€™ movie is a great example. Check it out here:</p>

<div><div class="extensions extensions--video">
  <iframe src="https://www.youtube.com/embed/LY7x2Ihqjmc?rel=0&amp;showinfo=0" frameborder="0" scrolling="no" allowfullscreen=""></iframe>
</div>
</div>

<p>Sunspring was generated by a model called Jetson, and, as youâ€™ll see in the video, it was created using a technique called LSTM â€“ which stands for â€˜Long Short Term Memory,â€™ and itâ€™s a way that a machine learning model can understand context in a sequence, and predict what should come next in that sequence.</p>

<p>This is good for writing text â€“ given a series of words â€“ it can predict the next likely word in the corpus. So, for example, if an LSTM was trained on Star Wars, and given the sequence â€œMay the force,â€ it would quite likely predict the next word to be â€œbeâ€. Theyâ€™re a lot of fun, but as you think about it, the reason why they become gibberish quickly is that they really only predict one word at a time.</p>

<p>So, if you were to start a sequence with</p>

<p>â€œMay the forceâ€, it would likely predict â€œbeâ€. I say â€˜likely,â€™ because it will look at every possible word used in â€˜Star Wars,â€™ and then calculate the probability of each one being the next word. Say â€˜beâ€™ has an 80% probability. Now you have â€œMay the force be,â€ and you want to get the <em>next</em> word. It will do the same â€“ and calculate the probability of every word used in Star Wars, and the most likely one is â€˜with,â€™ with, say a 70% probability. Then the next word, of course will be â€˜you.â€™</p>

<p>But then what? â€˜be with you,â€™ would be generated, but what would come next? This is a phrase that happens a lot in that corpus â€“ so â€œMay the force be with you,â€ is relatively easy to generate. The next word will be calculated based on the probabilities of every word in the corpusâ€¦so itâ€™s unlikely it will have a high probability like â€˜be,â€™ â€˜with,â€™ and â€˜youâ€™ had. And the next one would be even lower and so on. Thus, very quickly, low probability words would be spat out. And very quickly, the script would become gibberish.</p>

<p>For more detail â€“ check out my tutorial series <a href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S">here</a>, where I teach Natural Language Processing (NLP), and show how to create lyrics for traditional Irish songs with an LSTM.</p>

<h3 id="but-stargate-neeeded-something-better">But Stargate neeeded something better</h3>
<p>Realizing that this LSTM-based approach would get us into gibberish territory really quickly <em>and</em> knowing that we would be working with a limited set of characters who werenâ€™t all in all 300+ episodes together, I wanted to approach this a little differently.</p>

<p>So the first step was to understand the anatomy of a script.</p>

<p>Typically a script will look something like this:</p>

<p><img src="/assets/scriptsnippet.png" alt="Example script page with annotations for Scene Heading, Action and Dialog" /></p>

<p>Note the architecture of a script. Thereâ€™s a scene heading that sets the scene â€“ in this case we can see a hospital in LA in 1941. Thereâ€™s an <em>action description</em> that tells you whatâ€™s going on, and then thereâ€™s dialog â€“ what each character says.</p>

<p>So, what, I thought â€“ if I build models for <em>each</em> of these elements. Given a scene heading (for example INT. ATLANTIS GATEROOM,) what type of <em>action</em> description would follow? It might come up with something like â€œThe Stargate is active.â€. Then I can choose to continue with the action, and it may come up with â€œAn alarm goes off, incoming wormhole,â€ and the story begins to take shape.</p>

<p>In the script, actors usually have some kind of response to the action, so, at that point, what if I trained a model for each character, and how they respond to action? Perhaps the character, in response to the above text might say â€œUnscheduled offworld activation,â€.</p>

<p>And thenâ€¦what if I also created a model for each character that predicts how they might respond to dialog, as well as action.</p>

<p>â€¦and the idea of a cluster of models was born.</p>

<p>So, for the Stargate AI project, thatâ€™s what we have.</p>

<ul>
  <li>A model to predict action from a scene heading</li>
  <li>A model to predict more action from existing action</li>
  <li>A model to prediction action in response to dialog</li>
</ul>

<p>And then, for each character:</p>
<ul>
  <li>A model to predict dialog from action</li>
  <li>A model to predict dialog from dialog</li>
  <li>A model to continue dialog</li>
</ul>

<p>Given that Stargate AI has three confirmed characters (McKay, Jackson, Carter,) this involved the creation and training of 12 models, each with between 1 and 2 million parameters.</p>

<p>â€¦and thereâ€™s more on the way!</p>

<p>And to continue to push the state of the art with this, instead of using LSTM, I opted instead to use a <em>Transformer</em> architecture. Iâ€™ll cover more of that in future articles, but the idea of that architecture is to predict sequences as opposed to individual words.</p>

<p>So, with a transformer, you would feed it a sequence like â€œMay the force be with you,â€ and it might come back with â€œand also with you,â€ which is more realistic for a script-writing AI like weâ€™re trying to build for Stargate.</p>

<h3 id="the-data">The Data</h3>
<p>Iâ€™ll go into more detail on the Transformers in a future article, but, if youâ€™re into ML and AI, you know the hardest part is getting the data. So, in the video below I allude a little to how we did it.</p>

<p>I created a new XML-variant that I call SML (Script Markup Language) that allows me to pull dialog out of a scene (i.e. if Iâ€™m predicting dialog from action, it should be action in the same scene, not the previous one!) and from there train a model quickly an easily.</p>

<p>Hereâ€™s a sample of SML:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;dialog character='CARTER'&gt;
   &lt;spoken s='4' i='4'&gt;Mining what?&lt;/spoken&gt;
&lt;/dialog&gt;
&lt;dialog character='MCKAY'&gt;
   &lt;spoken s='4' i='5'&gt;Havent a clue. Whatever it was, they cleaned out the deposits and left.&lt;/spoken&gt;
&lt;/dialog&gt;
&lt;dialog character='CARTER'&gt;
   &lt;spoken s='4' i='6'&gt;Soâ€¦?&lt;/spoken&gt;
&lt;/dialog&gt;
&lt;dialog character='SHEPPARD'&gt;
   &lt;spoken s='4' i='7'&gt;So, they think were like the Genii, and they want their cut.&lt;/spoken&gt;
&lt;/dialog&gt;
&lt;dialog character='CARTER'&gt;
   &lt;spoken s='4' i='8'&gt;Ah.&lt;/spoken&gt;
&lt;/dialog&gt;
&lt;dialog character='KELLER'&gt;
   &lt;spoken s='4' i='9'&gt;Theyre willing to move, but they have a list of demands a mile and a half long.&lt;/spoken&gt;
&lt;/dialog&gt;
&lt;dialog character='SHEPPARD'&gt;
   &lt;spoken s='4' i='10'&gt;And negotiating with alien settlements is not exactly why I joined the Air Force.&lt;/spoken&gt;
&lt;/dialog&gt;
&lt;action s='4'&gt;[Carter half-quirks a wry smile.]&lt;/action&gt;

</code></pre></div></div>

<p>You can learn more in this video, where Brad, David Hewlett and I announce and discuss the project:</p>

<div><div class="extensions extensions--video">
  <iframe src="https://www.youtube.com/embed/2yIEUfDUiHg?rel=0&amp;showinfo=0" frameborder="0" scrolling="no" allowfullscreen=""></iframe>
</div>
</div>

<p>In the next article, weâ€™ll talk Transformers, and Iâ€™ll show some examples of how models came up with dialog for the characters!</p>

:ET