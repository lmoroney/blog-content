I"§
<p>There were so many new announcements for AI and ML developers at Google IO this year. In this video, Iâ€™ll share my top 10!. Iâ€™m excited with how much the AI and ML ecosystems have grown, and  hope these updates will help you solve any future challenges you face.</p>

<p>Check it out here:</p>
<div><div class="extensions extensions--video">
  <iframe src="https://www.youtube.com/embed/3k37fz9p6_k?rel=0&amp;showinfo=0" frameborder="0" scrolling="no" allowfullscreen=""></iframe>
</div>
</div>

<p>Sunspring was generated by a model called Jetson, and, as youâ€™ll see in the video, it was created using a technique called LSTM â€“ which stands for â€˜Long Short Term Memory,â€™ and itâ€™s a way that a machine learning model can understand context in a sequence, and predict what should come next in that sequence.</p>

<p>This is good for writing text â€“ given a series of words â€“ it can predict the next likely word in the corpus. So, for example, if an LSTM was trained on Star Wars, and given the sequence â€œMay the force,â€ it would quite likely predict the next word to be â€œbeâ€. Theyâ€™re a lot of fun, but as you think about it, the reason why they become gibberish quickly is that they really only predict one word at a time.</p>

<p>So, if you were to start a sequence with</p>

<p>â€œMay the forceâ€, it would likely predict â€œbeâ€. I say â€˜likely,â€™ because it will look at every possible word used in â€˜Star Wars,â€™ and then calculate the probability of each one being the next word. Say â€˜beâ€™ has an 80% probability. Now you have â€œMay the force be,â€ and you want to get the <em>next</em> word. It will do the same â€“ and calculate the probability of every word used in Star Wars, and the most likely one is â€˜with,â€™ with, say a 70% probability. Then the next word, of course will be â€˜you.â€™</p>

<p>But then what? â€˜be with you,â€™ would be generated, but what would come next? This is a phrase that happens a lot in that corpus â€“ so â€œMay the force be with you,â€ is relatively easy to generate. The next word will be calculated based on the probabilities of every word in the corpusâ€¦so itâ€™s unlikely it will have a high probability like â€˜be,â€™ â€˜with,â€™ and â€˜youâ€™ had. And the next one would be even lower and so on. Thus, very quickly, low probability words would be spat out. And very quickly, the script would become gibberish.</p>

<p>For more detail â€“ check out my tutorial series <a href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S">here</a>, where I teach Natural Language Processing (NLP), and show how to create lyrics for traditional Irish songs with an LSTM.</p>

<p>##But Stargate neeeded something better</p>

:ET