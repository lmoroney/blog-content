<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-06-18T09:18:09-07:00</updated><id>/feed.xml</id><title type="html">Laurence Moroney - The AI Guy.</title><subtitle>This is the blog and general web site of Laurence Moroney, author, lecturer, teacher and AI lead at Google with Musings on AI, Quantum and other future tech
</subtitle><author><name>Laurence Moroney</name></author><entry><title type="html">Amazing ML work by Japanese children</title><link href="/2021/06/18/inspirational-ai.html" rel="alternate" type="text/html" title="Amazing ML work by Japanese children" /><published>2021-06-18T00:00:00-07:00</published><updated>2021-06-18T00:00:00-07:00</updated><id>/2021/06/18/inspirational-ai</id><content type="html" xml:base="/2021/06/18/inspirational-ai.html">&lt;p&gt;The song “The Greatest Love of all,” performed beautifully by Whitney Houston, begins with the lyrics:&lt;/p&gt;

&lt;p&gt;“I believe the children are our future, teach them well and let them lead the way…”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/japanese.jpg&quot; alt=&quot;Japanese Children&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s a beautiful sentiment, made much more so because it can be true. So I wanted to share the stories of three children in Japan who epitomize these lyrics and show us a potentially bright future made possible by ML.&lt;/p&gt;

&lt;p&gt;Tontoko, a 14-year old girl from Tottori, Mebumebu, an 11 years old boy from Gifu and Ririka, an 11 years old girl from Okinawa, are wonderful examples of this.&lt;/p&gt;

&lt;p&gt;In this video, they share their stories. A fabulous example is Ririka built an app using audio classification to alert drivers to slow down in the vicinity of endangered birds! You’ll also see how Tontoko used computer vision to connect books with music and how Mebumebu created a device to prevent excessive drinking by a beloved grandparent!&lt;/p&gt;

&lt;p&gt;I’m always amazed by the solutions children see to problems in the world around them. So I’m inspired to be a small part of solutions like these and encourage you, dear reader, to do the same!&lt;/p&gt;

&lt;p&gt;“A common misconception about machine learning is that it takes a genius to do it, but I think it’s actually easy and fun!” – Tontoko.&lt;/p&gt;

&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/ztIGjv3YZlE?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Laurence Moroney</name></author><category term="ai" /><summary type="html">The song “The Greatest Love of all,” performed beautifully by Whitney Houston, begins with the lyrics: “I believe the children are our future, teach them well and let them lead the way…” It’s a beautiful sentiment, made much more so because it can be true. So I wanted to share the stories of three children in Japan who epitomize these lyrics and show us a potentially bright future made possible by ML. Tontoko, a 14-year old girl from Tottori, Mebumebu, an 11 years old boy from Gifu and Ririka, an 11 years old girl from Okinawa, are wonderful examples of this. In this video, they share their stories. A fabulous example is Ririka built an app using audio classification to alert drivers to slow down in the vicinity of endangered birds! You’ll also see how Tontoko used computer vision to connect books with music and how Mebumebu created a device to prevent excessive drinking by a beloved grandparent! I’m always amazed by the solutions children see to problems in the world around them. So I’m inspired to be a small part of solutions like these and encourage you, dear reader, to do the same! “A common misconception about machine learning is that it takes a genius to do it, but I think it’s actually easy and fun!” – Tontoko.</summary></entry><entry><title type="html">My Sound to Emoji ML App for WWDC21Challenges</title><link href="/2021/06/09/WWDCChallenge.html" rel="alternate" type="text/html" title="My Sound to Emoji ML App for WWDC21Challenges" /><published>2021-06-09T00:00:00-07:00</published><updated>2021-06-09T00:00:00-07:00</updated><id>/2021/06/09/WWDCChallenge</id><content type="html" xml:base="/2021/06/09/WWDCChallenge.html">&lt;p&gt;Open sourcing my Sound-&amp;gt;Emoji demo app. I created this as part of the app challenges at WWDC21!&lt;/p&gt;

&lt;p&gt;First of all, you need to be an Apple Developer to do this, because it requires the Xcode 13 beta, and iOS 15 in order to work. It does work really well in the emulator!&lt;/p&gt;

&lt;p&gt;As a starting point, I used the code that’s available from &lt;a href=&quot;https://developer.apple.com/documentation/soundanalysis/classifying_live_audio_input_with_a_built-in_sound_classifier&quot;&gt;Apple Developers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This works really well as a starter – giving you an app with two views. The first allows you to pick which sounds you want to classify for. The second then has meters indicating the intensity of the chosen sound.&lt;/p&gt;

&lt;p&gt;So, for example, if I’m breathing – the second view will show something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/breathing.png&quot; alt=&quot;Breathing View&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following code within ContentView helps you pick which view to render. If you are in setup mode, it will use the SetupMonitoredSoundsView, otherwise it will be the DetectSoundsView:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;some&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;View&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;ZStack&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;showSetup&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;kt&quot;&gt;SetupMonitoredSoundsView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                  &lt;span class=&quot;nv&quot;&gt;querySoundOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;AppConfiguration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;listAllValidSoundIdentifiers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                  &lt;span class=&quot;nv&quot;&gt;selectedSounds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;appConfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitoredSounds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;nv&quot;&gt;doneAction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;showSetup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;appState&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;restartDetection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;kt&quot;&gt;DetectSoundsView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;nv&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;appConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;nv&quot;&gt;configureAction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;showSetup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For my emoji viewer, I just edited the original DetectSoundsView to make it much simpler. One suprising thing is that the view redraws &lt;em&gt;on every inference&lt;/em&gt; which seems excessive, but I didn’t change that (for now), which is why the emoji viewing video might look a little flickery.&lt;/p&gt;

&lt;p&gt;I edited DetectSoundsView just to draw a label with the emoji, instead of all the meters from the original with this code:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generateDetectionsGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;detections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;SoundIdentifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;DetectionState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Dictionary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;some&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;View&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ScrollView&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;kt&quot;&gt;ForEach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentConfidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
                    &lt;span class=&quot;kt&quot;&gt;Label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;systemImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;font&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This uses a dictionary to convert the label of the detected sound to an emoji. This is just a simple Dictionary&amp;lt;String, String&amp;gt; object that I create by manually assigning a label to an emoji. My code is &lt;a href=&quot;https://github.com/lmoroney/funcode/blob/master/ios15/classifysound/ClassifySound/Support/EmojiDictionaryHelper.swift&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;…and if you want a video of the app in action, you can see it on &lt;a href=&quot;https://twitter.com/lmoroney/status/1403002757674459142&quot;&gt;my twitter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The full code for the app, borrowing heavily from the apple ample is &lt;a href=&quot;https://github.com/lmoroney/funcode/tree/master/ios15/classifysound&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="ai" /><category term="coding" /><category term="personal" /><summary type="html">Open sourcing my Sound-&amp;gt;Emoji demo app. I created this as part of the app challenges at WWDC21! First of all, you need to be an Apple Developer to do this, because it requires the Xcode 13 beta, and iOS 15 in order to work. It does work really well in the emulator! As a starting point, I used the code that’s available from Apple Developers This works really well as a starter – giving you an app with two views. The first allows you to pick which sounds you want to classify for. The second then has meters indicating the intensity of the chosen sound. So, for example, if I’m breathing – the second view will show something like this: The following code within ContentView helps you pick which view to render. If you are in setup mode, it will use the SetupMonitoredSoundsView, otherwise it will be the DetectSoundsView: var body: some View { ZStack { if showSetup { SetupMonitoredSoundsView( querySoundOptions: { return try AppConfiguration.listAllValidSoundIdentifiers() }, selectedSounds: $appConfig.monitoredSounds, doneAction: { showSetup = false appState.restartDetection(config: appConfig) }) } else { DetectSoundsView(state: appState, config: $appConfig, configureAction: { showSetup = true }) } } } For my emoji viewer, I just edited the original DetectSoundsView to make it much simpler. One suprising thing is that the view redraws on every inference which seems excessive, but I didn’t change that (for now), which is why the emoji viewing video might look a little flickery. I edited DetectSoundsView just to draw a label with the emoji, instead of all the meters from the original with this code: static func generateDetectionsGrid(_ detections: [(SoundIdentifier, DetectionState)], dictionary: Dictionary&amp;lt;String,String&amp;gt;) -&amp;gt; some View { return ScrollView { ForEach(detections, id: \.0.labelName) { if($0.1.currentConfidence&amp;gt;0.3){ Label(dictionary[$0.0.labelName]!, systemImage: &quot;&quot;).font(.system(size:120)) } } } } This uses a dictionary to convert the label of the detected sound to an emoji. This is just a simple Dictionary&amp;lt;String, String&amp;gt; object that I create by manually assigning a label to an emoji. My code is here …and if you want a video of the app in action, you can see it on my twitter The full code for the app, borrowing heavily from the apple ample is here.</summary></entry><entry><title type="html">Widening Access to Applied ML with TinyML</title><link href="/2021/06/08/tinyml.html" rel="alternate" type="text/html" title="Widening Access to Applied ML with TinyML" /><published>2021-06-08T00:00:00-07:00</published><updated>2021-06-08T00:00:00-07:00</updated><id>/2021/06/08/tinyml</id><content type="html" xml:base="/2021/06/08/tinyml.html">&lt;p&gt;I’m delighted to announce that the &lt;a href=&quot;https://arxiv.org/pdf/2106.04008.pdf&quot;&gt;paper&lt;/a&gt;, I’ve worked on with folks from Harvard and Google, has been published online to arxiv! We discuss the strategies that we used in designing a curriculum to widen access to Applied ML, focusing on the rapid growth of TinyML and how a new teaching methodology can open doorways previously shut to the traditionally underrepresented.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/coursedesign.png&quot; alt=&quot;Course Design&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s critically important to do this type of teaching &lt;em&gt;at this time&lt;/em&gt; because with the rapid growth of ML, and TinyML in particular, I anticipate that we’ll soon end up with several prominent players in this space and that these players will likely be in the traditional high tech areas. By opening up easy access to teaching materials for &lt;em&gt;everyone&lt;/em&gt;, we hope to level the playing field.&lt;/p&gt;

&lt;p&gt;With that in mind, we designed the course to have the academic rigor and depth of a top-tier university like Harvard, with the breadth of understanding required to work in a major company like Google. Our goal was linking the two to bring students holistically through a journey of understanding ML, using it, and then deploying it to embedded systems and microcontrollers.&lt;/p&gt;

&lt;p&gt;One observation we made while working on this is an inverse relationship between the number of available ML educational resources and the number of systems in the field! We wanted to fix that!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tinymlnumbers.png&quot; alt=&quot;Picture of numbers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Please check out the &lt;a href=&quot;https://arxiv.org/pdf/2106.04008.pdf&quot;&gt;paper&lt;/a&gt;, or indeed the entire course at &lt;a href=&quot;https://www.edx.org/professional-certificate/harvardx-tiny-machine-learning&quot;&gt;edX&lt;/a&gt; where you can audit it at no cost.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="talks" /><category term="mooc" /><summary type="html">I’m delighted to announce that the paper, I’ve worked on with folks from Harvard and Google, has been published online to arxiv! We discuss the strategies that we used in designing a curriculum to widen access to Applied ML, focusing on the rapid growth of TinyML and how a new teaching methodology can open doorways previously shut to the traditionally underrepresented. It’s critically important to do this type of teaching at this time because with the rapid growth of ML, and TinyML in particular, I anticipate that we’ll soon end up with several prominent players in this space and that these players will likely be in the traditional high tech areas. By opening up easy access to teaching materials for everyone, we hope to level the playing field. With that in mind, we designed the course to have the academic rigor and depth of a top-tier university like Harvard, with the breadth of understanding required to work in a major company like Google. Our goal was linking the two to bring students holistically through a journey of understanding ML, using it, and then deploying it to embedded systems and microcontrollers. One observation we made while working on this is an inverse relationship between the number of available ML educational resources and the number of systems in the field! We wanted to fix that! Please check out the paper, or indeed the entire course at edX where you can audit it at no cost.</summary></entry><entry><title type="html">Impressions of iOS and iPadOS 15 from WWDC</title><link href="/2021/06/07/ios.html" rel="alternate" type="text/html" title="Impressions of iOS and iPadOS 15 from WWDC" /><published>2021-06-07T00:00:00-07:00</published><updated>2021-06-07T00:00:00-07:00</updated><id>/2021/06/07/ios</id><content type="html" xml:base="/2021/06/07/ios.html">&lt;p&gt;Impressions of the WWDC Keynote&lt;/p&gt;

&lt;p&gt;TL;DR A definite change in attitude from Apple. A lot less hubris while delivering a condensed, concise set of features and updates to multiple operating systems while still keeping it clear and relevant for developers. My thoughts on iOS and iPadOS updates:&lt;/p&gt;

&lt;h2 id=&quot;ios-15&quot;&gt;iOS 15:&lt;/h2&gt;

&lt;p&gt;Updates to the social experience came thick and fast, from SharePlay that allows you to watch, listen and work together through a facetime call to spatial audio so that voices will sound like they’re coming from where the person is located on the screen. Not sure about the latter, given that phone screens are small, but let’s wait and see. I &lt;em&gt;love&lt;/em&gt; Spatial Audio when used with things like Airpods Max and the Disney+ App, so I’m hopeful!&lt;/p&gt;

&lt;p&gt;I love the new Mic modes so that you can reduce background noise for meetings etc. Nothing groundbreaking or revolutionary here – it’s stuff you’ve probably experienced before on different apps or platforms. Still, it will be worth exploring with the regular Apple polish and tight integration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/iphone.png&quot; alt=&quot;iOS 15&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are also updates to messenger, where the ability to share things like pictures in messages is greatly improved. Photo collections where automated slideshows integrating music from Apple Music are a nice touch, but again, merely an incremental improvement on what has gone before.&lt;/p&gt;

&lt;p&gt;It also includes a ‘Focus’ ability which integrates down to things like message settings, which is nice. I haven’t figured out how to use it yet, though!&lt;/p&gt;

&lt;p&gt;Maps have a whole host of updates, with beautiful new cartoon-like visualizations.&lt;/p&gt;

&lt;p&gt;The immersive walking instructions appear to have been lifted directly from Google Maps, though! Seeing stuff I used several years ago on my Android, now being portrayed as ‘new,’ is annoying!&lt;/p&gt;

&lt;p&gt;Safari has also had a refresh to make it a little more mobile-friendly with tabs at the bottom navigable with a swipe. Maybe I’m just not used to it, but I did find it clunky.&lt;/p&gt;

&lt;p&gt;Live Text in the camera is pretty cool, but again, nothing groundbreaking that you haven’t seen before. It includes ‘Visual Look Up,’ which is basically Google Lens.&lt;/p&gt;

&lt;p&gt;There’s a tonne of updates for health, but I’ll have to cover them later when I’ve had a chance to play with them.&lt;/p&gt;

&lt;h2 id=&quot;ipados-15&quot;&gt;iPadOS 15&lt;/h2&gt;

&lt;p&gt;I like Multitasking in iPadOS 15. Finally, it doesn’t look like Windows 8! The home screen and widgets have been updated so that widgets do feel like they’re fully integrated instead of shoved to the side as they were with iPadOS 14.x&lt;/p&gt;

&lt;p&gt;I think my very favorite feature is the new ‘Quick Note’, where you can swipe up from the corner and get a little popup note that allows you to make a quick note with your finger or a pencil. Notes are synced across your devices and linked to websites so that you can connect a note with content on the page. (Long ago, I worked on a Windows 8 launch app called ‘strands’ that did this, but it didn’t make it to the Windows 8 launch, so lovely the see the idea re-used!)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ipaddev.png&quot; alt=&quot;iPad Dev&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another excellent update is Swift Playgrounds now on iPad! Yes, you can write code in SwiftUI with code completion and preview….and you can submit your apps straight to the app store! Yes, your iPad is now a developer tool!&lt;/p&gt;

&lt;p&gt;If you’re using macOS Monterey, you can even work across devices – with your mouse/trackpad from your macOS device being usable on your iPad – giving you an authentic second-screen experience! I’m going to play with that one and report back! :)&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="coding" /><summary type="html">Impressions of the WWDC Keynote TL;DR A definite change in attitude from Apple. A lot less hubris while delivering a condensed, concise set of features and updates to multiple operating systems while still keeping it clear and relevant for developers. My thoughts on iOS and iPadOS updates: iOS 15: Updates to the social experience came thick and fast, from SharePlay that allows you to watch, listen and work together through a facetime call to spatial audio so that voices will sound like they’re coming from where the person is located on the screen. Not sure about the latter, given that phone screens are small, but let’s wait and see. I love Spatial Audio when used with things like Airpods Max and the Disney+ App, so I’m hopeful! I love the new Mic modes so that you can reduce background noise for meetings etc. Nothing groundbreaking or revolutionary here – it’s stuff you’ve probably experienced before on different apps or platforms. Still, it will be worth exploring with the regular Apple polish and tight integration. There are also updates to messenger, where the ability to share things like pictures in messages is greatly improved. Photo collections where automated slideshows integrating music from Apple Music are a nice touch, but again, merely an incremental improvement on what has gone before. It also includes a ‘Focus’ ability which integrates down to things like message settings, which is nice. I haven’t figured out how to use it yet, though! Maps have a whole host of updates, with beautiful new cartoon-like visualizations. The immersive walking instructions appear to have been lifted directly from Google Maps, though! Seeing stuff I used several years ago on my Android, now being portrayed as ‘new,’ is annoying! Safari has also had a refresh to make it a little more mobile-friendly with tabs at the bottom navigable with a swipe. Maybe I’m just not used to it, but I did find it clunky. Live Text in the camera is pretty cool, but again, nothing groundbreaking that you haven’t seen before. It includes ‘Visual Look Up,’ which is basically Google Lens. There’s a tonne of updates for health, but I’ll have to cover them later when I’ve had a chance to play with them. iPadOS 15 I like Multitasking in iPadOS 15. Finally, it doesn’t look like Windows 8! The home screen and widgets have been updated so that widgets do feel like they’re fully integrated instead of shoved to the side as they were with iPadOS 14.x I think my very favorite feature is the new ‘Quick Note’, where you can swipe up from the corner and get a little popup note that allows you to make a quick note with your finger or a pencil. Notes are synced across your devices and linked to websites so that you can connect a note with content on the page. (Long ago, I worked on a Windows 8 launch app called ‘strands’ that did this, but it didn’t make it to the Windows 8 launch, so lovely the see the idea re-used!) Another excellent update is Swift Playgrounds now on iPad! Yes, you can write code in SwiftUI with code completion and preview….and you can submit your apps straight to the app store! Yes, your iPad is now a developer tool! If you’re using macOS Monterey, you can even work across devices – with your mouse/trackpad from your macOS device being usable on your iPad – giving you an authentic second-screen experience! I’m going to play with that one and report back! :)</summary></entry><entry><title type="html">Some things I’d like to see at WWDC 2021</title><link href="/2021/06/05/WWDC.html" rel="alternate" type="text/html" title="Some things I’d like to see at WWDC 2021" /><published>2021-06-05T00:00:00-07:00</published><updated>2021-06-05T00:00:00-07:00</updated><id>/2021/06/05/WWDC</id><content type="html" xml:base="/2021/06/05/WWDC.html">&lt;p&gt;So, WWDC starts on Monday, June 7, and I can unashamedly say it’s one of my most anticipated developer events of the year. I enjoy all Apple keynotes (despite sometimes being just a little too self-congratulatory), and the WWDC one is my favorite. It’s best when they focus on features in each of their new versions of iOS, macOS, iPadOS, etc., and at its worst when they use their platform to attack their competitors. So, as well as just focussing on their products, here’s a few things I would love to see at WWDC this year.&lt;/p&gt;

&lt;h2 id=&quot;create-ml-model-interpretability&quot;&gt;Create ML Model Interpretability&lt;/h2&gt;
&lt;p&gt;Create ML is a &lt;em&gt;great&lt;/em&gt; tool, but for some reason, Apple does not give you any indication of the model architecture it creates. Additionally, the various workarounds that &lt;em&gt;used&lt;/em&gt; to be available have now been obfuscated. That’s a big step backward in an era of model interpretability. Let’s be honest here, nobody creating a model with a tool like Create ML is developing a novel architecture – they’re making a model using existing images, and the tool creates the model, so what is there to protect? Worse – what if there are fairness issues with your model? Not having any control over its architecture is just crazy. So, please, Apple Developers, take it in the other direction and let folks see the architecture of the model output by the tool.&lt;/p&gt;

&lt;p&gt;And let’s be honest here – in the case of Image Classification, it’s probably just a new classification head on the bottom of a model like MobileNet or EfficientDet, with the new number of classes. So why obfuscate? Or, best of all, maybe give us developers the opportunity to choose?&lt;/p&gt;

&lt;h4 id=&quot;likelihood-low&quot;&gt;Likelihood: Low&lt;/h4&gt;

&lt;h2 id=&quot;know-your-data---like-tools-in-create-ml&quot;&gt;Know your data - like tools in Create ML&lt;/h2&gt;
&lt;p&gt;On the theme of AI Fairness and ethics, it would be fantastic if a tool like &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/&quot;&gt;Know Your Data&lt;/a&gt; got integrated into Create ML. It’s an immensely useful tool that lets you inspect your data against things that could cause possible biases. For example, please take a look at my &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#tab=STATS&amp;amp;dataset=horses_or_humans&quot;&gt;Horses of Humans&lt;/a&gt; dataset. I created this explicitly to avoid bias by having men and women of multiple sizes, skin tones, etc. Without realizing it, in many cases, the lighting I used made humans of some skin tones have unrecognizable faces, and I would never have known this without KYD!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kyd.png&quot; alt=&quot;KYD Issue&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, if people create models quickly and easily with Create ML, let’s not lead them down the path of having biased models unintentionally! Please integrate KYD, or something like it!&lt;/p&gt;

&lt;h4 id=&quot;likelihood-tiny&quot;&gt;Likelihood: Tiny&lt;/h4&gt;

&lt;h2 id=&quot;export-to-tf-lite&quot;&gt;Export to TF Lite&lt;/h2&gt;
&lt;p&gt;TensorFlow Lite is the premier framework for running ML on &lt;em&gt;all&lt;/em&gt; flavors of mobile. Android, iOS, JavaScript, Linux-based systems, embedded systems, etc. Create ML is a terrific tool. Wouldn’t it be nice for it also to export in .tflite format, so we could use it as a tool to create models for all of the above? I know it’s not the typical approach for Apple, but it sure would be nice!&lt;/p&gt;

&lt;h4 id=&quot;likelihood-near-absolute-zero&quot;&gt;Likelihood: Near absolute Zero&lt;/h4&gt;

&lt;h2 id=&quot;airtags-api&quot;&gt;AirTags API&lt;/h2&gt;
&lt;p&gt;I bought a pack of 4 Air Tags and am waiting for my holders to attach them to things I don’t want to lose. They are undeniably cool, not least because they use the network of iOS devices globally to act as finders. There’s the possibility of applications beyond ‘Find My’ being able to use them. I have a game I want to write to use them instead of beacons, which didn’t live up to the promise. I’m hoping Apple will extend the Nearby API for Airtags at WWDC. Bonus points if their &lt;a href=&quot;https://developer.apple.com/documentation/nearbyinteraction/implementing_interactions_between_users_in_close_proximity&quot;&gt;Nearby Interaction demo&lt;/a&gt;, which in my book goes down as one of the best developer demos ever, is updated for Air Tags.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/NIDemo.png&quot; alt=&quot;NI Demo Image&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;likelihood-quite-possible&quot;&gt;Likelihood: Quite possible&lt;/h4&gt;

&lt;h2 id=&quot;access-to-heart-rate-sensor-in-realtime&quot;&gt;Access to heart rate sensor in realtime&lt;/h2&gt;
&lt;p&gt;Maybe this is already available, but I can’t find it. With the watch having a heart rate sensor, it would be nice to have an API that triggers an alert when the heart rate goes above a certain amount. You might even be able to gamify this. Honest story: I was once playing Catan with my wife, and she was offering a trade that would have won her the game. It seemed a good trade to me. But, her apple watch was visible, and I saw her heart rate was higher than usual, indicating (to me, at least) that she was probably lying! I didn’t take the trade. It didn’t matter; she won on the next turn! But it did get me thinking!&lt;/p&gt;

&lt;h4 id=&quot;likelihood-probably-not&quot;&gt;Likelihood: Probably Not&lt;/h4&gt;</content><author><name>Laurence Moroney</name></author><category term="personal" /><summary type="html">So, WWDC starts on Monday, June 7, and I can unashamedly say it’s one of my most anticipated developer events of the year. I enjoy all Apple keynotes (despite sometimes being just a little too self-congratulatory), and the WWDC one is my favorite. It’s best when they focus on features in each of their new versions of iOS, macOS, iPadOS, etc., and at its worst when they use their platform to attack their competitors. So, as well as just focussing on their products, here’s a few things I would love to see at WWDC this year. Create ML Model Interpretability Create ML is a great tool, but for some reason, Apple does not give you any indication of the model architecture it creates. Additionally, the various workarounds that used to be available have now been obfuscated. That’s a big step backward in an era of model interpretability. Let’s be honest here, nobody creating a model with a tool like Create ML is developing a novel architecture – they’re making a model using existing images, and the tool creates the model, so what is there to protect? Worse – what if there are fairness issues with your model? Not having any control over its architecture is just crazy. So, please, Apple Developers, take it in the other direction and let folks see the architecture of the model output by the tool. And let’s be honest here – in the case of Image Classification, it’s probably just a new classification head on the bottom of a model like MobileNet or EfficientDet, with the new number of classes. So why obfuscate? Or, best of all, maybe give us developers the opportunity to choose? Likelihood: Low Know your data - like tools in Create ML On the theme of AI Fairness and ethics, it would be fantastic if a tool like Know Your Data got integrated into Create ML. It’s an immensely useful tool that lets you inspect your data against things that could cause possible biases. For example, please take a look at my Horses of Humans dataset. I created this explicitly to avoid bias by having men and women of multiple sizes, skin tones, etc. Without realizing it, in many cases, the lighting I used made humans of some skin tones have unrecognizable faces, and I would never have known this without KYD! So, if people create models quickly and easily with Create ML, let’s not lead them down the path of having biased models unintentionally! Please integrate KYD, or something like it! Likelihood: Tiny Export to TF Lite TensorFlow Lite is the premier framework for running ML on all flavors of mobile. Android, iOS, JavaScript, Linux-based systems, embedded systems, etc. Create ML is a terrific tool. Wouldn’t it be nice for it also to export in .tflite format, so we could use it as a tool to create models for all of the above? I know it’s not the typical approach for Apple, but it sure would be nice! Likelihood: Near absolute Zero AirTags API I bought a pack of 4 Air Tags and am waiting for my holders to attach them to things I don’t want to lose. They are undeniably cool, not least because they use the network of iOS devices globally to act as finders. There’s the possibility of applications beyond ‘Find My’ being able to use them. I have a game I want to write to use them instead of beacons, which didn’t live up to the promise. I’m hoping Apple will extend the Nearby API for Airtags at WWDC. Bonus points if their Nearby Interaction demo, which in my book goes down as one of the best developer demos ever, is updated for Air Tags. Likelihood: Quite possible Access to heart rate sensor in realtime Maybe this is already available, but I can’t find it. With the watch having a heart rate sensor, it would be nice to have an API that triggers an alert when the heart rate goes above a certain amount. You might even be able to gamify this. Honest story: I was once playing Catan with my wife, and she was offering a trade that would have won her the game. It seemed a good trade to me. But, her apple watch was visible, and I saw her heart rate was higher than usual, indicating (to me, at least) that she was probably lying! I didn’t take the trade. It didn’t matter; she won on the next turn! But it did get me thinking! Likelihood: Probably Not</summary></entry><entry><title type="html">Tips for getting a job in tech - Part Three - The Interview</title><link href="/2021/06/04/interviewtips.html" rel="alternate" type="text/html" title="Tips for getting a job in tech - Part Three - The Interview" /><published>2021-06-04T00:00:00-07:00</published><updated>2021-06-04T00:00:00-07:00</updated><id>/2021/06/04/interviewtips</id><content type="html" xml:base="/2021/06/04/interviewtips.html">&lt;p&gt;It’s all come together. Your &lt;a href=&quot;https://laurencemoroney.com/2021/06/02/resumetips.html&quot;&gt;resume&lt;/a&gt; passed through the system. Your &lt;a href=&quot;https://laurencemoroney.com/2021/06/03/networkingtips.html&quot;&gt;networking&lt;/a&gt; got you noticed. Now it’s time for…the interview!&lt;/p&gt;

&lt;p&gt;Now all companies will interview differently, so I’m going to focus on what I’m more experienced in, and that is how larger companies tend to fill specific spots. And that’s with an initial screen by a recruiter, followed by a technical screen, followed by an interview loop.&lt;/p&gt;

&lt;h2 id=&quot;the-initial-screen&quot;&gt;The Initial Screen&lt;/h2&gt;
&lt;p&gt;The initial screen is often given by a recruiter, who isn’t very technical, and if they have technical questions, they are likely working from a script, so you need to keep your answers precise and concise. The questions won’t be open-ended. But, of course, you can’t work &lt;em&gt;together&lt;/em&gt; with them to come up with an answer. If you don’t know it, take your best shot, and move on. The worst answer you can give is complaining about the question!&lt;/p&gt;

&lt;p&gt;You might get a question along the lines of: “Profiling tools show a memory leak in your app. What steps would you take to fix it?”&lt;/p&gt;

&lt;p&gt;It’s pretty straightforward. Either you know it (excellent!), or you don’t (not the end of the world, but how you deal with trying to answer it can tell a lot about you), so don’t panic. Do your best, and try to build a rapport with the interviewer. It’s not a confrontation; it’s an opportunity for friendship.&lt;/p&gt;

&lt;p&gt;If they ask if &lt;em&gt;you&lt;/em&gt; have any questions, please take advantage of this. Ask about the next steps, what you might need to do to prepare, or, best of all, if there’s anything else the interviewer needs from you because you understand their time is valuable too.&lt;/p&gt;

&lt;h2 id=&quot;the-tech-screen&quot;&gt;The Tech Screen&lt;/h2&gt;
&lt;p&gt;Like the initial screen, this one will be with someone more tech-savvy, and thus, may be more open-ended and involve coding. You may get a Google Doc (or similar) shared with you, and they’ll ask you to pseudocode a problem. Note: Typing it in real-time is essential here. Some candidates have a library of common questions/answers that they’ve scoured off the internet that they can paste into an answer doc. Whenever I do a tech screen, if I see a paste of code, it’s an instant red flag, and I’ll ask many questions about that code. If someone starts typing and starts talking their way through the problem, asking clarifying questions of me – then I’m impressed! The code doesn’t have to be perfect. The point is to see if the person &lt;em&gt;can&lt;/em&gt; code and not a test to see if they can &lt;em&gt;deliver working code&lt;/em&gt;. All too often, people think the latter and end up cheating. Please don’t do it.&lt;/p&gt;

&lt;p&gt;And this is your opportunity to ask on-the-job questions with somebody that does the job all the time. Try not to veer into questions about pay or benefits, or hours. Focus on the job at hand.&lt;/p&gt;

&lt;h2 id=&quot;the-interview-loop&quot;&gt;The Interview Loop&lt;/h2&gt;
&lt;p&gt;Not every experience will be the same, but a loop will typically involve about five interviews, and these will be from folks that you immediately work with, as well as folks from adjacent teams that you’ll work alongside. A Developer Advocate interview loop may, for example, have two DAs, one Manager, one marketing person, and one product team person. The important thing that they are all looking for is if they can work with you. You need more than just ability, but also, very importantly, attitude and ethic. For example, you might respect the DAs and the product person but be subconsciously dismissive of the marketing one. It’s a habit in engineering. Don’t do that! Use every interview as a learning opportunity. If you don’t get the job, at least you’ve gotten better at understanding how to fit into a job at a large company.&lt;/p&gt;

&lt;p&gt;You’ll get technical questions. Sometimes it will be a piece of code where you have to find the bugs. Sometimes it will be to write an algorithm on the whiteboard to solve something. Sometimes it will be a quirky corner-case in a particular language. The best three things you can do to prepare are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Be honest on your resume. If you claim expertise on something, and you can’t answer questions or code in it, it’s game over. Possibly forever with that company.&lt;/li&gt;
  &lt;li&gt;Read books on cracking the coding interview to learn the standard algorithms and problem types. Things like recursion, map-reduce, etc. Problems like the traveling salesperson or the eight queens. It’s a great way to prepare, and fun too!&lt;/li&gt;
  &lt;li&gt;Have repos of &lt;em&gt;your&lt;/em&gt; code on your resume. It’s much more fun for you (and for the interviewer) to talk about your real stuff than it is to talk about hypotheticals&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let me give an example of this. When I interviewed at Google in 2013, it was as a Cloud Developer Advocate. I had never touched Google Cloud before. In the month before the interview, I crammed. I knew Java. App Engine ran Java.&lt;/p&gt;

&lt;p&gt;So I wrote some Java code to run on App Engine. And from that, I learned how easy it was to put Java code on the internet with Google Cloud. It was always a pain to access a public-facing App server without costing an arm and a leg. App Engine did it for free.&lt;/p&gt;

&lt;p&gt;Then I learned how to use Cloud Datastore because a relational DB from a virtual machine didn’t work well.&lt;/p&gt;

&lt;p&gt;Then I learned how to use memory caching to speed up parts of my app. I saw how App Engine VMs were pre-emptively warmed up for performance. I also saw how, with Cloud, I only paid for the capacity I used, not a flat monthly rate regardless of traffic.&lt;/p&gt;

&lt;p&gt;I put that on my resume. Three of my Five interviews ended up enthusiastically discussing my code and overall experience with the product.&lt;/p&gt;

&lt;p&gt;For my final interview, the guy started by telling me that the previous interviewers had all chatted with me about my Java code, so to be fair, he wanted to try something else! He gave me a problem right out of the ‘Cracking the Coding Interview’ book. They hired me.&lt;/p&gt;

&lt;p&gt;So, yes – please go ahead and write some code for the area(s) you want to get hired for. When you submit your resume for that specific job, make sure it’s prominent!&lt;/p&gt;

&lt;h2 id=&quot;things-not-to-do&quot;&gt;Things not to do&lt;/h2&gt;
&lt;p&gt;I’ve given many interviews, and I’ve seen some very bizarre things. I’m not going to talk about the weird ones (but that make a fun blog post someday,) but here are some common pitfalls.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dress. Don’t overdress, but don’t underdress either. If you’re going for a developer job, think about how you might dress on the job, and maybe take it up a little bit to err on the side of caution. Try to avoid any excess logos. You might love &lt;em&gt;Manchester United&lt;/em&gt;, but don’t wear their jersey to the interview! Try to avoid any logos or statements on clothing that will change the conversation away from &lt;em&gt;you&lt;/em&gt; and into &lt;em&gt;what you support&lt;/em&gt;. A lot of that is fine, later, after you get the job! Most places won’t discriminate, so your ‘Jesus Saves’ tee shirt won’t prevent you from getting the job, but if all they remember is that, and not your coding or working abilities, you’ve done yourself a massive disservice. Consider the same for sexy or revealing clothing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Attitude. You are a &lt;em&gt;guest&lt;/em&gt; at the company during the interview, not a &lt;em&gt;VIP&lt;/em&gt;. Treat your hosts with respect. One common pitfall I see here (and I think I have failed in this, myself) is that if multiple people are interviewing you simultaneously, treat them all equally. It’s easy to ignore the quiet ones and focus on those speaking. Don’t assume those speaking are the most senior, and don’t dismiss anybody as not worth your time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lunch. Often, because the day is long, they’ll take you for lunch. This is still an interview, and treat it as such. Please don’t use it as an opportunity to let your hair down and gripe about the morning’s interviews. (I have experienced this!). Also, don’t treat this as an opportunity to gorge yourself on free food! Have enough to get yourself through the day, but not so much that your host can’t hear a word you say over the munching.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Diversity. There’s a well-documented diversity problem in tech. Be mindful of this, in particular, if you are being interviewed by those not traditionally represented. This is an opportunity for you to show that you &lt;em&gt;can&lt;/em&gt; work with people from all walks of life. It’s also a problem if you fail to treat them with the same respect as others.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Handshakes. Once upon a time, it was common wisdom that you should give a firm handshake to all you meet in a business environment. The stronger the handshake, the better. That’s not the case anymore. It’s good to be polite, and if people offer to shake your hand, take it. But don’t follow the former advice and attempt to crush their bones. And don’t go strong if they are younger or smaller than you. An interview isn’t a domination game, and if you treat it as such, it will be a hard pass. I still remember interviewing one guy who shook my hand so hard, crushing a ring on my finger, that for the rest of the interview, I was so sore, I can’t remember anything he said. He did not get hired.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;False assumptions. Please don’t assume that something familiar to you is also normal to them. And don’t dismiss them for not knowing it. Often they may feign ignorance as a way to get you to talk about it. I once interviewed a guy who put that his undergrad GPA was 3.9 on his resume. It was in bold, italics, and a larger font than the rest. I asked him about it, being a relatively recent immigrant from a country with a different scoring system. He belittled my apparent stupidity, told me the number of people who get that grade, threw out some random Latin words, and spoke about the colored cords at graduation. But he never actually explained what it meant. He didn’t get the job. Similarly, I once interviewed a woman who mentioned a particular algorithm type that she’d worked extensively with. I asked her to explain it to me – her response: “You work in X, and you don’t know algorithm Y, what kind of place is this?” I asked her why it should be necessary for X to know Y, and she continued her lambasting. She did not get the job. Full disclosure: Of course, I knew Y, but I never got to know if her level of expertise was more significant than mine. I had hoped it was.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’m sure there’s more, but I’ll leave it there. I hope this is helpful to you!&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="careers" /><summary type="html">It’s all come together. Your resume passed through the system. Your networking got you noticed. Now it’s time for…the interview! Now all companies will interview differently, so I’m going to focus on what I’m more experienced in, and that is how larger companies tend to fill specific spots. And that’s with an initial screen by a recruiter, followed by a technical screen, followed by an interview loop. The Initial Screen The initial screen is often given by a recruiter, who isn’t very technical, and if they have technical questions, they are likely working from a script, so you need to keep your answers precise and concise. The questions won’t be open-ended. But, of course, you can’t work together with them to come up with an answer. If you don’t know it, take your best shot, and move on. The worst answer you can give is complaining about the question! You might get a question along the lines of: “Profiling tools show a memory leak in your app. What steps would you take to fix it?” It’s pretty straightforward. Either you know it (excellent!), or you don’t (not the end of the world, but how you deal with trying to answer it can tell a lot about you), so don’t panic. Do your best, and try to build a rapport with the interviewer. It’s not a confrontation; it’s an opportunity for friendship. If they ask if you have any questions, please take advantage of this. Ask about the next steps, what you might need to do to prepare, or, best of all, if there’s anything else the interviewer needs from you because you understand their time is valuable too. The Tech Screen Like the initial screen, this one will be with someone more tech-savvy, and thus, may be more open-ended and involve coding. You may get a Google Doc (or similar) shared with you, and they’ll ask you to pseudocode a problem. Note: Typing it in real-time is essential here. Some candidates have a library of common questions/answers that they’ve scoured off the internet that they can paste into an answer doc. Whenever I do a tech screen, if I see a paste of code, it’s an instant red flag, and I’ll ask many questions about that code. If someone starts typing and starts talking their way through the problem, asking clarifying questions of me – then I’m impressed! The code doesn’t have to be perfect. The point is to see if the person can code and not a test to see if they can deliver working code. All too often, people think the latter and end up cheating. Please don’t do it. And this is your opportunity to ask on-the-job questions with somebody that does the job all the time. Try not to veer into questions about pay or benefits, or hours. Focus on the job at hand. The Interview Loop Not every experience will be the same, but a loop will typically involve about five interviews, and these will be from folks that you immediately work with, as well as folks from adjacent teams that you’ll work alongside. A Developer Advocate interview loop may, for example, have two DAs, one Manager, one marketing person, and one product team person. The important thing that they are all looking for is if they can work with you. You need more than just ability, but also, very importantly, attitude and ethic. For example, you might respect the DAs and the product person but be subconsciously dismissive of the marketing one. It’s a habit in engineering. Don’t do that! Use every interview as a learning opportunity. If you don’t get the job, at least you’ve gotten better at understanding how to fit into a job at a large company. You’ll get technical questions. Sometimes it will be a piece of code where you have to find the bugs. Sometimes it will be to write an algorithm on the whiteboard to solve something. Sometimes it will be a quirky corner-case in a particular language. The best three things you can do to prepare are: Be honest on your resume. If you claim expertise on something, and you can’t answer questions or code in it, it’s game over. Possibly forever with that company. Read books on cracking the coding interview to learn the standard algorithms and problem types. Things like recursion, map-reduce, etc. Problems like the traveling salesperson or the eight queens. It’s a great way to prepare, and fun too! Have repos of your code on your resume. It’s much more fun for you (and for the interviewer) to talk about your real stuff than it is to talk about hypotheticals Let me give an example of this. When I interviewed at Google in 2013, it was as a Cloud Developer Advocate. I had never touched Google Cloud before. In the month before the interview, I crammed. I knew Java. App Engine ran Java. So I wrote some Java code to run on App Engine. And from that, I learned how easy it was to put Java code on the internet with Google Cloud. It was always a pain to access a public-facing App server without costing an arm and a leg. App Engine did it for free. Then I learned how to use Cloud Datastore because a relational DB from a virtual machine didn’t work well. Then I learned how to use memory caching to speed up parts of my app. I saw how App Engine VMs were pre-emptively warmed up for performance. I also saw how, with Cloud, I only paid for the capacity I used, not a flat monthly rate regardless of traffic. I put that on my resume. Three of my Five interviews ended up enthusiastically discussing my code and overall experience with the product. For my final interview, the guy started by telling me that the previous interviewers had all chatted with me about my Java code, so to be fair, he wanted to try something else! He gave me a problem right out of the ‘Cracking the Coding Interview’ book. They hired me. So, yes – please go ahead and write some code for the area(s) you want to get hired for. When you submit your resume for that specific job, make sure it’s prominent! Things not to do I’ve given many interviews, and I’ve seen some very bizarre things. I’m not going to talk about the weird ones (but that make a fun blog post someday,) but here are some common pitfalls. Dress. Don’t overdress, but don’t underdress either. If you’re going for a developer job, think about how you might dress on the job, and maybe take it up a little bit to err on the side of caution. Try to avoid any excess logos. You might love Manchester United, but don’t wear their jersey to the interview! Try to avoid any logos or statements on clothing that will change the conversation away from you and into what you support. A lot of that is fine, later, after you get the job! Most places won’t discriminate, so your ‘Jesus Saves’ tee shirt won’t prevent you from getting the job, but if all they remember is that, and not your coding or working abilities, you’ve done yourself a massive disservice. Consider the same for sexy or revealing clothing. Attitude. You are a guest at the company during the interview, not a VIP. Treat your hosts with respect. One common pitfall I see here (and I think I have failed in this, myself) is that if multiple people are interviewing you simultaneously, treat them all equally. It’s easy to ignore the quiet ones and focus on those speaking. Don’t assume those speaking are the most senior, and don’t dismiss anybody as not worth your time. Lunch. Often, because the day is long, they’ll take you for lunch. This is still an interview, and treat it as such. Please don’t use it as an opportunity to let your hair down and gripe about the morning’s interviews. (I have experienced this!). Also, don’t treat this as an opportunity to gorge yourself on free food! Have enough to get yourself through the day, but not so much that your host can’t hear a word you say over the munching. Diversity. There’s a well-documented diversity problem in tech. Be mindful of this, in particular, if you are being interviewed by those not traditionally represented. This is an opportunity for you to show that you can work with people from all walks of life. It’s also a problem if you fail to treat them with the same respect as others. Handshakes. Once upon a time, it was common wisdom that you should give a firm handshake to all you meet in a business environment. The stronger the handshake, the better. That’s not the case anymore. It’s good to be polite, and if people offer to shake your hand, take it. But don’t follow the former advice and attempt to crush their bones. And don’t go strong if they are younger or smaller than you. An interview isn’t a domination game, and if you treat it as such, it will be a hard pass. I still remember interviewing one guy who shook my hand so hard, crushing a ring on my finger, that for the rest of the interview, I was so sore, I can’t remember anything he said. He did not get hired. False assumptions. Please don’t assume that something familiar to you is also normal to them. And don’t dismiss them for not knowing it. Often they may feign ignorance as a way to get you to talk about it. I once interviewed a guy who put that his undergrad GPA was 3.9 on his resume. It was in bold, italics, and a larger font than the rest. I asked him about it, being a relatively recent immigrant from a country with a different scoring system. He belittled my apparent stupidity, told me the number of people who get that grade, threw out some random Latin words, and spoke about the colored cords at graduation. But he never actually explained what it meant. He didn’t get the job. Similarly, I once interviewed a woman who mentioned a particular algorithm type that she’d worked extensively with. I asked her to explain it to me – her response: “You work in X, and you don’t know algorithm Y, what kind of place is this?” I asked her why it should be necessary for X to know Y, and she continued her lambasting. She did not get the job. Full disclosure: Of course, I knew Y, but I never got to know if her level of expertise was more significant than mine. I had hoped it was. I’m sure there’s more, but I’ll leave it there. I hope this is helpful to you!</summary></entry><entry><title type="html">Tips for getting a job in tech - Part Two - Networking</title><link href="/2021/06/03/networkingtips.html" rel="alternate" type="text/html" title="Tips for getting a job in tech - Part Two - Networking" /><published>2021-06-03T00:00:00-07:00</published><updated>2021-06-03T00:00:00-07:00</updated><id>/2021/06/03/networkingtips</id><content type="html" xml:base="/2021/06/03/networkingtips.html">&lt;p&gt;Following on from &lt;a href=&quot;https://laurencemoroney.com/2021/06/02/resumetips.html&quot;&gt;Part One&lt;/a&gt; where we discussed resumes or CVs, let’s now explore another part of job hunting that is very different in the tech world: networking. And no, I don’t mean doing stuff over the internet or a LAN. It’s about getting to know people and having people get to know you. Networking is generally a powerful tool in any job hunt, but I think it’s unique in tech.&lt;/p&gt;

&lt;h2 id=&quot;communities&quot;&gt;Communities&lt;/h2&gt;
&lt;p&gt;It’s part of tech culture that we form communities. Just about every company, product, open-source tool, you name it, have some form of community attached. Join them, and participate in them to get your name known in the community.&lt;/p&gt;

&lt;p&gt;But get yourself known for &lt;em&gt;positive&lt;/em&gt; things. Don’t be a gatekeeper to new folks coming in. Don’t be toxic with those you disagree with, no matter how tempting. Being a force for good can get you stomped on sometimes, but pick yourself right up and keep going. Being a positive force in a community is enough of a reward! If you’re looking for a career, it’s essential.&lt;/p&gt;

&lt;h2 id=&quot;meet-ups&quot;&gt;Meet-ups&lt;/h2&gt;
&lt;p&gt;Building on the communities point above, developer communities associated with larger frameworks or big companies like FAAANG tend to have meet-ups in most cities. Join them. Contribute, even if it just means doing the most basic helpful tasks. I’ve run several meet-ups, and the most valuable people are the ones that don’t consider anything beneath them. They’ll take on the most mundane tasks. That helps them network with peers and, importantly, allows organizers to get to know them well.&lt;/p&gt;

&lt;p&gt;I recall speaking at the Google Developer Group (GDG) in Seattle, where a young woman approached me. She had attended many meet-ups over the final two years of her degree in Software Engineering at a nearby University. She was just about to graduate and soon to start a job at an excellent local tech company. How she did it? She attended every Microsoft, Amazon, Google, Apple, and independent meet-up she could find. Recruiters also participated at these, and one from that tech company got to know her and eventually hired her. A textbook example of someone doing it the &lt;em&gt;right&lt;/em&gt; way.&lt;/p&gt;

&lt;h2 id=&quot;expert-groups&quot;&gt;Expert Groups&lt;/h2&gt;
&lt;p&gt;Most companies like Google, Microsoft, and others have a formal community of folks deemed ‘experts in the field. In Google, we call then GDEs (Google Developer Experts). These folks usually come through the communities and meet-ups and get known as experts in an area. They get nominated to be reviewed by the company, usually with an interview, before achieving this status. Being recognized as an expert by the likes of Google or Microsoft helps you find a career &lt;em&gt;anywhere&lt;/em&gt;. 
Becoming a recognized expert like this can be a great career launchpad. It’s worth investigating!&lt;/p&gt;

&lt;h2 id=&quot;referrals&quot;&gt;Referrals&lt;/h2&gt;
&lt;p&gt;Referrals are possibly the most powerful tool in getting recruited by larger companies. They’re also one of the most misunderstood.&lt;/p&gt;

&lt;p&gt;First of all, here’s what &lt;em&gt;not&lt;/em&gt; to do. Don’t reach out to people you do not know, asking them for a referral into their company. Seriously, please don’t do it. I’ve seen advice to the opposite, where people say it’s just like going and knocking on doors until someone answers, as a great sales technique. That’s a bad analogy. It’s more like approaching a stranger at a bar and asking them for an introduction to all their single friends telling them how awesome you are, in the hope of getting a date. Creepy, right? That’s &lt;em&gt;more&lt;/em&gt; what spamming someone and asking for a referral resembles. People argue that one should be more lenient on folks who are desperate to find a job. I argue back that desperation isn’t an excuse for laziness and pushing the burden onto others. If one genuinely is desperate (as I have been many times), one should be willing to burn the calories to do it &lt;em&gt;right&lt;/em&gt; and not look for damaging shortcuts.&lt;/p&gt;

&lt;p&gt;Why? Because a referral is supposed to be just that. Telling the company about somebody you &lt;em&gt;know&lt;/em&gt; who would be an excellent fit for available jobs. Referring a person means you are putting your &lt;em&gt;reputation&lt;/em&gt; on the line to give them a chance at a career. You don’t just ask a random stranger to do that.&lt;/p&gt;

&lt;p&gt;I get at least ten people a day asking &lt;em&gt;me&lt;/em&gt; to do that.&lt;/p&gt;

&lt;p&gt;Oh, and think about it deeper – it demonstrates very &lt;em&gt;poor&lt;/em&gt; judgment to approach a stranger like this. What happens if you then apply for the company, your resume ends up on their desk, they look you up on linked in, and they see that you once spammed them for a referral? They’ll likely move onto the next resume. And what if the person you spammed is a recruiter? They’ll probably flag your resume so that nobody in the company will be bothered by someone with such poor judgment.&lt;/p&gt;

&lt;p&gt;So please don’t do it.&lt;/p&gt;

&lt;p&gt;What can you do? Well, consider all of the above – being a part of a community, participating in meet-ups, by joining expert groups. What happens then? People from your target company &lt;em&gt;will&lt;/em&gt; get to know you and &lt;em&gt;will&lt;/em&gt; be able to refer you.&lt;/p&gt;

&lt;p&gt;Because a referral usually involves them answering questions about how they know you, how long they’ve known you, in what capacity, what your abilities are, etc.&lt;/p&gt;

&lt;p&gt;And if they only know you as a random spammer - well, is that the reputation you want to build?&lt;/p&gt;

&lt;p&gt;But if it’s a good referral that answers all of the above questions well – it can get you to the front of the line for your resume to be read. It can bypass a lot of the stuff that we spoke about in the last article.&lt;/p&gt;

&lt;p&gt;I applied for Google three times before they hired me. The third time came through a referral from a skip-level manager who knew me at a previous company. Despite me flunking some of the interviews, they still hired me!&lt;/p&gt;

&lt;p&gt;That’s how powerful a referral can be!&lt;/p&gt;

&lt;p&gt;So, please, take the time to do it right.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="careers" /><summary type="html">Following on from Part One where we discussed resumes or CVs, let’s now explore another part of job hunting that is very different in the tech world: networking. And no, I don’t mean doing stuff over the internet or a LAN. It’s about getting to know people and having people get to know you. Networking is generally a powerful tool in any job hunt, but I think it’s unique in tech. Communities It’s part of tech culture that we form communities. Just about every company, product, open-source tool, you name it, have some form of community attached. Join them, and participate in them to get your name known in the community. But get yourself known for positive things. Don’t be a gatekeeper to new folks coming in. Don’t be toxic with those you disagree with, no matter how tempting. Being a force for good can get you stomped on sometimes, but pick yourself right up and keep going. Being a positive force in a community is enough of a reward! If you’re looking for a career, it’s essential. Meet-ups Building on the communities point above, developer communities associated with larger frameworks or big companies like FAAANG tend to have meet-ups in most cities. Join them. Contribute, even if it just means doing the most basic helpful tasks. I’ve run several meet-ups, and the most valuable people are the ones that don’t consider anything beneath them. They’ll take on the most mundane tasks. That helps them network with peers and, importantly, allows organizers to get to know them well. I recall speaking at the Google Developer Group (GDG) in Seattle, where a young woman approached me. She had attended many meet-ups over the final two years of her degree in Software Engineering at a nearby University. She was just about to graduate and soon to start a job at an excellent local tech company. How she did it? She attended every Microsoft, Amazon, Google, Apple, and independent meet-up she could find. Recruiters also participated at these, and one from that tech company got to know her and eventually hired her. A textbook example of someone doing it the right way. Expert Groups Most companies like Google, Microsoft, and others have a formal community of folks deemed ‘experts in the field. In Google, we call then GDEs (Google Developer Experts). These folks usually come through the communities and meet-ups and get known as experts in an area. They get nominated to be reviewed by the company, usually with an interview, before achieving this status. Being recognized as an expert by the likes of Google or Microsoft helps you find a career anywhere. Becoming a recognized expert like this can be a great career launchpad. It’s worth investigating! Referrals Referrals are possibly the most powerful tool in getting recruited by larger companies. They’re also one of the most misunderstood. First of all, here’s what not to do. Don’t reach out to people you do not know, asking them for a referral into their company. Seriously, please don’t do it. I’ve seen advice to the opposite, where people say it’s just like going and knocking on doors until someone answers, as a great sales technique. That’s a bad analogy. It’s more like approaching a stranger at a bar and asking them for an introduction to all their single friends telling them how awesome you are, in the hope of getting a date. Creepy, right? That’s more what spamming someone and asking for a referral resembles. People argue that one should be more lenient on folks who are desperate to find a job. I argue back that desperation isn’t an excuse for laziness and pushing the burden onto others. If one genuinely is desperate (as I have been many times), one should be willing to burn the calories to do it right and not look for damaging shortcuts. Why? Because a referral is supposed to be just that. Telling the company about somebody you know who would be an excellent fit for available jobs. Referring a person means you are putting your reputation on the line to give them a chance at a career. You don’t just ask a random stranger to do that. I get at least ten people a day asking me to do that. Oh, and think about it deeper – it demonstrates very poor judgment to approach a stranger like this. What happens if you then apply for the company, your resume ends up on their desk, they look you up on linked in, and they see that you once spammed them for a referral? They’ll likely move onto the next resume. And what if the person you spammed is a recruiter? They’ll probably flag your resume so that nobody in the company will be bothered by someone with such poor judgment. So please don’t do it. What can you do? Well, consider all of the above – being a part of a community, participating in meet-ups, by joining expert groups. What happens then? People from your target company will get to know you and will be able to refer you. Because a referral usually involves them answering questions about how they know you, how long they’ve known you, in what capacity, what your abilities are, etc. And if they only know you as a random spammer - well, is that the reputation you want to build? But if it’s a good referral that answers all of the above questions well – it can get you to the front of the line for your resume to be read. It can bypass a lot of the stuff that we spoke about in the last article. I applied for Google three times before they hired me. The third time came through a referral from a skip-level manager who knew me at a previous company. Despite me flunking some of the interviews, they still hired me! That’s how powerful a referral can be! So, please, take the time to do it right.</summary></entry><entry><title type="html">Tips for getting a job in tech - Part One - Your Resume</title><link href="/2021/06/02/resumetips.html" rel="alternate" type="text/html" title="Tips for getting a job in tech - Part One - Your Resume" /><published>2021-06-02T00:00:00-07:00</published><updated>2021-06-02T00:00:00-07:00</updated><id>/2021/06/02/resumetips</id><content type="html" xml:base="/2021/06/02/resumetips.html">&lt;p&gt;A few tips to explore how to build a better resume. This advice won’t guarantee you to pass any tech interview, but many common pitfalls cause people to fail. As always, free advice is seldom cheap, so this might involve you spending quite a bit of effort! :)&lt;/p&gt;

&lt;h2 id=&quot;keywords-and-filtering&quot;&gt;Keywords and Filtering&lt;/h2&gt;
&lt;p&gt;Let’s start with the first and most obvious one: Your resume or CV.&lt;/p&gt;

&lt;p&gt;When applying to large companies, there’s often an automated system that processes your application. Often these use keyword matching to ‘score’ your resume as being appropriate to a job. Hiring managers have limited time, and they can’t read &lt;em&gt;every&lt;/em&gt; resume, so an initial automated triage is required. As such, you &lt;em&gt;must read the job description&lt;/em&gt; and make sure that your resume is keyword-heavy, matching the job description.&lt;/p&gt;

&lt;p&gt;If it asks for somebody who has senior-level knowledge in Kotlin, you can’t say that you have equivalent knowledge in Java and have the machine understand that! So, while you cannot put every iota of your abilities on a 1-2 page resume, you &lt;em&gt;should&lt;/em&gt; tailor it to a specific job application, making it keyword heavy.&lt;/p&gt;

&lt;p&gt;So, for example, if they’re looking for a mobile developer who knows the backend, and you’re primarily a backend developer who knows mobile, be sure to tailor the resume to be heavier on your mobile background.&lt;/p&gt;

&lt;p&gt;This brings me to the next point&lt;/p&gt;

&lt;h2 id=&quot;dont-lie&quot;&gt;Don’t lie&lt;/h2&gt;
&lt;p&gt;It might be tempting to pad your stats, lie about abilities, fake some experience or education, or something else while tailoring your resume. Don’t do this. You WILL get caught, and the &lt;em&gt;best&lt;/em&gt; that could happen is that they reject and forget you, and you just wasted your time and energy. Also, tech is a smaller community than you might think, and news &lt;em&gt;does&lt;/em&gt; get around. If you can’t be honest on a resume, you can’t be honest on the job, and it will hurt you in the long run. And if someone tells you ‘everybody does it,’ don’t listen to them!&lt;/p&gt;

&lt;h2 id=&quot;education-is-important-but-not-essential&quot;&gt;Education is important, but not essential&lt;/h2&gt;
&lt;p&gt;I get many questions about ‘Do I need an MS or Ph.D.?’, and the simple answer is ‘it depends. Every job is different. Read the requirements closely. There’s usually a ‘minimum’ and a ‘preferred’ set of qualifications. If you’re somewhere close to these, go ahead and be confident that you’re ok. Often they will say ‘…or equivalent experience’, which is your opportunity to show what you can do &lt;em&gt;beyond&lt;/em&gt; education. I find that type of wording is more common nowadays.&lt;/p&gt;

&lt;p&gt;While putting education on your resume, you don’t need to be exhaustive. Listing every class you did makes it hard to read. Simply tell the degree, the school, the date granted, and any particular things you’d like to call out. For example, if you did a significant project that matches the type of thing they’re looking for in the job, call that out! But you are wasting resume space by listing every single class :)&lt;/p&gt;

&lt;h2 id=&quot;proof-of-ability-is-easier-than-ever-take-advantage-of-this&quot;&gt;Proof of ability is easier than ever. Take advantage of this!&lt;/h2&gt;
&lt;p&gt;It’s easy nowadays for you to &lt;em&gt;show&lt;/em&gt; that you can do the job. You can blog, post to YouTube, open-source code on GitHub, enter coding competitions, get rankings, and so much more. When I read a resume, this is the &lt;em&gt;first&lt;/em&gt; thing I will look at before education or experience. It’s free, easy to do, and you don’t have to be in the top charts. There’s another hidden bonus here – often, tech interviews will have tech questions. If you have code in your GitHub, I think most tech interviewers would prefer to ask about that instead of coming up with hypothetical scenarios. So now, you’re speaking from a position of strength!&lt;/p&gt;

&lt;p&gt;…which brings me to the next point:&lt;/p&gt;

&lt;h2 id=&quot;dont-lie-about-code&quot;&gt;Don’t Lie about Code&lt;/h2&gt;
&lt;p&gt;Just like lying on a resume, don’t put other people’s code in your GitHub and try to pass it off as your own. If you’re asked about what is supposed to be YOUR CODE and have no deep idea what it does, you WILL get caught. Write your own code, or extend other people’s code, but put your work down as your work, and don’t take from someone else!&lt;/p&gt;

&lt;p&gt;Finally, find a good friend who will be honest with you and who will tear your resume apart, finding flaws so that you can fix them! :)&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="careers" /><summary type="html">A few tips to explore how to build a better resume. This advice won’t guarantee you to pass any tech interview, but many common pitfalls cause people to fail. As always, free advice is seldom cheap, so this might involve you spending quite a bit of effort! :) Keywords and Filtering Let’s start with the first and most obvious one: Your resume or CV. When applying to large companies, there’s often an automated system that processes your application. Often these use keyword matching to ‘score’ your resume as being appropriate to a job. Hiring managers have limited time, and they can’t read every resume, so an initial automated triage is required. As such, you must read the job description and make sure that your resume is keyword-heavy, matching the job description. If it asks for somebody who has senior-level knowledge in Kotlin, you can’t say that you have equivalent knowledge in Java and have the machine understand that! So, while you cannot put every iota of your abilities on a 1-2 page resume, you should tailor it to a specific job application, making it keyword heavy. So, for example, if they’re looking for a mobile developer who knows the backend, and you’re primarily a backend developer who knows mobile, be sure to tailor the resume to be heavier on your mobile background. This brings me to the next point Don’t lie It might be tempting to pad your stats, lie about abilities, fake some experience or education, or something else while tailoring your resume. Don’t do this. You WILL get caught, and the best that could happen is that they reject and forget you, and you just wasted your time and energy. Also, tech is a smaller community than you might think, and news does get around. If you can’t be honest on a resume, you can’t be honest on the job, and it will hurt you in the long run. And if someone tells you ‘everybody does it,’ don’t listen to them! Education is important, but not essential I get many questions about ‘Do I need an MS or Ph.D.?’, and the simple answer is ‘it depends. Every job is different. Read the requirements closely. There’s usually a ‘minimum’ and a ‘preferred’ set of qualifications. If you’re somewhere close to these, go ahead and be confident that you’re ok. Often they will say ‘…or equivalent experience’, which is your opportunity to show what you can do beyond education. I find that type of wording is more common nowadays. While putting education on your resume, you don’t need to be exhaustive. Listing every class you did makes it hard to read. Simply tell the degree, the school, the date granted, and any particular things you’d like to call out. For example, if you did a significant project that matches the type of thing they’re looking for in the job, call that out! But you are wasting resume space by listing every single class :) Proof of ability is easier than ever. Take advantage of this! It’s easy nowadays for you to show that you can do the job. You can blog, post to YouTube, open-source code on GitHub, enter coding competitions, get rankings, and so much more. When I read a resume, this is the first thing I will look at before education or experience. It’s free, easy to do, and you don’t have to be in the top charts. There’s another hidden bonus here – often, tech interviews will have tech questions. If you have code in your GitHub, I think most tech interviewers would prefer to ask about that instead of coming up with hypothetical scenarios. So now, you’re speaking from a position of strength! …which brings me to the next point: Don’t Lie about Code Just like lying on a resume, don’t put other people’s code in your GitHub and try to pass it off as your own. If you’re asked about what is supposed to be YOUR CODE and have no deep idea what it does, you WILL get caught. Write your own code, or extend other people’s code, but put your work down as your work, and don’t take from someone else! Finally, find a good friend who will be honest with you and who will tear your resume apart, finding flaws so that you can fix them! :)</summary></entry><entry><title type="html">Jobs in ML - Getting the TensorFlow Certificate</title><link href="/2021/06/01/certificate.html" rel="alternate" type="text/html" title="Jobs in ML - Getting the TensorFlow Certificate" /><published>2021-06-01T00:00:00-07:00</published><updated>2021-06-01T00:00:00-07:00</updated><id>/2021/06/01/certificate</id><content type="html" xml:base="/2021/06/01/certificate.html">&lt;p&gt;Almost daily, I get asked on LinkedIn what skills somebody needs to get a job in ML.&lt;/p&gt;

&lt;p&gt;It’s not an easy question to answer because almost everybody hiring in ML will require different skills. So I wanted to approach it a little differently and explore what skills one needs to build a platform for a &lt;em&gt;career&lt;/em&gt; in ML.&lt;/p&gt;

&lt;h2 id=&quot;skills-to-be-an-ml-developer&quot;&gt;Skills to be an ML Developer&lt;/h2&gt;
&lt;p&gt;But then I had to slice that a little further – there are many types of ML careers, from data scientists to product managers and beyond. So I narrowed my focus on my area of expertise: Software Developers.&lt;/p&gt;

&lt;p&gt;So, if you want to be a &lt;em&gt;software developer&lt;/em&gt; in the realm of ML, what do you need to be able to do &lt;em&gt;in addition to&lt;/em&gt; the traditional skills you need as a dev.&lt;/p&gt;

&lt;p&gt;It started with doing a job search myself. I explored lots of different job sites and different job listings. What I found was that the vast majority of people who were hiring software developers in ML roles needed at least 1 of the following three broad skillsets:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: Including Deep Neural Networks, Convolutional Neural Networks, image processing, mobile skills, and more&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;: Including GRUs, LSTMs, Convolutional Neural Networks, string management, tokenization, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sequence Modelling&lt;/strong&gt;: Including Convolutional Neural Networks, time-series management, basic statistics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Beyond these specific needs, of course, are the general ML skills required as a foundation: data management, feature engineering, model architecture setup, choosing appropriate loss functions and optimizers, overfitting/underfitting, python, NumPy, TensorFlow, etc.&lt;/p&gt;

&lt;p&gt;You will also need a foundation of software engineering and coding, at least in Python, but preferably in other domain-specific languages. From experience, I have also found that you will need at least entry-level ability to build distributed applications, understanding web client and server architecture.&lt;/p&gt;

&lt;p&gt;With all of that in mind, Google launched the &lt;a href=&quot;https://www.tensorflow.org/certificate&quot;&gt;TensorFlow Developer Certificate&lt;/a&gt;, designed around testing the skills I mentioned above. Holders of this certificate pass a 5-hour exam, where they go from a skeleton of code to a fully working model in each of the above three scenarios, as well as a few more foundational ones. I helped write the &lt;a href=&quot;https://www.tensorflow.org/site-assets/downloads/marketing/cert/TF_Certificate_Candidate_Handbook.pdf&quot;&gt;Candidate Handbook&lt;/a&gt; which I’d strongly recommend reading.&lt;/p&gt;

&lt;p&gt;This certificate is a &lt;em&gt;really&lt;/em&gt; powerful way of demonstrating that you have all the above skills. It’s a great way to show employers that you have the foundational knowledge, but it’s also a great way of building a career beyond your first job.&lt;/p&gt;

&lt;p&gt;Being a certificate holder also grants entry to the &lt;a href=&quot;https://developers.google.com/certification/directory/tensorflow&quot;&gt;TensorFlow Certificate Network&lt;/a&gt; where prospective employers can find &lt;em&gt;you&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;preparing-for-the-exam&quot;&gt;Preparing for the exam&lt;/h2&gt;
&lt;p&gt;We also worked with &lt;a href=&quot;https://deeplearning.ai&quot;&gt;deeplearning.ai&lt;/a&gt; and &lt;a href=&quot;https://coursera.org&quot;&gt;Coursera&lt;/a&gt; to produce specializations that teach the material you need to know to take this exam. You can find the beginner course &lt;a href=&quot;https://www.coursera.org/learn/introduction-tensorflow&quot;&gt;here&lt;/a&gt; if you want to experiment before going deeper! My book &lt;a href=&quot;https://www.oreilly.com/library/view/ai-and-machine/9781492078180/&quot;&gt;AI and Machine Learning for Coders&lt;/a&gt; also covers this syllabus and helps prepare for the exam!&lt;/p&gt;

&lt;h2 id=&quot;widening-access&quot;&gt;Widening access&lt;/h2&gt;
&lt;p&gt;I’m also passionate about widening access to the traditionally underserved, so people of diverse backgrounds, experiences, geographies, and perspectives can transform machine learning for the better. An equitable approach to AI and ML is essential for us all to succeed. We’ve already seen great dividends with projects worldwide and a massive global developer community helping push the platform forward. 
With that in mind, there is a stipend available, and successful applications will get no-cost access to the TensorFlow Professional Certificate courses at Coursera, as well as a discount on the exam fee.&lt;/p&gt;

&lt;p&gt;If you’re exploring expanding your software developer toolkit with Machine Learning, I strongly recommend checking it out at https://www.tensorflow.org/certificate&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="careers" /><summary type="html">Almost daily, I get asked on LinkedIn what skills somebody needs to get a job in ML. It’s not an easy question to answer because almost everybody hiring in ML will require different skills. So I wanted to approach it a little differently and explore what skills one needs to build a platform for a career in ML. Skills to be an ML Developer But then I had to slice that a little further – there are many types of ML careers, from data scientists to product managers and beyond. So I narrowed my focus on my area of expertise: Software Developers. So, if you want to be a software developer in the realm of ML, what do you need to be able to do in addition to the traditional skills you need as a dev. It started with doing a job search myself. I explored lots of different job sites and different job listings. What I found was that the vast majority of people who were hiring software developers in ML roles needed at least 1 of the following three broad skillsets: Computer Vision: Including Deep Neural Networks, Convolutional Neural Networks, image processing, mobile skills, and more Natural Language Processing: Including GRUs, LSTMs, Convolutional Neural Networks, string management, tokenization, etc. Sequence Modelling: Including Convolutional Neural Networks, time-series management, basic statistics. Beyond these specific needs, of course, are the general ML skills required as a foundation: data management, feature engineering, model architecture setup, choosing appropriate loss functions and optimizers, overfitting/underfitting, python, NumPy, TensorFlow, etc. You will also need a foundation of software engineering and coding, at least in Python, but preferably in other domain-specific languages. From experience, I have also found that you will need at least entry-level ability to build distributed applications, understanding web client and server architecture. With all of that in mind, Google launched the TensorFlow Developer Certificate, designed around testing the skills I mentioned above. Holders of this certificate pass a 5-hour exam, where they go from a skeleton of code to a fully working model in each of the above three scenarios, as well as a few more foundational ones. I helped write the Candidate Handbook which I’d strongly recommend reading. This certificate is a really powerful way of demonstrating that you have all the above skills. It’s a great way to show employers that you have the foundational knowledge, but it’s also a great way of building a career beyond your first job. Being a certificate holder also grants entry to the TensorFlow Certificate Network where prospective employers can find you. Preparing for the exam We also worked with deeplearning.ai and Coursera to produce specializations that teach the material you need to know to take this exam. You can find the beginner course here if you want to experiment before going deeper! My book AI and Machine Learning for Coders also covers this syllabus and helps prepare for the exam! Widening access I’m also passionate about widening access to the traditionally underserved, so people of diverse backgrounds, experiences, geographies, and perspectives can transform machine learning for the better. An equitable approach to AI and ML is essential for us all to succeed. We’ve already seen great dividends with projects worldwide and a massive global developer community helping push the platform forward. With that in mind, there is a stipend available, and successful applications will get no-cost access to the TensorFlow Professional Certificate courses at Coursera, as well as a discount on the exam fee. If you’re exploring expanding your software developer toolkit with Machine Learning, I strongly recommend checking it out at https://www.tensorflow.org/certificate</summary></entry><entry><title type="html">Learning TensorFlow.js - Book Review</title><link href="/2021/05/28/tfjsbook.html" rel="alternate" type="text/html" title="Learning TensorFlow.js - Book Review" /><published>2021-05-28T00:00:00-07:00</published><updated>2021-05-28T00:00:00-07:00</updated><id>/2021/05/28/tfjsbook</id><content type="html" xml:base="/2021/05/28/tfjsbook.html">&lt;p&gt;&lt;img src=&quot;/assets/gantcover.jpg&quot; alt=&quot;Gant Cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was delighted to get an advance copy of Gant Laborde’s “Learning TensorFlow.js” from O’Reilly and privileged to have written the foreword.&lt;/p&gt;

&lt;p&gt;Gant has done something special with this book. In just 300 pages, he takes you end-to-end, in-depth through everything you need to know from an introduction to AI, understanding tensors, using them in the browser, deploying them, and more.&lt;/p&gt;

&lt;p&gt;It ends with a capstone project (what a great idea, I might have to steal it for my next book!), where you can use Machine Learning to convert an image into a set of dice, like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dice.jpg&quot; alt=&quot;Gant Dice Sculpture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How much fun is that?&lt;/p&gt;

&lt;p&gt;I &lt;em&gt;love&lt;/em&gt; this book because it is for a different audience than the traditional ML one. It starts with a great introduction to AI and then tells you about TensorFlow.js and how you can use it to build Machine Learning apps. Then, the mystery of Tensors is cracked open, and Gant leads you through some detailed examples of how you can convert images into Tensors for training and inference.&lt;/p&gt;

&lt;p&gt;It guides you through the three main ways to get a working model.&lt;/p&gt;

&lt;p&gt;First, you can find an existing model from TensorFlow Hub, and in Chapter 5, Gant leads you through using the inception model in JavaScript. Inception isn’t any toy model, though – it is a Convolutional Neural Network designed for image analysis and object detection. It’s not that long ago that it was state-of-the-art in research. And now it’s available in JavaScript!&lt;/p&gt;

&lt;p&gt;Or, you can create your model from scratch, and Gant takes you through the code for defining layers, with deep neural networks to help predict numeric data (such as the famous titanic dataset) or Convolutional Neural networks for image classification.&lt;/p&gt;

&lt;p&gt;Finally, there’s Transfer Learning, which could be the most exciting method for most developers, where you have a hybrid of both of the previous methods. You can stand on the shoulders of giants by using parts of an existing model, like Inception, but catered for your own needs.&lt;/p&gt;

&lt;p&gt;When I started my Machine Learning journey, one frustration I had was that there was lots of material for creating models but relatively little for using them. The tutorial would end with a validation set showing how accurate the model was, and then it would move on to the next thing! I am delighted to say that this book does not fall into that pattern! So, if you want to build a browser-based app that uses a model, you’ll get lots of code showing you how!&lt;/p&gt;

&lt;p&gt;For example, Chapter 6 shows you how to use the webcam in the browser, capturing frames and passing them to a model for classification. Chapter 10 shows you how to create a basic sketchpad for drawing images that a model can interpret.&lt;/p&gt;

&lt;p&gt;Whether you’re an experienced Machine Learning expert, looking to see how to apply JavaScript to help solve your problems, or a JavaScript developer who wants to enter the wonderful world of ML, this book is for you. &lt;a href=&quot;https://www.oreilly.com/library/view/learning-tensorflowjs/9781492090786/&quot;&gt;Check it out!&lt;/a&gt;&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="tensorflow" /><category term="ai" /><category term="books" /><summary type="html">I was delighted to get an advance copy of Gant Laborde’s “Learning TensorFlow.js” from O’Reilly and privileged to have written the foreword. Gant has done something special with this book. In just 300 pages, he takes you end-to-end, in-depth through everything you need to know from an introduction to AI, understanding tensors, using them in the browser, deploying them, and more. It ends with a capstone project (what a great idea, I might have to steal it for my next book!), where you can use Machine Learning to convert an image into a set of dice, like this: How much fun is that? I love this book because it is for a different audience than the traditional ML one. It starts with a great introduction to AI and then tells you about TensorFlow.js and how you can use it to build Machine Learning apps. Then, the mystery of Tensors is cracked open, and Gant leads you through some detailed examples of how you can convert images into Tensors for training and inference. It guides you through the three main ways to get a working model. First, you can find an existing model from TensorFlow Hub, and in Chapter 5, Gant leads you through using the inception model in JavaScript. Inception isn’t any toy model, though – it is a Convolutional Neural Network designed for image analysis and object detection. It’s not that long ago that it was state-of-the-art in research. And now it’s available in JavaScript! Or, you can create your model from scratch, and Gant takes you through the code for defining layers, with deep neural networks to help predict numeric data (such as the famous titanic dataset) or Convolutional Neural networks for image classification. Finally, there’s Transfer Learning, which could be the most exciting method for most developers, where you have a hybrid of both of the previous methods. You can stand on the shoulders of giants by using parts of an existing model, like Inception, but catered for your own needs. When I started my Machine Learning journey, one frustration I had was that there was lots of material for creating models but relatively little for using them. The tutorial would end with a validation set showing how accurate the model was, and then it would move on to the next thing! I am delighted to say that this book does not fall into that pattern! So, if you want to build a browser-based app that uses a model, you’ll get lots of code showing you how! For example, Chapter 6 shows you how to use the webcam in the browser, capturing frames and passing them to a model for classification. Chapter 10 shows you how to create a basic sketchpad for drawing images that a model can interpret. Whether you’re an experienced Machine Learning expert, looking to see how to apply JavaScript to help solve your problems, or a JavaScript developer who wants to enter the wonderful world of ML, this book is for you. Check it out!</summary></entry></feed>