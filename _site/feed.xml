<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-06-02T08:34:39-07:00</updated><id>/feed.xml</id><title type="html">Laurence Moroney - The AI Guy.</title><subtitle>This is the blog and general web site of Laurence Moroney, author, lecturer, teacher and AI lead at Google with Musings on AI, Quantum and other future tech
</subtitle><author><name>Laurence Moroney</name></author><entry><title type="html">Tips for getting a job in tech - Part One - Your Resume</title><link href="/2021/06/02/resumetips.html" rel="alternate" type="text/html" title="Tips for getting a job in tech - Part One - Your Resume" /><published>2021-06-02T00:00:00-07:00</published><updated>2021-06-02T00:00:00-07:00</updated><id>/2021/06/02/resumetips</id><content type="html" xml:base="/2021/06/02/resumetips.html">&lt;p&gt;A few tips to explore how to build a better resume. This advice won’t guarantee you to pass any tech interview, but many common pitfalls cause people to fail. As always, free advice is seldom cheap, so this might involve you spending quite a bit of effort! :)&lt;/p&gt;

&lt;h2 id=&quot;keywords-and-filtering&quot;&gt;Keywords and Filtering&lt;/h2&gt;
&lt;p&gt;Let’s start with the first and most obvious one: Your resume or CV.&lt;/p&gt;

&lt;p&gt;When applying to large companies, there’s often an automated system that processes your application. Often these use keyword matching to ‘score’ your resume as being appropriate to a job. Hiring managers have limited time, and they can’t read &lt;em&gt;every&lt;/em&gt; resume, so an initial automated triage is required. As such, you &lt;em&gt;must read the job description&lt;/em&gt; and make sure that your resume is keyword-heavy, matching the job description.&lt;/p&gt;

&lt;p&gt;If it asks for somebody who has senior-level knowledge in Kotlin, you can’t say that you have equivalent knowledge in Java and have the machine understand that! So, while you cannot put every iota of your abilities on a 1-2 page resume, you &lt;em&gt;should&lt;/em&gt; tailor it to a specific job application, making it keyword heavy.&lt;/p&gt;

&lt;p&gt;So, for example, if they’re looking for a mobile developer who knows the backend, and you’re primarily a backend developer who knows mobile, be sure to tailor the resume to be heavier on your mobile background.&lt;/p&gt;

&lt;p&gt;This brings me to the next point&lt;/p&gt;

&lt;h2 id=&quot;dont-lie&quot;&gt;Don’t lie&lt;/h2&gt;
&lt;p&gt;It might be tempting to pad your stats, lie about abilities, fake some experience or education, or something else while tailoring your resume. Don’t do this. You WILL get caught, and the &lt;em&gt;best&lt;/em&gt; that could happen is that they reject and forget you, and you just wasted your time and energy. Also, tech is a smaller community than you might think, and news &lt;em&gt;does&lt;/em&gt; get around. If you can’t be honest on a resume, you can’t be honest on the job, and it will hurt you in the long run. And if someone tells you ‘everybody does it,’ don’t listen to them!&lt;/p&gt;

&lt;h2 id=&quot;education-is-important-but-not-essential&quot;&gt;Education is important, but not essential&lt;/h2&gt;
&lt;p&gt;I get many questions about ‘Do I need an MS or Ph.D.?’, and the simple answer is ‘it depends. Every job is different. Read the requirements closely. There’s usually a ‘minimum’ and a ‘preferred’ set of qualifications. If you’re somewhere close to these, go ahead and be confident that you’re ok. Often they will say ‘…or equivalent experience’, which is your opportunity to show what you can do &lt;em&gt;beyond&lt;/em&gt; education. I find that type of wording is more common nowadays.&lt;/p&gt;

&lt;p&gt;While putting education on your resume, you don’t need to be exhaustive. Listing every class you did makes it hard to read. Simply tell the degree, the school, the date granted, and any particular things you’d like to call out. For example, if you did a significant project that matches the type of thing they’re looking for in the job, call that out! But you are wasting resume space by listing every single class :)&lt;/p&gt;

&lt;h2 id=&quot;proof-of-ability-is-easier-than-ever-take-advantage-of-this&quot;&gt;Proof of ability is easier than ever. Take advantage of this!&lt;/h2&gt;
&lt;p&gt;It’s easy nowadays for you to &lt;em&gt;show&lt;/em&gt; that you can do the job. You can blog, post to YouTube, open-source code on GitHub, enter coding competitions, get rankings, and so much more. When I read a resume, this is the &lt;em&gt;first&lt;/em&gt; thing I will look at before education or experience. It’s free, easy to do, and you don’t have to be in the top charts. There’s another hidden bonus here – often, tech interviews will have tech questions. If you have code in your GitHub, I think most tech interviewers would prefer to ask about that instead of coming up with hypothetical scenarios. So now, you’re speaking from a position of strength!&lt;/p&gt;

&lt;p&gt;…which brings me to the next point:&lt;/p&gt;

&lt;h2 id=&quot;dont-lie-about-code&quot;&gt;Don’t Lie about Code&lt;/h2&gt;
&lt;p&gt;Just like lying on a resume, don’t put other people’s code in your GitHub and try to pass it off as your own. If you’re asked about what is supposed to be YOUR CODE and have no deep idea what it does, you WILL get caught. Write your own code, or extend other people’s code, but put your work down as your work, and don’t take from someone else!&lt;/p&gt;

&lt;p&gt;Finally, find a good friend who will be honest with you and who will tear your resume apart, finding flaws so that you can fix them! :)&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="careers" /><summary type="html">A few tips to explore how to build a better resume. This advice won’t guarantee you to pass any tech interview, but many common pitfalls cause people to fail. As always, free advice is seldom cheap, so this might involve you spending quite a bit of effort! :) Keywords and Filtering Let’s start with the first and most obvious one: Your resume or CV. When applying to large companies, there’s often an automated system that processes your application. Often these use keyword matching to ‘score’ your resume as being appropriate to a job. Hiring managers have limited time, and they can’t read every resume, so an initial automated triage is required. As such, you must read the job description and make sure that your resume is keyword-heavy, matching the job description. If it asks for somebody who has senior-level knowledge in Kotlin, you can’t say that you have equivalent knowledge in Java and have the machine understand that! So, while you cannot put every iota of your abilities on a 1-2 page resume, you should tailor it to a specific job application, making it keyword heavy. So, for example, if they’re looking for a mobile developer who knows the backend, and you’re primarily a backend developer who knows mobile, be sure to tailor the resume to be heavier on your mobile background. This brings me to the next point Don’t lie It might be tempting to pad your stats, lie about abilities, fake some experience or education, or something else while tailoring your resume. Don’t do this. You WILL get caught, and the best that could happen is that they reject and forget you, and you just wasted your time and energy. Also, tech is a smaller community than you might think, and news does get around. If you can’t be honest on a resume, you can’t be honest on the job, and it will hurt you in the long run. And if someone tells you ‘everybody does it,’ don’t listen to them! Education is important, but not essential I get many questions about ‘Do I need an MS or Ph.D.?’, and the simple answer is ‘it depends. Every job is different. Read the requirements closely. There’s usually a ‘minimum’ and a ‘preferred’ set of qualifications. If you’re somewhere close to these, go ahead and be confident that you’re ok. Often they will say ‘…or equivalent experience’, which is your opportunity to show what you can do beyond education. I find that type of wording is more common nowadays. While putting education on your resume, you don’t need to be exhaustive. Listing every class you did makes it hard to read. Simply tell the degree, the school, the date granted, and any particular things you’d like to call out. For example, if you did a significant project that matches the type of thing they’re looking for in the job, call that out! But you are wasting resume space by listing every single class :) Proof of ability is easier than ever. Take advantage of this! It’s easy nowadays for you to show that you can do the job. You can blog, post to YouTube, open-source code on GitHub, enter coding competitions, get rankings, and so much more. When I read a resume, this is the first thing I will look at before education or experience. It’s free, easy to do, and you don’t have to be in the top charts. There’s another hidden bonus here – often, tech interviews will have tech questions. If you have code in your GitHub, I think most tech interviewers would prefer to ask about that instead of coming up with hypothetical scenarios. So now, you’re speaking from a position of strength! …which brings me to the next point: Don’t Lie about Code Just like lying on a resume, don’t put other people’s code in your GitHub and try to pass it off as your own. If you’re asked about what is supposed to be YOUR CODE and have no deep idea what it does, you WILL get caught. Write your own code, or extend other people’s code, but put your work down as your work, and don’t take from someone else! Finally, find a good friend who will be honest with you and who will tear your resume apart, finding flaws so that you can fix them! :)</summary></entry><entry><title type="html">Jobs in ML - Getting the TensorFlow Certificate</title><link href="/2021/06/01/certificate.html" rel="alternate" type="text/html" title="Jobs in ML - Getting the TensorFlow Certificate" /><published>2021-06-01T00:00:00-07:00</published><updated>2021-06-01T00:00:00-07:00</updated><id>/2021/06/01/certificate</id><content type="html" xml:base="/2021/06/01/certificate.html">&lt;p&gt;Almost daily, I get asked on LinkedIn what skills somebody needs to get a job in ML.&lt;/p&gt;

&lt;p&gt;It’s not an easy question to answer because almost everybody hiring in ML will require different skills. So I wanted to approach it a little differently and explore what skills one needs to build a platform for a &lt;em&gt;career&lt;/em&gt; in ML.&lt;/p&gt;

&lt;h2 id=&quot;skills-to-be-an-ml-developer&quot;&gt;Skills to be an ML Developer&lt;/h2&gt;
&lt;p&gt;But then I had to slice that a little further – there are many types of ML careers, from data scientists to product managers and beyond. So I narrowed my focus on my area of expertise: Software Developers.&lt;/p&gt;

&lt;p&gt;So, if you want to be a &lt;em&gt;software developer&lt;/em&gt; in the realm of ML, what do you need to be able to do &lt;em&gt;in addition to&lt;/em&gt; the traditional skills you need as a dev.&lt;/p&gt;

&lt;p&gt;It started with doing a job search myself. I explored lots of different job sites and different job listings. What I found was that the vast majority of people who were hiring software developers in ML roles needed at least 1 of the following three broad skillsets:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: Including Deep Neural Networks, Convolutional Neural Networks, image processing, mobile skills, and more&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;: Including GRUs, LSTMs, Convolutional Neural Networks, string management, tokenization, etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sequence Modelling&lt;/strong&gt;: Including Convolutional Neural Networks, time-series management, basic statistics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Beyond these specific needs, of course, are the general ML skills required as a foundation: data management, feature engineering, model architecture setup, choosing appropriate loss functions and optimizers, overfitting/underfitting, python, NumPy, TensorFlow, etc.&lt;/p&gt;

&lt;p&gt;You will also need a foundation of software engineering and coding, at least in Python, but preferably in other domain-specific languages. From experience, I have also found that you will need at least entry-level ability to build distributed applications, understanding web client and server architecture.&lt;/p&gt;

&lt;p&gt;With all of that in mind, Google launched the &lt;a href=&quot;https://www.tensorflow.org/certificate&quot;&gt;TensorFlow Developer Certificate&lt;/a&gt;, designed around testing the skills I mentioned above. Holders of this certificate pass a 5-hour exam, where they go from a skeleton of code to a fully working model in each of the above three scenarios, as well as a few more foundational ones. I helped write the &lt;a href=&quot;https://www.tensorflow.org/site-assets/downloads/marketing/cert/TF_Certificate_Candidate_Handbook.pdf&quot;&gt;Candidate Handbook&lt;/a&gt; which I’d strongly recommend reading.&lt;/p&gt;

&lt;p&gt;This certificate is a &lt;em&gt;really&lt;/em&gt; powerful way of demonstrating that you have all the above skills. It’s a great way to show employers that you have the foundational knowledge, but it’s also a great way of building a career beyond your first job.&lt;/p&gt;

&lt;p&gt;Being a certificate holder also grants entry to the &lt;a href=&quot;https://developers.google.com/certification/directory/tensorflow&quot;&gt;TensorFlow Certificate Network&lt;/a&gt; where prospective employers can find &lt;em&gt;you&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;preparing-for-the-exam&quot;&gt;Preparing for the exam&lt;/h2&gt;
&lt;p&gt;We also worked with &lt;a href=&quot;https://deeplearning.ai&quot;&gt;deeplearning.ai&lt;/a&gt; and &lt;a href=&quot;https://coursera.org&quot;&gt;Coursera&lt;/a&gt; to produce specializations that teach the material you need to know to take this exam. You can find the beginner course &lt;a href=&quot;https://www.coursera.org/learn/introduction-tensorflow&quot;&gt;here&lt;/a&gt; if you want to experiment before going deeper! My book &lt;a href=&quot;https://www.oreilly.com/library/view/ai-and-machine/9781492078180/&quot;&gt;AI and Machine Learning for Coders&lt;/a&gt; also covers this syllabus and helps prepare for the exam!&lt;/p&gt;

&lt;h2 id=&quot;widening-access&quot;&gt;Widening access&lt;/h2&gt;
&lt;p&gt;I’m also passionate about widening access to the traditionally underserved, so people of diverse backgrounds, experiences, geographies, and perspectives can transform machine learning for the better. An equitable approach to AI and ML is essential for us all to succeed. We’ve already seen great dividends with projects worldwide and a massive global developer community helping push the platform forward. 
With that in mind, there is a stipend available, and successful applications will get no-cost access to the TensorFlow Professional Certificate courses at Coursera, as well as a discount on the exam fee.&lt;/p&gt;

&lt;p&gt;If you’re exploring expanding your software developer toolkit with Machine Learning, I strongly recommend checking it out at https://www.tensorflow.org/certificate&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="careers" /><summary type="html">Almost daily, I get asked on LinkedIn what skills somebody needs to get a job in ML. It’s not an easy question to answer because almost everybody hiring in ML will require different skills. So I wanted to approach it a little differently and explore what skills one needs to build a platform for a career in ML. Skills to be an ML Developer But then I had to slice that a little further – there are many types of ML careers, from data scientists to product managers and beyond. So I narrowed my focus on my area of expertise: Software Developers. So, if you want to be a software developer in the realm of ML, what do you need to be able to do in addition to the traditional skills you need as a dev. It started with doing a job search myself. I explored lots of different job sites and different job listings. What I found was that the vast majority of people who were hiring software developers in ML roles needed at least 1 of the following three broad skillsets: Computer Vision: Including Deep Neural Networks, Convolutional Neural Networks, image processing, mobile skills, and more Natural Language Processing: Including GRUs, LSTMs, Convolutional Neural Networks, string management, tokenization, etc. Sequence Modelling: Including Convolutional Neural Networks, time-series management, basic statistics. Beyond these specific needs, of course, are the general ML skills required as a foundation: data management, feature engineering, model architecture setup, choosing appropriate loss functions and optimizers, overfitting/underfitting, python, NumPy, TensorFlow, etc. You will also need a foundation of software engineering and coding, at least in Python, but preferably in other domain-specific languages. From experience, I have also found that you will need at least entry-level ability to build distributed applications, understanding web client and server architecture. With all of that in mind, Google launched the TensorFlow Developer Certificate, designed around testing the skills I mentioned above. Holders of this certificate pass a 5-hour exam, where they go from a skeleton of code to a fully working model in each of the above three scenarios, as well as a few more foundational ones. I helped write the Candidate Handbook which I’d strongly recommend reading. This certificate is a really powerful way of demonstrating that you have all the above skills. It’s a great way to show employers that you have the foundational knowledge, but it’s also a great way of building a career beyond your first job. Being a certificate holder also grants entry to the TensorFlow Certificate Network where prospective employers can find you. Preparing for the exam We also worked with deeplearning.ai and Coursera to produce specializations that teach the material you need to know to take this exam. You can find the beginner course here if you want to experiment before going deeper! My book AI and Machine Learning for Coders also covers this syllabus and helps prepare for the exam! Widening access I’m also passionate about widening access to the traditionally underserved, so people of diverse backgrounds, experiences, geographies, and perspectives can transform machine learning for the better. An equitable approach to AI and ML is essential for us all to succeed. We’ve already seen great dividends with projects worldwide and a massive global developer community helping push the platform forward. With that in mind, there is a stipend available, and successful applications will get no-cost access to the TensorFlow Professional Certificate courses at Coursera, as well as a discount on the exam fee. If you’re exploring expanding your software developer toolkit with Machine Learning, I strongly recommend checking it out at https://www.tensorflow.org/certificate</summary></entry><entry><title type="html">Learning TensorFlow.js - Book Review</title><link href="/2021/05/28/tfjsbook.html" rel="alternate" type="text/html" title="Learning TensorFlow.js - Book Review" /><published>2021-05-28T00:00:00-07:00</published><updated>2021-05-28T00:00:00-07:00</updated><id>/2021/05/28/tfjsbook</id><content type="html" xml:base="/2021/05/28/tfjsbook.html">&lt;p&gt;&lt;img src=&quot;/assets/gantcover.jpg&quot; alt=&quot;Gant Cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was delighted to get an advance copy of Gant Laborde’s “Learning TensorFlow.js” from O’Reilly and privileged to have written the foreword.&lt;/p&gt;

&lt;p&gt;Gant has done something special with this book. In just 300 pages, he takes you end-to-end, in-depth through everything you need to know from an introduction to AI, understanding tensors, using them in the browser, deploying them, and more.&lt;/p&gt;

&lt;p&gt;It ends with a capstone project (what a great idea, I might have to steal it for my next book!), where you can use Machine Learning to convert an image into a set of dice, like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/dice.jpg&quot; alt=&quot;Gant Dice Sculpture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How much fun is that?&lt;/p&gt;

&lt;p&gt;I &lt;em&gt;love&lt;/em&gt; this book because it is for a different audience than the traditional ML one. It starts with a great introduction to AI and then tells you about TensorFlow.js and how you can use it to build Machine Learning apps. Then, the mystery of Tensors is cracked open, and Gant leads you through some detailed examples of how you can convert images into Tensors for training and inference.&lt;/p&gt;

&lt;p&gt;It guides you through the three main ways to get a working model.&lt;/p&gt;

&lt;p&gt;First, you can find an existing model from TensorFlow Hub, and in Chapter 5, Gant leads you through using the inception model in JavaScript. Inception isn’t any toy model, though – it is a Convolutional Neural Network designed for image analysis and object detection. It’s not that long ago that it was state-of-the-art in research. And now it’s available in JavaScript!&lt;/p&gt;

&lt;p&gt;Or, you can create your model from scratch, and Gant takes you through the code for defining layers, with deep neural networks to help predict numeric data (such as the famous titanic dataset) or Convolutional Neural networks for image classification.&lt;/p&gt;

&lt;p&gt;Finally, there’s Transfer Learning, which could be the most exciting method for most developers, where you have a hybrid of both of the previous methods. You can stand on the shoulders of giants by using parts of an existing model, like Inception, but catered for your own needs.&lt;/p&gt;

&lt;p&gt;When I started my Machine Learning journey, one frustration I had was that there was lots of material for creating models but relatively little for using them. The tutorial would end with a validation set showing how accurate the model was, and then it would move on to the next thing! I am delighted to say that this book does not fall into that pattern! So, if you want to build a browser-based app that uses a model, you’ll get lots of code showing you how!&lt;/p&gt;

&lt;p&gt;For example, Chapter 6 shows you how to use the webcam in the browser, capturing frames and passing them to a model for classification. Chapter 10 shows you how to create a basic sketchpad for drawing images that a model can interpret.&lt;/p&gt;

&lt;p&gt;Whether you’re an experienced Machine Learning expert, looking to see how to apply JavaScript to help solve your problems, or a JavaScript developer who wants to enter the wonderful world of ML, this book is for you. &lt;a href=&quot;https://www.oreilly.com/library/view/learning-tensorflowjs/9781492090786/&quot;&gt;Check it out!&lt;/a&gt;&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="tensorflow" /><category term="ai" /><category term="books" /><summary type="html">I was delighted to get an advance copy of Gant Laborde’s “Learning TensorFlow.js” from O’Reilly and privileged to have written the foreword. Gant has done something special with this book. In just 300 pages, he takes you end-to-end, in-depth through everything you need to know from an introduction to AI, understanding tensors, using them in the browser, deploying them, and more. It ends with a capstone project (what a great idea, I might have to steal it for my next book!), where you can use Machine Learning to convert an image into a set of dice, like this: How much fun is that? I love this book because it is for a different audience than the traditional ML one. It starts with a great introduction to AI and then tells you about TensorFlow.js and how you can use it to build Machine Learning apps. Then, the mystery of Tensors is cracked open, and Gant leads you through some detailed examples of how you can convert images into Tensors for training and inference. It guides you through the three main ways to get a working model. First, you can find an existing model from TensorFlow Hub, and in Chapter 5, Gant leads you through using the inception model in JavaScript. Inception isn’t any toy model, though – it is a Convolutional Neural Network designed for image analysis and object detection. It’s not that long ago that it was state-of-the-art in research. And now it’s available in JavaScript! Or, you can create your model from scratch, and Gant takes you through the code for defining layers, with deep neural networks to help predict numeric data (such as the famous titanic dataset) or Convolutional Neural networks for image classification. Finally, there’s Transfer Learning, which could be the most exciting method for most developers, where you have a hybrid of both of the previous methods. You can stand on the shoulders of giants by using parts of an existing model, like Inception, but catered for your own needs. When I started my Machine Learning journey, one frustration I had was that there was lots of material for creating models but relatively little for using them. The tutorial would end with a validation set showing how accurate the model was, and then it would move on to the next thing! I am delighted to say that this book does not fall into that pattern! So, if you want to build a browser-based app that uses a model, you’ll get lots of code showing you how! For example, Chapter 6 shows you how to use the webcam in the browser, capturing frames and passing them to a model for classification. Chapter 10 shows you how to create a basic sketchpad for drawing images that a model can interpret. Whether you’re an experienced Machine Learning expert, looking to see how to apply JavaScript to help solve your problems, or a JavaScript developer who wants to enter the wonderful world of ML, this book is for you. Check it out!</summary></entry><entry><title type="html">Vaccine Updates and Volunteering</title><link href="/2021/05/26/vaccine-volunteer.html" rel="alternate" type="text/html" title="Vaccine Updates and Volunteering" /><published>2021-05-26T00:00:00-07:00</published><updated>2021-05-26T00:00:00-07:00</updated><id>/2021/05/26/vaccine-volunteer</id><content type="html" xml:base="/2021/05/26/vaccine-volunteer.html">&lt;p&gt;&lt;img src=&quot;/assets/volunteer.jpg&quot; alt=&quot;Laurence and Claudia&quot; /&gt;
The Covid-19 pandemic has been (hopefully) a once-in-a-lifetime experience. My heart goes out to countries that are still suffering, but I’m super encouraged by the progress we have made in the USA.&lt;/p&gt;

&lt;p&gt;I have been volunteering regularly at vaccination clinics, and it’s been a privilege to be a part of the solution. I wanted to share some stats that I got from Seattle city today in a Thank you note!&lt;/p&gt;

&lt;p&gt;About 76% of our eligible population has had one vaccine, with 60% being fully vaccinated, so we’re approaching ‘herd’ immunity, and Seattle may fully re-open sooner than previously expected.&lt;/p&gt;

&lt;p&gt;The vaccination clinic at Lumen field (home of the Seattle Seahawks) administered over 97,000 vaccinations, and people of color were approximately 40% of those served.&lt;/p&gt;

&lt;p&gt;The mass clinics are closing down in the middle of June, and the strategy will change to smaller, localized ones.&lt;/p&gt;

&lt;p&gt;If you haven’t had your shot(s), please get them. The sooner we are all immunized, the sooner we are safe and can move on.&lt;/p&gt;

&lt;p&gt;My last volunteering day was on Tuesday. Here’s a picture with my daughter!&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="personal" /><summary type="html">The Covid-19 pandemic has been (hopefully) a once-in-a-lifetime experience. My heart goes out to countries that are still suffering, but I’m super encouraged by the progress we have made in the USA. I have been volunteering regularly at vaccination clinics, and it’s been a privilege to be a part of the solution. I wanted to share some stats that I got from Seattle city today in a Thank you note! About 76% of our eligible population has had one vaccine, with 60% being fully vaccinated, so we’re approaching ‘herd’ immunity, and Seattle may fully re-open sooner than previously expected. The vaccination clinic at Lumen field (home of the Seattle Seahawks) administered over 97,000 vaccinations, and people of color were approximately 40% of those served. The mass clinics are closing down in the middle of June, and the strategy will change to smaller, localized ones. If you haven’t had your shot(s), please get them. The sooner we are all immunized, the sooner we are safe and can move on. My last volunteering day was on Tuesday. Here’s a picture with my daughter!</summary></entry><entry><title type="html">Imperial College interview me!</title><link href="/2021/05/26/imperial-college-interview.html" rel="alternate" type="text/html" title="Imperial College interview me!" /><published>2021-05-26T00:00:00-07:00</published><updated>2021-05-26T00:00:00-07:00</updated><id>/2021/05/26/imperial-college-interview</id><content type="html" xml:base="/2021/05/26/imperial-college-interview.html">&lt;p&gt;&lt;img src=&quot;/assets/icinterview.png&quot; alt=&quot;Imperial interview&quot; /&gt;
I just discovered this video and had forgotten about it! It’s a fun interview with Doctor Kevin Webster of Imperial College in London, talking about TensorFlow 2.  Amongst other things, we discuss my unlikely start in AI, way back in 1992!&lt;/p&gt;

&lt;p&gt;This was filmed as part of the TensorFlow 2 specialization from Imperial College, which I strongly recommend you check out if you want to learn AI and ML!&lt;/p&gt;

&lt;p&gt;You can watch it &lt;a href=&quot;https://www.coursera.org/lecture/getting-started-with-tensor-flow2/interview-with-laurence-moroney-vB43N&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="mooc" /><summary type="html">I just discovered this video and had forgotten about it! It’s a fun interview with Doctor Kevin Webster of Imperial College in London, talking about TensorFlow 2. Amongst other things, we discuss my unlikely start in AI, way back in 1992! This was filmed as part of the TensorFlow 2 specialization from Imperial College, which I strongly recommend you check out if you want to learn AI and ML! You can watch it here</summary></entry><entry><title type="html">Top 10 AI and ML Announcements for Developers at Google IO</title><link href="/2021/05/25/io-top-ten.html" rel="alternate" type="text/html" title="Top 10 AI and ML Announcements for Developers at Google IO" /><published>2021-05-25T00:00:00-07:00</published><updated>2021-05-25T00:00:00-07:00</updated><id>/2021/05/25/io-top-ten</id><content type="html" xml:base="/2021/05/25/io-top-ten.html">&lt;p&gt;There were so many new announcements for AI and ML developers at Google IO this year. In this video, I’ll share my top 10!. I’m excited with how much the AI and ML ecosystems have grown, and  hope these updates will help you solve any future challenges you face.&lt;/p&gt;

&lt;p&gt;Check it out here:&lt;/p&gt;
&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/3k37fz9p6_k?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Laurence Moroney</name></author><category term="google" /><category term="talks" /><summary type="html">There were so many new announcements for AI and ML developers at Google IO this year. In this video, I’ll share my top 10!. I’m excited with how much the AI and ML ecosystems have grown, and hope these updates will help you solve any future challenges you face. Check it out here:</summary></entry><entry><title type="html">Cross-Platform Computer Vision Talk</title><link href="/2021/05/24/io-talk.html" rel="alternate" type="text/html" title="Cross-Platform Computer Vision Talk" /><published>2021-05-24T00:00:00-07:00</published><updated>2021-05-24T00:00:00-07:00</updated><id>/2021/05/24/io-talk</id><content type="html" xml:base="/2021/05/24/io-talk.html">&lt;p&gt;After a late scratch, they asked me to quickly create a talk for Google IO on getting started with cross-platform mobile Machine Learning. So, one Saturday morning, at a Starbucks in Mercer Island, Washington, I came up with a quick talk on how to do Cross-Platform Computer Vision. It’s a short video – about 12 minutes, but it will cover the basics of creating a Computer Vision model that recognizes different things and how to deploy it to mobile systems. I’ve included Python code for model building, Kotlin code to run it on Android, and Swift Code for iOS!&lt;/p&gt;

&lt;p&gt;Check it out here:&lt;/p&gt;
&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/GJvtOAtzZXg?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Laurence Moroney</name></author><category term="google" /><category term="talks" /><summary type="html">After a late scratch, they asked me to quickly create a talk for Google IO on getting started with cross-platform mobile Machine Learning. So, one Saturday morning, at a Starbucks in Mercer Island, Washington, I came up with a quick talk on how to do Cross-Platform Computer Vision. It’s a short video – about 12 minutes, but it will cover the basics of creating a Computer Vision model that recognizes different things and how to deploy it to mobile systems. I’ve included Python code for model building, Kotlin code to run it on Android, and Swift Code for iOS! Check it out here:</summary></entry><entry><title type="html">My next book - AI and ML for On-Device Developers</title><link href="/2021/05/23/next-book.html" rel="alternate" type="text/html" title="My next book - AI and ML for On-Device Developers" /><published>2021-05-23T00:00:00-07:00</published><updated>2021-05-23T00:00:00-07:00</updated><id>/2021/05/23/next-book</id><content type="html" xml:base="/2021/05/23/next-book.html">&lt;p&gt;My passion at the moment is cross-platform mobile development, and enabling developers to create Machine Learning applications that run on Android or iOS.&lt;/p&gt;

&lt;p&gt;With that in mind, my &lt;a href=&quot;https://www.oreilly.com/library/view/ai-and-machine/9781098101732/&quot;&gt;next book&lt;/a&gt; due in September is about exactly that!&lt;/p&gt;

&lt;p&gt;From the publisher:
AI is nothing without somewhere to run it. Now that mobile devices have become the primary computing device for most people, it’s essential that mobile developers add AI to their toolbox. This insightful book is your guide to creating models and running them on popular mobile platforms such as iOS and Android.&lt;/p&gt;

&lt;p&gt;Laurence Moroney, lead AI advocate at Google, offers an introduction to machine learning techniques and tools, then walks you through writing Android and iOS apps powered by common ML models like computer vision and text recognition, using tools such as ML Kit, TensorFlow Lite, and Core ML. If you’re a mobile developer, this book will help you take advantage of the ML revolution today.&lt;/p&gt;

&lt;p&gt;Explore the options for implementing ML and AI on mobile devices–and when to use each
Create ML models for iOS and Android
Write ML Kit and TensorFlow Lite apps for iOS and Android and Core ML/Create ML apps for iOS
Understand how to choose the best techniques and tools for your use case: on-device inference versus cloud-based inference, high-level APIs versus low-level APIs, and more
Learn privacy and ethics best practices for ML on devices&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="books" /><summary type="html">My passion at the moment is cross-platform mobile development, and enabling developers to create Machine Learning applications that run on Android or iOS. With that in mind, my next book due in September is about exactly that! From the publisher: AI is nothing without somewhere to run it. Now that mobile devices have become the primary computing device for most people, it’s essential that mobile developers add AI to their toolbox. This insightful book is your guide to creating models and running them on popular mobile platforms such as iOS and Android. Laurence Moroney, lead AI advocate at Google, offers an introduction to machine learning techniques and tools, then walks you through writing Android and iOS apps powered by common ML models like computer vision and text recognition, using tools such as ML Kit, TensorFlow Lite, and Core ML. If you’re a mobile developer, this book will help you take advantage of the ML revolution today. Explore the options for implementing ML and AI on mobile devices–and when to use each Create ML models for iOS and Android Write ML Kit and TensorFlow Lite apps for iOS and Android and Core ML/Create ML apps for iOS Understand how to choose the best techniques and tools for your use case: on-device inference versus cloud-based inference, high-level APIs versus low-level APIs, and more Learn privacy and ethics best practices for ML on devices</summary></entry><entry><title type="html">Artificial Intelligence for Anyone - Part One</title><link href="/2021/05/22/what-is-ai.html" rel="alternate" type="text/html" title="Artificial Intelligence for Anyone - Part One" /><published>2021-05-22T00:00:00-07:00</published><updated>2021-05-22T00:00:00-07:00</updated><id>/2021/05/22/what-is-ai</id><content type="html" xml:base="/2021/05/22/what-is-ai.html">&lt;h1 id=&quot;part-one-what-is-artificial-intelligence&quot;&gt;Part One: What is Artificial Intelligence?&lt;/h1&gt;
&lt;p&gt;Artificial Intelligence (AI) is the single most overhyped area of technology I have ever encountered. The amount of misinformation and confusion around AI is staggering, so it’s the purpose of this set of tutorials to explain not just what AI &lt;em&gt;is&lt;/em&gt; but also what it &lt;em&gt;isn’t&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;You can ask ten technologists what AI is, and you’ll probably get ten different answers, so with that in mind, let me at least start you with my answer.&lt;/p&gt;

&lt;h2 id=&quot;its-not-artificial-life&quot;&gt;It’s not Artificial &lt;em&gt;Life&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;First of all, there’s &lt;em&gt;Artificial Life&lt;/em&gt; sometimes referred to as Artificial General Intelligence (AGI).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/aiml.png&quot; alt=&quot;Artificial Live v Artificial Intelligence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AGI does not exist &lt;em&gt;yet&lt;/em&gt;, and this has been the subject of speculative stories and prophecies for generations. These will often cast the artificial life in a &lt;em&gt;negative&lt;/em&gt; light, with one of the oldest I have been able to find coming from the Biblical Book of Revelation, authored around 80AD, where future events concerning the end of the world involve several beasts. At one point, it describes:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;It ordered them to set up an image of the beast who was wounded by the sword but yet lived. 
The second beast was given power to give breath to the *image* for the first beast, 
so that the *image* could speak and cause all who refuse to worship the **image** to be killed&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the word &lt;em&gt;image&lt;/em&gt; here is a translation of the Greek word for &lt;em&gt;statue&lt;/em&gt;, &lt;em&gt;figure&lt;/em&gt; or &lt;em&gt;representation&lt;/em&gt;. This word, in turn, derives from a word meaning something that is like another thing, an artificial copy of it.&lt;/p&gt;

&lt;p&gt;Later literature doubles down on this fear of artificial life. While stories like ‘The Golem of Prague’ and ‘Frankenstein’ may appear primitive in terms of modern technology, the underlying tale dramatically resembles that of ‘Terminator,’ ‘Battlestar Galactica,’ and countless others in that artificial life is something that will destroy us, and needs to be feared.&lt;/p&gt;

&lt;p&gt;And often, this type of artificial &lt;em&gt;life&lt;/em&gt; is conflated with artificial &lt;em&gt;intelligence&lt;/em&gt;, leading us to fear AI.&lt;/p&gt;

&lt;p&gt;So let’s first put that stake in the ground in our definition of AI. It is &lt;em&gt;not&lt;/em&gt; artificial life.&lt;/p&gt;

&lt;h2 id=&quot;its-not-smart-computers&quot;&gt;It’s not smart Computers&lt;/h2&gt;

&lt;p&gt;To understand what artificial intelligence is, let’s now turn to computing, and let’s try to understand what that’s all about. We often give computers credit for being smart, a meme brought about undoubtedly by science fiction, with the characters of shows like &lt;em&gt;Star Trek&lt;/em&gt; able to talk to their computer and find out just about anything they need to know. Indeed, I remember, when at the ripe age of twelve years old, I got my first computer. I remember plugging it into the TV and starting to write programs in BASIC. A friend came round to see it, and when given a turn, the first thing he typed was “What is the capital of Brazil?”.&lt;/p&gt;

&lt;p&gt;When the machine replied, “Syntax Error,” he was aghast. Computers were supposed to know &lt;em&gt;everything&lt;/em&gt;. And while that misconception has gone away with more exposure of the general public to computing platforms, the underlying belief remains.&lt;/p&gt;

&lt;p&gt;What computers &lt;em&gt;are&lt;/em&gt; very good at is doing lots of monotonous, repetitive tasks very quickly. Programmers can create sets of commands that computers will act on, but in many ways, the actual intelligence and the problem-solving aspect comes from the programmer’s mind and not from the machine.&lt;/p&gt;

&lt;p&gt;Since the dawn of computing, that’s been the job of programmers. Figure out how to solve a problem, break the steps of the solution down into simple-to-understand commands using a programming language, and then get the machine to do the job.&lt;/p&gt;

&lt;p&gt;This process has been the bread-and-butter for the Computer Science industry for decades. Until the advent of &lt;em&gt;Machine Learning&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;it-is-just-another-algorithm&quot;&gt;It is just another algorithm&lt;/h2&gt;

&lt;p&gt;Now, while the term Machine Learning might make it sound like you are dealing with an artificial lifeform capable of learning in much the same way an actual lifeform would, that’s not the case. Machine Learning is still just a computer doing many calculations to make its program work a bit better, but automating the process instead of needing a programmer to figure it out.&lt;/p&gt;

&lt;p&gt;Let’s consider an example here. We’ll start with a simple one for a human to program and use that as the basis for understanding how a machine can learn.&lt;/p&gt;

&lt;p&gt;There are two main scales for measuring temperature globally, and these are Celsius and Fahrenheit. There’s a formula that connects the two of these, so you would figure out how to express that formula using a coding language.&lt;/p&gt;

&lt;p&gt;So, for example, you might write:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You’d come up with this answer is because you probably looked up the formula for converting Celsius to Fahrenheit and then figured out the particular syntax to do that in a programming language such as Python.&lt;/p&gt;

&lt;p&gt;Consider what would happen if you didn’t have the formula but still needed to figure out the relationship between the two values. Instead, you have a table of readings on both scales. What would you then do?&lt;/p&gt;

&lt;p&gt;For example, you know that&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;10 degrees C is 50 degrees F&lt;/li&gt;
  &lt;li&gt;5 degrees C is 41 degrees F&lt;/li&gt;
  &lt;li&gt;0 degrees C is 32 degrees F&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the 0C = 32F relationship, you might surmise that the F value is 32+(something) times the C value. If that were the case, then you’d look at the 5C = 41F relationship and see that the 41 is 32+(1.8 times 5). You’d then verify this by calculating that the 10C=50F fit the same formula. You now know that the F value is 32 + 1.8 times the C value.&lt;/p&gt;

&lt;p&gt;Note that there were two &lt;em&gt;parameters&lt;/em&gt; in the above function. You calculated ‘f’ by multiplying ‘c’ by &lt;em&gt;something&lt;/em&gt; (in this case, 1.8) and adding another &lt;em&gt;something&lt;/em&gt; (32).&lt;/p&gt;

&lt;h2 id=&quot;the-key-to-machine-learning-pattern-matching&quot;&gt;The key to Machine Learning: Pattern Matching&lt;/h2&gt;

&lt;p&gt;The process that we call ‘Machine Learning’ does a very similar thing, but a lot less smartly than you just did. It will guess the two parameters, and then, given the data, it will measure how good or how bad that guess is. That measurement is called a ‘loss.’ It will then strategize guessing to reduce this loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/mlcycle.png&quot; alt=&quot;The ML Loop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, for example, it might guess that f = 10+(10&lt;em&gt;c), initializing both parameters to 10. If it were to do that, then 0C would be 10F, which would be off by 22. Similarly, 5C would be 60F, and 10C would be 110F, both wildly incorrect. So the machine would strategize and use some fancy math to come up with another, better guess, maybe f=15+(5&lt;/em&gt;c). It would still be wrong, but &lt;em&gt;less&lt;/em&gt; so. Getting closer and closer to the correct answer using brute force mathematics is commonly referred to as Machine Learning. So please, forget all those pictures of child-like robots sitting in a schoolroom. Those aren’t representative of Machine Learning at all!&lt;/p&gt;

&lt;p&gt;For a trivial example like this, where it’s easy for a human to establish the rules, this may seem pointless. But what about circumstances where it’s tough for a &lt;em&gt;human&lt;/em&gt; to see the rules that determine something? For example, what if you look at a picture like this. You can instantly tell that it’s a dog. But how do you have a computer recognize that? How can it tell the difference between a dog and a cat?&lt;/p&gt;

&lt;p&gt;In a process similar to the above, you can have a computer look at lots of pictures of dogs and cats and use a similar brute-force method to find out what in the data distinguishes one from another. Surprisingly, recent research has shown that with the correct Machine Learning algorithm, computers are often better at humans at distinguishing visual objects. Not only that, researchers have been ab[le to use these algorithms to spot things in images that humans don’t see. There’s a famous story from Google Research. When exploring data from &lt;a href=&quot;https://www.nature.com/articles/s41551-018-0195-0&quot;&gt;retina scans&lt;/a&gt;, a researcher noticed that gender and age data were available, so they wrote a Machine Learning algorithm to figure out a person’s age and birth-assigned gender from a retina scan. It proved to be more accurate than most humans at guessing these details!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/retina.png&quot; alt=&quot;Retina Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This technique, Machine Learning, gives us a new way to program computers that behave more like intelligent creatures. So, instead of a picture just being numbers representing the dots that make it up, a computer can now figure out that it contains a cat, a dog, or a diseased retina. Or similarly, it can represent a sound as a type of bird, a regular heartbeat, or an alarm, instead of just a bunch of frequencies.&lt;/p&gt;

&lt;p&gt;But, it is important to remember they are &lt;em&gt;not&lt;/em&gt; alive, and they are &lt;em&gt;not&lt;/em&gt; intelligent. And that is why the field is called &lt;em&gt;Artificial&lt;/em&gt; intelligence.&lt;/p&gt;

&lt;p&gt;In the next article in this series, I’ll go deeper into Machine Learning, look in more detail at some applications and how these applications can change the world for the better.&lt;/p&gt;

&lt;!--more--&gt;</content><author><name>Laurence Moroney</name></author><category term="AI4ALL" /><summary type="html">Part One: What is Artificial Intelligence? Artificial Intelligence (AI) is the single most overhyped area of technology I have ever encountered. The amount of misinformation and confusion around AI is staggering, so it’s the purpose of this set of tutorials to explain not just what AI is but also what it isn’t. You can ask ten technologists what AI is, and you’ll probably get ten different answers, so with that in mind, let me at least start you with my answer. It’s not Artificial Life First of all, there’s Artificial Life sometimes referred to as Artificial General Intelligence (AGI). AGI does not exist yet, and this has been the subject of speculative stories and prophecies for generations. These will often cast the artificial life in a negative light, with one of the oldest I have been able to find coming from the Biblical Book of Revelation, authored around 80AD, where future events concerning the end of the world involve several beasts. At one point, it describes: &quot;It ordered them to set up an image of the beast who was wounded by the sword but yet lived. The second beast was given power to give breath to the *image* for the first beast, so that the *image* could speak and cause all who refuse to worship the **image** to be killed&quot; Note that the word image here is a translation of the Greek word for statue, figure or representation. This word, in turn, derives from a word meaning something that is like another thing, an artificial copy of it. Later literature doubles down on this fear of artificial life. While stories like ‘The Golem of Prague’ and ‘Frankenstein’ may appear primitive in terms of modern technology, the underlying tale dramatically resembles that of ‘Terminator,’ ‘Battlestar Galactica,’ and countless others in that artificial life is something that will destroy us, and needs to be feared. And often, this type of artificial life is conflated with artificial intelligence, leading us to fear AI. So let’s first put that stake in the ground in our definition of AI. It is not artificial life. It’s not smart Computers To understand what artificial intelligence is, let’s now turn to computing, and let’s try to understand what that’s all about. We often give computers credit for being smart, a meme brought about undoubtedly by science fiction, with the characters of shows like Star Trek able to talk to their computer and find out just about anything they need to know. Indeed, I remember, when at the ripe age of twelve years old, I got my first computer. I remember plugging it into the TV and starting to write programs in BASIC. A friend came round to see it, and when given a turn, the first thing he typed was “What is the capital of Brazil?”. When the machine replied, “Syntax Error,” he was aghast. Computers were supposed to know everything. And while that misconception has gone away with more exposure of the general public to computing platforms, the underlying belief remains. What computers are very good at is doing lots of monotonous, repetitive tasks very quickly. Programmers can create sets of commands that computers will act on, but in many ways, the actual intelligence and the problem-solving aspect comes from the programmer’s mind and not from the machine. Since the dawn of computing, that’s been the job of programmers. Figure out how to solve a problem, break the steps of the solution down into simple-to-understand commands using a programming language, and then get the machine to do the job. This process has been the bread-and-butter for the Computer Science industry for decades. Until the advent of Machine Learning. It is just another algorithm Now, while the term Machine Learning might make it sound like you are dealing with an artificial lifeform capable of learning in much the same way an actual lifeform would, that’s not the case. Machine Learning is still just a computer doing many calculations to make its program work a bit better, but automating the process instead of needing a programmer to figure it out. Let’s consider an example here. We’ll start with a simple one for a human to program and use that as the basis for understanding how a machine can learn. There are two main scales for measuring temperature globally, and these are Celsius and Fahrenheit. There’s a formula that connects the two of these, so you would figure out how to express that formula using a coding language. So, for example, you might write: def c-to-f(c): f = (c*1.8) + 32 return f You’d come up with this answer is because you probably looked up the formula for converting Celsius to Fahrenheit and then figured out the particular syntax to do that in a programming language such as Python. Consider what would happen if you didn’t have the formula but still needed to figure out the relationship between the two values. Instead, you have a table of readings on both scales. What would you then do? For example, you know that 10 degrees C is 50 degrees F 5 degrees C is 41 degrees F 0 degrees C is 32 degrees F From the 0C = 32F relationship, you might surmise that the F value is 32+(something) times the C value. If that were the case, then you’d look at the 5C = 41F relationship and see that the 41 is 32+(1.8 times 5). You’d then verify this by calculating that the 10C=50F fit the same formula. You now know that the F value is 32 + 1.8 times the C value. Note that there were two parameters in the above function. You calculated ‘f’ by multiplying ‘c’ by something (in this case, 1.8) and adding another something (32). The key to Machine Learning: Pattern Matching The process that we call ‘Machine Learning’ does a very similar thing, but a lot less smartly than you just did. It will guess the two parameters, and then, given the data, it will measure how good or how bad that guess is. That measurement is called a ‘loss.’ It will then strategize guessing to reduce this loss. So, for example, it might guess that f = 10+(10c), initializing both parameters to 10. If it were to do that, then 0C would be 10F, which would be off by 22. Similarly, 5C would be 60F, and 10C would be 110F, both wildly incorrect. So the machine would strategize and use some fancy math to come up with another, better guess, maybe f=15+(5c). It would still be wrong, but less so. Getting closer and closer to the correct answer using brute force mathematics is commonly referred to as Machine Learning. So please, forget all those pictures of child-like robots sitting in a schoolroom. Those aren’t representative of Machine Learning at all! For a trivial example like this, where it’s easy for a human to establish the rules, this may seem pointless. But what about circumstances where it’s tough for a human to see the rules that determine something? For example, what if you look at a picture like this. You can instantly tell that it’s a dog. But how do you have a computer recognize that? How can it tell the difference between a dog and a cat? In a process similar to the above, you can have a computer look at lots of pictures of dogs and cats and use a similar brute-force method to find out what in the data distinguishes one from another. Surprisingly, recent research has shown that with the correct Machine Learning algorithm, computers are often better at humans at distinguishing visual objects. Not only that, researchers have been ab[le to use these algorithms to spot things in images that humans don’t see. There’s a famous story from Google Research. When exploring data from retina scans, a researcher noticed that gender and age data were available, so they wrote a Machine Learning algorithm to figure out a person’s age and birth-assigned gender from a retina scan. It proved to be more accurate than most humans at guessing these details! This technique, Machine Learning, gives us a new way to program computers that behave more like intelligent creatures. So, instead of a picture just being numbers representing the dots that make it up, a computer can now figure out that it contains a cat, a dog, or a diseased retina. Or similarly, it can represent a sound as a type of bird, a regular heartbeat, or an alarm, instead of just a bunch of frequencies. But, it is important to remember they are not alive, and they are not intelligent. And that is why the field is called Artificial intelligence. In the next article in this series, I’ll go deeper into Machine Learning, look in more detail at some applications and how these applications can change the world for the better.</summary></entry><entry><title type="html">Machine Learning for anomaly detection</title><link href="/2021/05/21/io-workshop.html" rel="alternate" type="text/html" title="Machine Learning for anomaly detection" /><published>2021-05-21T00:00:00-07:00</published><updated>2021-05-21T00:00:00-07:00</updated><id>/2021/05/21/io-workshop</id><content type="html" xml:base="/2021/05/21/io-workshop.html">&lt;p&gt;At Google IO this year, I had many activities, one of which was to teach a workshop in anomaly detection. It uses a novel approach – by training an encoder-decoder architecture with electrocardiogram (EKG) values and exploring the reconstruction error.&lt;/p&gt;

&lt;p&gt;The logic was that when an autoencoder model gets trained on only good EKG values, then the reconstruction error for good ones will be small. Thus, an abnormal EKG would have a high reconstruction error, and if this is above a certain threshold, we know that it’s an anomaly.&lt;/p&gt;

&lt;p&gt;Watch the entire workshop here:&lt;/p&gt;
&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/2K3ScZp1dXQ?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Laurence Moroney</name></author><category term="google" /><category term="talks" /><summary type="html">At Google IO this year, I had many activities, one of which was to teach a workshop in anomaly detection. It uses a novel approach – by training an encoder-decoder architecture with electrocardiogram (EKG) values and exploring the reconstruction error. The logic was that when an autoencoder model gets trained on only good EKG values, then the reconstruction error for good ones will be small. Thus, an abnormal EKG would have a high reconstruction error, and if this is above a certain threshold, we know that it’s an anomaly. Watch the entire workshop here:</summary></entry></feed>