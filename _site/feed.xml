<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-05-24T08:30:20-07:00</updated><id>/feed.xml</id><title type="html">Laurence Moroney - The AI Guy.</title><subtitle>This is the blog and general web site of Laurence Moroney, author, lecturer, teacher and AI lead at Google with Musings on AI, Quantum and other future tech
</subtitle><author><name>Laurence Moroney</name></author><entry><title type="html">Cross-Platform Computer Vision Talk</title><link href="/2021/05/24/io-talk.html" rel="alternate" type="text/html" title="Cross-Platform Computer Vision Talk" /><published>2021-05-24T00:00:00-07:00</published><updated>2021-05-24T00:00:00-07:00</updated><id>/2021/05/24/io-talk</id><content type="html" xml:base="/2021/05/24/io-talk.html">&lt;p&gt;After a late scratch, they asked me to quickly create a talk for Google IO on getting started with cross-platform mobile Machine Learning. So, one Saturday morning, at a Starbucks in Mercer Island, Washington, I came up with a quick talk on how to do Cross-Platform Computer Vision. It’s a short video – about 12 minutes, but it will cover the basics of creating a Computer Vision model that recognizes different things and how to deploy it to mobile systems. I’ve included Python code for model building, Kotlin code to run it on Android, and Swift Code for iOS!&lt;/p&gt;

&lt;p&gt;Check it out here:&lt;/p&gt;
&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/GJvtOAtzZXg?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Laurence Moroney</name></author><category term="google" /><category term="talks" /><summary type="html">After a late scratch, they asked me to quickly create a talk for Google IO on getting started with cross-platform mobile Machine Learning. So, one Saturday morning, at a Starbucks in Mercer Island, Washington, I came up with a quick talk on how to do Cross-Platform Computer Vision. It’s a short video – about 12 minutes, but it will cover the basics of creating a Computer Vision model that recognizes different things and how to deploy it to mobile systems. I’ve included Python code for model building, Kotlin code to run it on Android, and Swift Code for iOS! Check it out here:</summary></entry><entry><title type="html">My next book - AI and ML for On-Device Developers</title><link href="/2021/05/23/next-book.html" rel="alternate" type="text/html" title="My next book - AI and ML for On-Device Developers" /><published>2021-05-23T00:00:00-07:00</published><updated>2021-05-23T00:00:00-07:00</updated><id>/2021/05/23/next-book</id><content type="html" xml:base="/2021/05/23/next-book.html">&lt;p&gt;My passion at the moment is cross-platform mobile development, and enabling developers to create Machine Learning applications that run on Android or iOS.&lt;/p&gt;

&lt;p&gt;With that in mind, my &lt;a href=&quot;https://www.oreilly.com/library/view/ai-and-machine/9781098101732/&quot;&gt;next book&lt;/a&gt; due in September is about exactly that!&lt;/p&gt;

&lt;p&gt;From the publisher:
AI is nothing without somewhere to run it. Now that mobile devices have become the primary computing device for most people, it’s essential that mobile developers add AI to their toolbox. This insightful book is your guide to creating models and running them on popular mobile platforms such as iOS and Android.&lt;/p&gt;

&lt;p&gt;Laurence Moroney, lead AI advocate at Google, offers an introduction to machine learning techniques and tools, then walks you through writing Android and iOS apps powered by common ML models like computer vision and text recognition, using tools such as ML Kit, TensorFlow Lite, and Core ML. If you’re a mobile developer, this book will help you take advantage of the ML revolution today.&lt;/p&gt;

&lt;p&gt;Explore the options for implementing ML and AI on mobile devices–and when to use each
Create ML models for iOS and Android
Write ML Kit and TensorFlow Lite apps for iOS and Android and Core ML/Create ML apps for iOS
Understand how to choose the best techniques and tools for your use case: on-device inference versus cloud-based inference, high-level APIs versus low-level APIs, and more
Learn privacy and ethics best practices for ML on devices&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="books" /><summary type="html">My passion at the moment is cross-platform mobile development, and enabling developers to create Machine Learning applications that run on Android or iOS. With that in mind, my next book due in September is about exactly that! From the publisher: AI is nothing without somewhere to run it. Now that mobile devices have become the primary computing device for most people, it’s essential that mobile developers add AI to their toolbox. This insightful book is your guide to creating models and running them on popular mobile platforms such as iOS and Android. Laurence Moroney, lead AI advocate at Google, offers an introduction to machine learning techniques and tools, then walks you through writing Android and iOS apps powered by common ML models like computer vision and text recognition, using tools such as ML Kit, TensorFlow Lite, and Core ML. If you’re a mobile developer, this book will help you take advantage of the ML revolution today. Explore the options for implementing ML and AI on mobile devices–and when to use each Create ML models for iOS and Android Write ML Kit and TensorFlow Lite apps for iOS and Android and Core ML/Create ML apps for iOS Understand how to choose the best techniques and tools for your use case: on-device inference versus cloud-based inference, high-level APIs versus low-level APIs, and more Learn privacy and ethics best practices for ML on devices</summary></entry><entry><title type="html">Artificial Intelligence for Anyone - Part One</title><link href="/2021/05/22/what-is-ai.html" rel="alternate" type="text/html" title="Artificial Intelligence for Anyone - Part One" /><published>2021-05-22T00:00:00-07:00</published><updated>2021-05-22T00:00:00-07:00</updated><id>/2021/05/22/what-is-ai</id><content type="html" xml:base="/2021/05/22/what-is-ai.html">&lt;h1 id=&quot;part-one-what-is-artificial-intelligence&quot;&gt;Part One: What is Artificial Intelligence?&lt;/h1&gt;
&lt;p&gt;Artificial Intelligence (AI) is the single most overhyped area of technology I have ever encountered. The amount of misinformation and confusion around AI is staggering, so it’s the purpose of this set of tutorials to explain not just what AI &lt;em&gt;is&lt;/em&gt; but also what it &lt;em&gt;isn’t&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;You can ask ten technologists what AI is, and you’ll probably get ten different answers, so with that in mind, let me at least start you with my answer.&lt;/p&gt;

&lt;h2 id=&quot;its-not-artificial-life&quot;&gt;It’s not Artificial &lt;em&gt;Life&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;First of all, there’s &lt;em&gt;Artificial Life&lt;/em&gt; sometimes referred to as Artificial General Intelligence (AGI).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/aiml.png&quot; alt=&quot;Artificial Live v Artificial Intelligence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;AGI does not exist &lt;em&gt;yet&lt;/em&gt;, and this has been the subject of speculative stories and prophecies for generations. These will often cast the artificial life in a &lt;em&gt;negative&lt;/em&gt; light, with one of the oldest I have been able to find coming from the Biblical Book of Revelation, authored around 80AD, where future events concerning the end of the world involve several beasts. At one point, it describes:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;It ordered them to set up an image of the beast who was wounded by the sword but yet lived. 
The second beast was given power to give breath to the *image* for the first beast, 
so that the *image* could speak and cause all who refuse to worship the **image** to be killed&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the word &lt;em&gt;image&lt;/em&gt; here is a translation of the Greek word for &lt;em&gt;statue&lt;/em&gt;, &lt;em&gt;figure&lt;/em&gt; or &lt;em&gt;representation&lt;/em&gt;. This word, in turn, derives from a word meaning something that is like another thing, an artificial copy of it.&lt;/p&gt;

&lt;p&gt;Later literature doubles down on this fear of artificial life. While stories like ‘The Golem of Prague’ and ‘Frankenstein’ may appear primitive in terms of modern technology, the underlying tale dramatically resembles that of ‘Terminator,’ ‘Battlestar Galactica,’ and countless others in that artificial life is something that will destroy us, and needs to be feared.&lt;/p&gt;

&lt;p&gt;And often, this type of artificial &lt;em&gt;life&lt;/em&gt; is conflated with artificial &lt;em&gt;intelligence&lt;/em&gt;, leading us to fear AI.&lt;/p&gt;

&lt;p&gt;So let’s first put that stake in the ground in our definition of AI. It is &lt;em&gt;not&lt;/em&gt; artificial life.&lt;/p&gt;

&lt;h2 id=&quot;its-not-smart-computers&quot;&gt;It’s not smart Computers&lt;/h2&gt;

&lt;p&gt;To understand what artificial intelligence is, let’s now turn to computing, and let’s try to understand what that’s all about. We often give computers credit for being smart, a meme brought about undoubtedly by science fiction, with the characters of shows like &lt;em&gt;Star Trek&lt;/em&gt; able to talk to their computer and find out just about anything they need to know. Indeed, I remember, when at the ripe age of twelve years old, I got my first computer. I remember plugging it into the TV and starting to write programs in BASIC. A friend came round to see it, and when given a turn, the first thing he typed was “What is the capital of Brazil?”.&lt;/p&gt;

&lt;p&gt;When the machine replied, “Syntax Error,” he was aghast. Computers were supposed to know &lt;em&gt;everything&lt;/em&gt;. And while that misconception has gone away with more exposure of the general public to computing platforms, the underlying belief remains.&lt;/p&gt;

&lt;p&gt;What computers &lt;em&gt;are&lt;/em&gt; very good at is doing lots of monotonous, repetitive tasks very quickly. Programmers can create sets of commands that computers will act on, but in many ways, the actual intelligence and the problem-solving aspect comes from the programmer’s mind and not from the machine.&lt;/p&gt;

&lt;p&gt;Since the dawn of computing, that’s been the job of programmers. Figure out how to solve a problem, break the steps of the solution down into simple-to-understand commands using a programming language, and then get the machine to do the job.&lt;/p&gt;

&lt;p&gt;This process has been the bread-and-butter for the Computer Science industry for decades. Until the advent of &lt;em&gt;Machine Learning&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;it-is-just-another-algorithm&quot;&gt;It is just another algorithm&lt;/h2&gt;

&lt;p&gt;Now, while the term Machine Learning might make it sound like you are dealing with an artificial lifeform capable of learning in much the same way an actual lifeform would, that’s not the case. Machine Learning is still just a computer doing many calculations to make its program work a bit better, but automating the process instead of needing a programmer to figure it out.&lt;/p&gt;

&lt;p&gt;Let’s consider an example here. We’ll start with a simple one for a human to program and use that as the basis for understanding how a machine can learn.&lt;/p&gt;

&lt;p&gt;There are two main scales for measuring temperature globally, and these are Celsius and Fahrenheit. There’s a formula that connects the two of these, so you would figure out how to express that formula using a coding language.&lt;/p&gt;

&lt;p&gt;So, for example, you might write:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You’d come up with this answer is because you probably looked up the formula for converting Celsius to Fahrenheit and then figured out the particular syntax to do that in a programming language such as Python.&lt;/p&gt;

&lt;p&gt;Consider what would happen if you didn’t have the formula but still needed to figure out the relationship between the two values. Instead, you have a table of readings on both scales. What would you then do?&lt;/p&gt;

&lt;p&gt;For example, you know that&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;10 degrees C is 50 degrees F&lt;/li&gt;
  &lt;li&gt;5 degrees C is 41 degrees F&lt;/li&gt;
  &lt;li&gt;0 degrees C is 32 degrees F&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From the 0C = 32F relationship, you might surmise that the F value is 32+(something) times the C value. If that were the case, then you’d look at the 5C = 41F relationship and see that the 41 is 32+(1.8 times 5). You’d then verify this by calculating that the 10C=50F fit the same formula. You now know that the F value is 32 + 1.8 times the C value.&lt;/p&gt;

&lt;p&gt;Note that there were two &lt;em&gt;parameters&lt;/em&gt; in the above function. You calculated ‘f’ by multiplying ‘c’ by &lt;em&gt;something&lt;/em&gt; (in this case, 1.8) and adding another &lt;em&gt;something&lt;/em&gt; (32).&lt;/p&gt;

&lt;h2 id=&quot;the-key-to-machine-learning-pattern-matching&quot;&gt;The key to Machine Learning: Pattern Matching&lt;/h2&gt;

&lt;p&gt;The process that we call ‘Machine Learning’ does a very similar thing, but a lot less smartly than you just did. It will guess the two parameters, and then, given the data, it will measure how good or how bad that guess is. That measurement is called a ‘loss.’ It will then strategize guessing to reduce this loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/mlcycle.png&quot; alt=&quot;The ML Loop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, for example, it might guess that f = 10+(10&lt;em&gt;c), initializing both parameters to 10. If it were to do that, then 0C would be 10F, which would be off by 22. Similarly, 5C would be 60F, and 10C would be 110F, both wildly incorrect. So the machine would strategize and use some fancy math to come up with another, better guess, maybe f=15+(5&lt;/em&gt;c). It would still be wrong, but &lt;em&gt;less&lt;/em&gt; so. Getting closer and closer to the correct answer using brute force mathematics is commonly referred to as Machine Learning. So please, forget all those pictures of child-like robots sitting in a schoolroom. Those aren’t representative of Machine Learning at all!&lt;/p&gt;

&lt;p&gt;For a trivial example like this, where it’s easy for a human to establish the rules, this may seem pointless. But what about circumstances where it’s tough for a &lt;em&gt;human&lt;/em&gt; to see the rules that determine something? For example, what if you look at a picture like this. You can instantly tell that it’s a dog. But how do you have a computer recognize that? How can it tell the difference between a dog and a cat?&lt;/p&gt;

&lt;p&gt;In a process similar to the above, you can have a computer look at lots of pictures of dogs and cats and use a similar brute-force method to find out what in the data distinguishes one from another. Surprisingly, recent research has shown that with the correct Machine Learning algorithm, computers are often better at humans at distinguishing visual objects. Not only that, researchers have been ab[le to use these algorithms to spot things in images that humans don’t see. There’s a famous story from Google Research. When exploring data from &lt;a href=&quot;https://www.nature.com/articles/s41551-018-0195-0&quot;&gt;retina scans&lt;/a&gt;, a researcher noticed that gender and age data were available, so they wrote a Machine Learning algorithm to figure out a person’s age and birth-assigned gender from a retina scan. It proved to be more accurate than most humans at guessing these details!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/retina.png&quot; alt=&quot;Retina Image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This technique, Machine Learning, gives us a new way to program computers that behave more like intelligent creatures. So, instead of a picture just being numbers representing the dots that make it up, a computer can now figure out that it contains a cat, a dog, or a diseased retina. Or similarly, it can represent a sound as a type of bird, a regular heartbeat, or an alarm, instead of just a bunch of frequencies.&lt;/p&gt;

&lt;p&gt;But, it is important to remember they are &lt;em&gt;not&lt;/em&gt; alive, and they are &lt;em&gt;not&lt;/em&gt; intelligent. And that is why the field is called &lt;em&gt;Artificial&lt;/em&gt; intelligence.&lt;/p&gt;

&lt;p&gt;In the next article in this series, I’ll go deeper into Machine Learning, look in more detail at some applications and how these applications can change the world for the better.&lt;/p&gt;

&lt;!--more--&gt;</content><author><name>Laurence Moroney</name></author><category term="AI4ALL" /><summary type="html">Part One: What is Artificial Intelligence? Artificial Intelligence (AI) is the single most overhyped area of technology I have ever encountered. The amount of misinformation and confusion around AI is staggering, so it’s the purpose of this set of tutorials to explain not just what AI is but also what it isn’t. You can ask ten technologists what AI is, and you’ll probably get ten different answers, so with that in mind, let me at least start you with my answer. It’s not Artificial Life First of all, there’s Artificial Life sometimes referred to as Artificial General Intelligence (AGI). AGI does not exist yet, and this has been the subject of speculative stories and prophecies for generations. These will often cast the artificial life in a negative light, with one of the oldest I have been able to find coming from the Biblical Book of Revelation, authored around 80AD, where future events concerning the end of the world involve several beasts. At one point, it describes: &quot;It ordered them to set up an image of the beast who was wounded by the sword but yet lived. The second beast was given power to give breath to the *image* for the first beast, so that the *image* could speak and cause all who refuse to worship the **image** to be killed&quot; Note that the word image here is a translation of the Greek word for statue, figure or representation. This word, in turn, derives from a word meaning something that is like another thing, an artificial copy of it. Later literature doubles down on this fear of artificial life. While stories like ‘The Golem of Prague’ and ‘Frankenstein’ may appear primitive in terms of modern technology, the underlying tale dramatically resembles that of ‘Terminator,’ ‘Battlestar Galactica,’ and countless others in that artificial life is something that will destroy us, and needs to be feared. And often, this type of artificial life is conflated with artificial intelligence, leading us to fear AI. So let’s first put that stake in the ground in our definition of AI. It is not artificial life. It’s not smart Computers To understand what artificial intelligence is, let’s now turn to computing, and let’s try to understand what that’s all about. We often give computers credit for being smart, a meme brought about undoubtedly by science fiction, with the characters of shows like Star Trek able to talk to their computer and find out just about anything they need to know. Indeed, I remember, when at the ripe age of twelve years old, I got my first computer. I remember plugging it into the TV and starting to write programs in BASIC. A friend came round to see it, and when given a turn, the first thing he typed was “What is the capital of Brazil?”. When the machine replied, “Syntax Error,” he was aghast. Computers were supposed to know everything. And while that misconception has gone away with more exposure of the general public to computing platforms, the underlying belief remains. What computers are very good at is doing lots of monotonous, repetitive tasks very quickly. Programmers can create sets of commands that computers will act on, but in many ways, the actual intelligence and the problem-solving aspect comes from the programmer’s mind and not from the machine. Since the dawn of computing, that’s been the job of programmers. Figure out how to solve a problem, break the steps of the solution down into simple-to-understand commands using a programming language, and then get the machine to do the job. This process has been the bread-and-butter for the Computer Science industry for decades. Until the advent of Machine Learning. It is just another algorithm Now, while the term Machine Learning might make it sound like you are dealing with an artificial lifeform capable of learning in much the same way an actual lifeform would, that’s not the case. Machine Learning is still just a computer doing many calculations to make its program work a bit better, but automating the process instead of needing a programmer to figure it out. Let’s consider an example here. We’ll start with a simple one for a human to program and use that as the basis for understanding how a machine can learn. There are two main scales for measuring temperature globally, and these are Celsius and Fahrenheit. There’s a formula that connects the two of these, so you would figure out how to express that formula using a coding language. So, for example, you might write: def c-to-f(c): f = (c*1.8) + 32 return f You’d come up with this answer is because you probably looked up the formula for converting Celsius to Fahrenheit and then figured out the particular syntax to do that in a programming language such as Python. Consider what would happen if you didn’t have the formula but still needed to figure out the relationship between the two values. Instead, you have a table of readings on both scales. What would you then do? For example, you know that 10 degrees C is 50 degrees F 5 degrees C is 41 degrees F 0 degrees C is 32 degrees F From the 0C = 32F relationship, you might surmise that the F value is 32+(something) times the C value. If that were the case, then you’d look at the 5C = 41F relationship and see that the 41 is 32+(1.8 times 5). You’d then verify this by calculating that the 10C=50F fit the same formula. You now know that the F value is 32 + 1.8 times the C value. Note that there were two parameters in the above function. You calculated ‘f’ by multiplying ‘c’ by something (in this case, 1.8) and adding another something (32). The key to Machine Learning: Pattern Matching The process that we call ‘Machine Learning’ does a very similar thing, but a lot less smartly than you just did. It will guess the two parameters, and then, given the data, it will measure how good or how bad that guess is. That measurement is called a ‘loss.’ It will then strategize guessing to reduce this loss. So, for example, it might guess that f = 10+(10c), initializing both parameters to 10. If it were to do that, then 0C would be 10F, which would be off by 22. Similarly, 5C would be 60F, and 10C would be 110F, both wildly incorrect. So the machine would strategize and use some fancy math to come up with another, better guess, maybe f=15+(5c). It would still be wrong, but less so. Getting closer and closer to the correct answer using brute force mathematics is commonly referred to as Machine Learning. So please, forget all those pictures of child-like robots sitting in a schoolroom. Those aren’t representative of Machine Learning at all! For a trivial example like this, where it’s easy for a human to establish the rules, this may seem pointless. But what about circumstances where it’s tough for a human to see the rules that determine something? For example, what if you look at a picture like this. You can instantly tell that it’s a dog. But how do you have a computer recognize that? How can it tell the difference between a dog and a cat? In a process similar to the above, you can have a computer look at lots of pictures of dogs and cats and use a similar brute-force method to find out what in the data distinguishes one from another. Surprisingly, recent research has shown that with the correct Machine Learning algorithm, computers are often better at humans at distinguishing visual objects. Not only that, researchers have been ab[le to use these algorithms to spot things in images that humans don’t see. There’s a famous story from Google Research. When exploring data from retina scans, a researcher noticed that gender and age data were available, so they wrote a Machine Learning algorithm to figure out a person’s age and birth-assigned gender from a retina scan. It proved to be more accurate than most humans at guessing these details! This technique, Machine Learning, gives us a new way to program computers that behave more like intelligent creatures. So, instead of a picture just being numbers representing the dots that make it up, a computer can now figure out that it contains a cat, a dog, or a diseased retina. Or similarly, it can represent a sound as a type of bird, a regular heartbeat, or an alarm, instead of just a bunch of frequencies. But, it is important to remember they are not alive, and they are not intelligent. And that is why the field is called Artificial intelligence. In the next article in this series, I’ll go deeper into Machine Learning, look in more detail at some applications and how these applications can change the world for the better.</summary></entry><entry><title type="html">Machine Learning for anomaly detection</title><link href="/2021/05/21/io-workshop.html" rel="alternate" type="text/html" title="Machine Learning for anomaly detection" /><published>2021-05-21T00:00:00-07:00</published><updated>2021-05-21T00:00:00-07:00</updated><id>/2021/05/21/io-workshop</id><content type="html" xml:base="/2021/05/21/io-workshop.html">&lt;p&gt;At Google IO this year, I had many activities, one of which was to teach a workshop in anomaly detection. It uses a novel approach – by training an encoder-decoder architecture with electrocardiogram (EKG) values and exploring the reconstruction error.&lt;/p&gt;

&lt;p&gt;The logic was that when an autoencoder model gets trained on only good EKG values, then the reconstruction error for good ones will be small. Thus, an abnormal EKG would have a high reconstruction error, and if this is above a certain threshold, we know that it’s an anomaly.&lt;/p&gt;

&lt;p&gt;Watch the entire workshop here:&lt;/p&gt;
&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/2K3ScZp1dXQ?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Laurence Moroney</name></author><category term="google" /><category term="talks" /><summary type="html">At Google IO this year, I had many activities, one of which was to teach a workshop in anomaly detection. It uses a novel approach – by training an encoder-decoder architecture with electrocardiogram (EKG) values and exploring the reconstruction error. The logic was that when an autoencoder model gets trained on only good EKG values, then the reconstruction error for good ones will be small. Thus, an abnormal EKG would have a high reconstruction error, and if this is above a certain threshold, we know that it’s an anomaly. Watch the entire workshop here:</summary></entry></feed>