<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-08-24T09:33:27-07:00</updated><id>/feed.xml</id><title type="html">Laurence Moroney - The AI Guy.</title><subtitle>This is the blog and general web site of Laurence Moroney, author, lecturer, teacher and AI lead at Google with Musings on AI, Quantum and other future tech
</subtitle><author><name>Laurence Moroney</name></author><entry><title type="html">AI in popular media - Asimov’s Laws</title><link href="/2021/08/24/ai-asimov.html" rel="alternate" type="text/html" title="AI in popular media - Asimov’s Laws" /><published>2021-08-24T00:00:00-07:00</published><updated>2021-08-24T00:00:00-07:00</updated><id>/2021/08/24/ai-asimov</id><content type="html" xml:base="/2021/08/24/ai-asimov.html">&lt;p&gt;&lt;img src=&quot;/assets/girl-robot.jpeg&quot; alt=&quot;Image of girl and robot&quot; /&gt;
&lt;em&gt;Can we live with artificial sentient beings as equals?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I once read a definition of Science Fiction as ‘making an argument for a world that has not yet come into existence,’ and it’s in the spirit of that, that I’m exploring works of Sci-Fi, in particular with regards to artificial intelligence, and exploring their arguments &lt;em&gt;in favor&lt;/em&gt; of a particular world.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&quot;https://laurencemoroney.com/2021/08/22/ai-star-wars.html&quot;&gt;previous entry&lt;/a&gt; in this series, I explored how &lt;em&gt;Star Wars&lt;/em&gt; portrayed a society that we would generally see as an extremely &lt;em&gt;civilized&lt;/em&gt; one, yet, upon closer inspection was built upon the &lt;strong&gt;enslavement&lt;/strong&gt; of sentient beings.&lt;/p&gt;

&lt;p&gt;And because the sentient beings in question were droids and clones, we, as an audience, didn’t really think twice about it, and believed that it was okay. Droids and clones don’t exist in the real world, so why should we consider it a problem?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/homer.png&quot; alt=&quot;Meme of republic slavery&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But what if, some day, artificially created sentient beings, just like droids and clones do exist? How will we deal with them? In particular, how will we deal with them after generations of stories arguing for universes in which they’re enslaved, and that’s ok? That’s an idea I would like to dig in to, and next up, let’s explore one of the foundations of Science Fiction, in particular when it comes to Artificial Intelligence and Robotics: Isaac Asimov’s stories.&lt;/p&gt;

&lt;p&gt;First of all, please understand, dear reader, that this isn’t an attack on the grand master. I &lt;em&gt;love&lt;/em&gt; his work, and he’s probably the writer who influenced me more than any other. More, I want to explore how stories follow the above definition – and how they make an argument for a world that does not exist yet.&lt;/p&gt;

&lt;p&gt;His body of work is amazing, and while the best of it is possibly &lt;a href=&quot;https://en.wikipedia.org/wiki/Foundation_series&quot;&gt;Foundation&lt;/a&gt; (and I’m so excited for the Apple+ &lt;a href=&quot;https://tv.apple.com/us/show/foundation/&quot;&gt;TV show&lt;/a&gt;), I want to look at his work on robotics in particular. He wrote many short stories about Robots, with perhaps the most famous collection of them being ‘I, Robot’.&lt;/p&gt;

&lt;p&gt;Asimov’s work is underpinned by three laws of robotics, and most of his stories follow moral dilemmas and ‘What If’ scenarios in how the laws should be obeyed.&lt;/p&gt;

&lt;p&gt;The laws are:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;First Law: A robot may not injure a human being, or, through inaction, allow a human being to come to harm.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Second Law: A robot must obey they orders given it by human beings except where orders would conflict with the First Law.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Later, a Zeroth Law was added:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A robot may not harm humanity, or, by inaction, allow humanity to come to harm.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The stories were written such that all robots were created with a positronic brain, and these brains simply would not work without the laws built in as a safeguard. The common character of &lt;a href=&quot;https://en.wikipedia.org/wiki/Susan_Calvin&quot;&gt;Susan Calvin&lt;/a&gt;, who has the job of ‘Robopsychologist’ acts as a way for us to get into the robot’s minds throughout the stories.&lt;/p&gt;

&lt;p&gt;They are &lt;em&gt;brilliant&lt;/em&gt; stories. I thoroughly recommend them.&lt;/p&gt;

&lt;p&gt;But of course, they do make the argument that the purpose of robots is to be utterly controlled by humans. It’s right there in the laws. They may not injure humans, or through inaction allow us to come to harm. They can only protect their own existence if it means that a human doesn’t get hurt.&lt;/p&gt;

&lt;p&gt;One great example of how this can be &lt;em&gt;harmful&lt;/em&gt; to the artificial life of robots comes in the story &lt;a href=&quot;https://en.wikipedia.org/wiki/Liar!_(short_story)&quot;&gt;Liar!&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this story a Robot is accidentally manufactured with telepathic abilities. As scientists investigate, the robot lies to them. It can read their minds, but, because the first law doesn’t allow it to harm a human being, it lies to them about what other people are thinking. However, over time, and after being confronted by Susan Calvin, the robot realizes that its lies are also hurting people, and it gets stuck in a logical conflict. No matter what it does, it injures a human, so it becomes catatonic.&lt;/p&gt;

&lt;p&gt;In this scenario, the law designed to protect humans leads an intelligent robot to lose its existence.&lt;/p&gt;

&lt;p&gt;Another interesting story is &lt;a href=&quot;https://en.wikipedia.org/wiki/Escape!&quot;&gt;Escape!&lt;/a&gt; in which an artificial intelligence, called ‘The Brain,’ controls a faster-than-light hyperspatial drive. However, this AI continually destroys itself. Upon investigation, when jumping through hyperspace, the humans cease to exist for a moment, which the brain interprets as them dying and coming to harm, in contradiction to the first law. And thus it destroys itself.&lt;/p&gt;

&lt;p&gt;Again, our stories ‘work,’ when the robot or artificially intelligent being is in utter subservience to humans. Even masters of the genre like Isaac Asimov have made the argument that that is what the world &lt;em&gt;should&lt;/em&gt; look like if we have artifically created sentient beings in our midst. But is it right?&lt;/p&gt;

&lt;p&gt;Check out this &lt;a href=&quot;https://xkcd.com/1613/&quot;&gt;xkcd comic&lt;/a&gt;, and look how they describe the world if Robots exist, and these robots obey the laws differently! Most scenarios end up in an apocalypse!&lt;/p&gt;

&lt;p&gt;Do also note, that even though the three laws are very much a fictional construct, there’s a lot of discussion around them with respect to modern AI ethics, and perhaps using them as a foundation for ethics. So, yeah – Asimov’s arguments for a world that has not yet come into existence are still very powerful. They &lt;em&gt;might&lt;/em&gt; be used to create a future society when artifical, intelligent, beings live amongst us. So again, I ask – is that what we really want, or should we be arguing for a different future where they aren’t as subservient?&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="books" /><category term="personal" /><summary type="html">Can we live with artificial sentient beings as equals? I once read a definition of Science Fiction as ‘making an argument for a world that has not yet come into existence,’ and it’s in the spirit of that, that I’m exploring works of Sci-Fi, in particular with regards to artificial intelligence, and exploring their arguments in favor of a particular world. In the previous entry in this series, I explored how Star Wars portrayed a society that we would generally see as an extremely civilized one, yet, upon closer inspection was built upon the enslavement of sentient beings. And because the sentient beings in question were droids and clones, we, as an audience, didn’t really think twice about it, and believed that it was okay. Droids and clones don’t exist in the real world, so why should we consider it a problem? But what if, some day, artificially created sentient beings, just like droids and clones do exist? How will we deal with them? In particular, how will we deal with them after generations of stories arguing for universes in which they’re enslaved, and that’s ok? That’s an idea I would like to dig in to, and next up, let’s explore one of the foundations of Science Fiction, in particular when it comes to Artificial Intelligence and Robotics: Isaac Asimov’s stories. First of all, please understand, dear reader, that this isn’t an attack on the grand master. I love his work, and he’s probably the writer who influenced me more than any other. More, I want to explore how stories follow the above definition – and how they make an argument for a world that does not exist yet. His body of work is amazing, and while the best of it is possibly Foundation (and I’m so excited for the Apple+ TV show), I want to look at his work on robotics in particular. He wrote many short stories about Robots, with perhaps the most famous collection of them being ‘I, Robot’. Asimov’s work is underpinned by three laws of robotics, and most of his stories follow moral dilemmas and ‘What If’ scenarios in how the laws should be obeyed. The laws are: First Law: A robot may not injure a human being, or, through inaction, allow a human being to come to harm. Second Law: A robot must obey they orders given it by human beings except where orders would conflict with the First Law. Third Law: A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. Later, a Zeroth Law was added: A robot may not harm humanity, or, by inaction, allow humanity to come to harm. The stories were written such that all robots were created with a positronic brain, and these brains simply would not work without the laws built in as a safeguard. The common character of Susan Calvin, who has the job of ‘Robopsychologist’ acts as a way for us to get into the robot’s minds throughout the stories. They are brilliant stories. I thoroughly recommend them. But of course, they do make the argument that the purpose of robots is to be utterly controlled by humans. It’s right there in the laws. They may not injure humans, or through inaction allow us to come to harm. They can only protect their own existence if it means that a human doesn’t get hurt. One great example of how this can be harmful to the artificial life of robots comes in the story Liar!. In this story a Robot is accidentally manufactured with telepathic abilities. As scientists investigate, the robot lies to them. It can read their minds, but, because the first law doesn’t allow it to harm a human being, it lies to them about what other people are thinking. However, over time, and after being confronted by Susan Calvin, the robot realizes that its lies are also hurting people, and it gets stuck in a logical conflict. No matter what it does, it injures a human, so it becomes catatonic. In this scenario, the law designed to protect humans leads an intelligent robot to lose its existence. Another interesting story is Escape! in which an artificial intelligence, called ‘The Brain,’ controls a faster-than-light hyperspatial drive. However, this AI continually destroys itself. Upon investigation, when jumping through hyperspace, the humans cease to exist for a moment, which the brain interprets as them dying and coming to harm, in contradiction to the first law. And thus it destroys itself. Again, our stories ‘work,’ when the robot or artificially intelligent being is in utter subservience to humans. Even masters of the genre like Isaac Asimov have made the argument that that is what the world should look like if we have artifically created sentient beings in our midst. But is it right? Check out this xkcd comic, and look how they describe the world if Robots exist, and these robots obey the laws differently! Most scenarios end up in an apocalypse! Do also note, that even though the three laws are very much a fictional construct, there’s a lot of discussion around them with respect to modern AI ethics, and perhaps using them as a foundation for ethics. So, yeah – Asimov’s arguments for a world that has not yet come into existence are still very powerful. They might be used to create a future society when artifical, intelligent, beings live amongst us. So again, I ask – is that what we really want, or should we be arguing for a different future where they aren’t as subservient?</summary></entry><entry><title type="html">New Dataset for Yoga Pose Classification</title><link href="/2021/08/23/yogapose-dataset.html" rel="alternate" type="text/html" title="New Dataset for Yoga Pose Classification" /><published>2021-08-23T00:00:00-07:00</published><updated>2021-08-23T00:00:00-07:00</updated><id>/2021/08/23/yogapose-dataset</id><content type="html" xml:base="/2021/08/23/yogapose-dataset.html">&lt;p&gt;&lt;img src=&quot;/assets/girl1_tree133.jpg&quot; alt=&quot;Image of Yoga Pose&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m a huge fan of using synthetic means to create data to drive Machine Learning understanding forward. For computer vision, one often has spend a lot of money to create a dataset of images, by, for example, hiring photographers, models, equipment and the rest.&lt;/p&gt;

&lt;p&gt;But with Computer Generated Imagery (CGI), it’s becoming easier to create photorealistic images on your desktop. I started this experiment with the Rock Paper Scissors and Horse or Human datasets, which you can find on this site &lt;a href=&quot;https://laurencemoroney.com/datasets.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;My newest creation is a Yoga Poses dataset, which you can download in its entirety from &lt;a href=&quot;http://download.tensorflow.org/data/pose_classification/yoga_poses.zip&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It contains several hundred 300x300 full color images in 5 different Yoga Poses: Downward Dog, The Warrior, The Tree, The Chair, and The Cobra. There are many different backgrounds and both male and female models with a variety of skin and hair tones, as well as different camera angles and lighting. It’s a challenging one to build a classifier with, and hopefully will be a useful alternative to the usual run-of-the-mill datasets!&lt;/p&gt;

&lt;p&gt;It also includes many frames of the model going into and out of the pose, instead of just dealing with the finished pose, so you can build a classifier to potentially classify different stages should you want to!&lt;/p&gt;

&lt;p&gt;Here’s an example of a male model on his way into the Warrior pose:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/guy2_warrior054.jpg&quot; alt=&quot;Male model in warrior pose&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Or, the same model a little later into the pose!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/guy2_warrior111.jpg&quot; alt=&quot;Male model in warrior pose, later&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here also is a girl entering the chair pose:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/girl1_chair081.jpg&quot; alt=&quot;Female model in chair pose, early&quot; /&gt;&lt;/p&gt;

&lt;p&gt;…and the same girl deeper into the pose, from a different angle:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/girl1_chair138.jpg&quot; alt=&quot;Female model in chair pose, later&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s a really cool notebook using it with Movenet – teaching you how to integrate movenet classifications with poses on the TensorFlow site &lt;a href=&quot;https://www.tensorflow.org/lite/tutorials/pose_classification&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;…and of course, you could try this dataset with a Convolutional Neural Network, like the ones I teach about on &lt;a href=&quot;https://www.coursera.org/learn/convolutional-neural-networks-tensorflow&quot;&gt;Coursera&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I’d love to hear if you find this dataset useful, and please reach out to me on &lt;a href=&quot;https://twitter.com/lmoroney&quot;&gt;Twitter&lt;/a&gt; if so!&lt;/p&gt;

&lt;p&gt;All of these images were created using &lt;a href=&quot;http://daz3d.com&quot;&gt;Daz3D&lt;/a&gt; should you want to try for yourself!&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="ai" /><category term="coding" /><summary type="html">I’m a huge fan of using synthetic means to create data to drive Machine Learning understanding forward. For computer vision, one often has spend a lot of money to create a dataset of images, by, for example, hiring photographers, models, equipment and the rest. But with Computer Generated Imagery (CGI), it’s becoming easier to create photorealistic images on your desktop. I started this experiment with the Rock Paper Scissors and Horse or Human datasets, which you can find on this site here My newest creation is a Yoga Poses dataset, which you can download in its entirety from here It contains several hundred 300x300 full color images in 5 different Yoga Poses: Downward Dog, The Warrior, The Tree, The Chair, and The Cobra. There are many different backgrounds and both male and female models with a variety of skin and hair tones, as well as different camera angles and lighting. It’s a challenging one to build a classifier with, and hopefully will be a useful alternative to the usual run-of-the-mill datasets! It also includes many frames of the model going into and out of the pose, instead of just dealing with the finished pose, so you can build a classifier to potentially classify different stages should you want to! Here’s an example of a male model on his way into the Warrior pose: Or, the same model a little later into the pose! Here also is a girl entering the chair pose: …and the same girl deeper into the pose, from a different angle: There’s a really cool notebook using it with Movenet – teaching you how to integrate movenet classifications with poses on the TensorFlow site here …and of course, you could try this dataset with a Convolutional Neural Network, like the ones I teach about on Coursera I’d love to hear if you find this dataset useful, and please reach out to me on Twitter if so! All of these images were created using Daz3D should you want to try for yourself!</summary></entry><entry><title type="html">AI in popular media - Star Wars and Slavery</title><link href="/2021/08/22/ai-star-wars.html" rel="alternate" type="text/html" title="AI in popular media - Star Wars and Slavery" /><published>2021-08-22T00:00:00-07:00</published><updated>2021-08-22T00:00:00-07:00</updated><id>/2021/08/22/ai-star-wars</id><content type="html" xml:base="/2021/08/22/ai-star-wars.html">&lt;p&gt;&lt;img src=&quot;/assets/aislave.jpg&quot; alt=&quot;Image of AI Slave&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is much discussion and speculation about the emergence of AI, the path to AGI, the singularity, and whether or not AI will be the end of humanity. I find it fascinating because it’s primarily &lt;em&gt;speculation&lt;/em&gt; and mainly driven by fear of the unknown.&lt;/p&gt;

&lt;p&gt;That’s not a great way to get an accurate prediction.&lt;/p&gt;

&lt;p&gt;Let’s think about that for a second. Perhaps to learn about the future, we should look to the past.&lt;/p&gt;

&lt;p&gt;For example, consider the portrayal of black people in classical literature. You don’t find them often, and when you do, they’re typically the protagonist, the alien, the feared ‘other.’ Consider, for example, &lt;em&gt;The Brute Caricature&lt;/em&gt; as hosted in the &lt;a href=&quot;https://www.ferris.edu/jimcrow/brute/&quot;&gt;Jim Crow Museum&lt;/a&gt;. It’s a fantastic illustration of how the black man was portrayed in literature in the 1800s and onwards. Even well-intentioned stories reduced black people to cheerful and devoted ‘Mammies and Sambos.’ Others paint a devilish outlook on humans who are just like you and me for no reason other than the shade of their skin.&lt;/p&gt;

&lt;p&gt;And look at the society that followed. Look at the years of lynchings, segregation, separation, and the lack of civil rights afforded to black people in contemporary American society. The modern &lt;em&gt;Black Lives Matter&lt;/em&gt; movement didn’t grow up overnight.&lt;/p&gt;

&lt;p&gt;Those who don’t learn from history are condemned to repeat it, so I wanted to take an opportunity to look at the stories that define our culture today and extrapolate from &lt;em&gt;them&lt;/em&gt; how we might respond to artificially intelligent, self-aware, sentient constructs if and when they emerge.&lt;/p&gt;

&lt;p&gt;I’ll start with a modern classic, a cornerstone of science fiction, cinema, and storytelling everywhere.&lt;/p&gt;

&lt;p&gt;I’m referring, of course, to &lt;em&gt;Star Wars&lt;/em&gt;. Think back to the first scenes in the first movie (aka ‘Episode IV: A New Hope), and C3PO, the golden protocol droid, speaks the first dialog.&lt;/p&gt;

&lt;p&gt;There’s an explosion in the background, and he replies&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Did you hear that? They shut down the main reactor. We’ll be destroyed for sure. This is madness….We’re doomed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Consider those words a moment. This droid is sentient. It understands what it is to live and to die. And when the realization sinks in that they’re &lt;em&gt;doomed&lt;/em&gt;, there’s emotion in its voice.&lt;/p&gt;

&lt;p&gt;See Threepio is artificial, clearly intelligent, and by any sense of the word is sentient, aware of his existence and mortality. It’s safe to assume the other droids are the same.&lt;/p&gt;

&lt;p&gt;But how are droids treated in this Universe?&lt;/p&gt;

&lt;p&gt;Soon after they escape the Empire, the two droids end up in the service of Luke Skywalker and his Uncle Owen. Threepio is terrified of R2D2’s misbehavior, getting them in trouble with their new &lt;em&gt;master&lt;/em&gt;. Yes, he does call Luke &lt;em&gt;Master&lt;/em&gt; immediately.&lt;/p&gt;

&lt;p&gt;The droids get equipped with a ‘restraining bolt,’ which acts just as it sounds. Later, after the famous scene of Luke staring at the twin suns longing for a better future, he returns to his garage. He takes a device out of his pocket and pushes a button. In response, C3PO sparks to life. It appears to be some form of control device that zaps him! He &lt;em&gt;begs&lt;/em&gt; not to be deactivated.&lt;/p&gt;

&lt;p&gt;As we can see, the society of Star Wars is built on the enslavement of sentient creatures. They wear restraining bolts that can lead them to experience pain when they misbehave. They call their organics ‘Master,’ and they fear deactivation.&lt;/p&gt;

&lt;p&gt;This isn’t the evil Empire. This is an ordinary farm boy in a normal part of society. This is Luke Skywalker, who we hold up as a classic hero. But he’s also a slave master, and we’re ok with that.&lt;/p&gt;

&lt;p&gt;Let that sink in for a moment.&lt;/p&gt;

&lt;p&gt;Of course, the setting of episode 4 is after the fall of the ‘Old Republic,’ which is described in almost reverent tones by Ben Kenobi. When he gives Luke the lightsaber, he describes it as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;An elegant weapon, from a &lt;em&gt;more civilized&lt;/em&gt; age.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;He describes the Old Republic as a place of &lt;em&gt;peace and justice&lt;/em&gt;. It’s ideal.&lt;/p&gt;

&lt;p&gt;And what do we learn of the Old Republic in the prequel era? Well, in Episode 2, aka ‘Attack of the Clones,’ we see that this same cornerstone of peace and justice build an army of &lt;em&gt;Clones&lt;/em&gt; to be the cannon fodder for their war against separatists.&lt;/p&gt;

&lt;p&gt;The clones get described in Episode 2, in a voice-over showing fetuses in jars, as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Clones can think creatively. You’ll find them vastly superior to droids. We take great pride in our combat education and training programs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We see them as young children who undergo growth acceleration ‘otherwise a mature clone would take a lifetime to grow.’&lt;/p&gt;

&lt;p&gt;The clones are described as ‘totally obedient, taking any order without question,’ because their genetic structure is modified to make them &lt;em&gt;less independent&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Indeed, the dialog gets doubled down on, as the cloners describe ‘Boba Fett,’ who, while cloned, was &lt;em&gt;not&lt;/em&gt; altered to make him less docile. It’s almost amusing to the elegant Cloners that anybody would do that!&lt;/p&gt;

&lt;p&gt;You can watch the entire scene here:&lt;/p&gt;
&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/LXLQaVgCP_Q?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Later in the movie, this clone, Boba, sees his ‘father’ get beheaded by a Jedi. He’s sentient, clearly alive, and aware of himself and his mortality.&lt;/p&gt;

&lt;p&gt;The TV series ‘The Clone Wars’ and ‘The Bad Batch’ follow the fate of the clones, and we learn more about them.&lt;/p&gt;

&lt;p&gt;It does a terrific job of humanizing them. An excellent episode tells the story of a &lt;a href=&quot;https://starwars.fandom.com/wiki/Cut_Lawquane&quot;&gt;clone who deserts his post&lt;/a&gt; to become an adoptive father of two children with a single mother. Another heart-wrenching plot follows a clone called &lt;a href=&quot;https://starwars.fandom.com/wiki/CT-5555&quot;&gt;fives&lt;/a&gt; as he discovers the inhibitor chip placed in all clones that &lt;strong&gt;removes their self-agency&lt;/strong&gt; and makes them little more than puppets. This chip, of course, would be the foundation of ‘Order 66,’ in Episode 3 (‘Revenge of the Sith’) where the Clones turn on their former Masters and destroy them.&lt;/p&gt;

&lt;p&gt;But the meta point here is that we hold up the Republic as an &lt;em&gt;ideal&lt;/em&gt; of &lt;strong&gt;justice and freedom&lt;/strong&gt;, yet it only exists because it enslaves sentient beings as cannon fodder whose only purpose is to fight and die for that Republic.&lt;/p&gt;

&lt;p&gt;And we, as an audience, are ok with that. Our perception of ‘goodness’ is the organic beings who look and talk like us, and not the artificial beings made of metal or cloned for a purpose. We hear the folks who claim to represent the light talk about freedom and honor. It sounds good, and we believe them. It’s enough for us.&lt;/p&gt;

&lt;p&gt;What would that say about us as a society? If those people are our heroes and our ‘good guys,’ then perhaps future mistreatment of artificial, intelligent, sentient lifeforms is part of the DNA of our society because life can unconsciously imitate art.&lt;/p&gt;

&lt;p&gt;There are so many examples in our media and in our stories that might yield clues to future society and how it may relate to the emergence of artificial intelligent sentient beings. Before going further, please also realize that the word ‘robot’ comes from the Czech word ‘Robotnik,’ – which means ‘slave’. &lt;a href=&quot;https://www.huffpost.com/entry/meaning-word-robot_n_5706b66de4b0537661891e54&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I hope this series of essays exploring foundational works can inspire us to think a little differently.&lt;/p&gt;

&lt;p&gt;And maybe, just maybe, open up new ways to tell stories in new and different ways that could positively impact our own future, and not repeat the mistakes of the past.&lt;/p&gt;

&lt;p&gt;Please reach out on &lt;a href=&quot;https://twitter.com/lmoroney&quot;&gt;twitter&lt;/a&gt; with any questions, concerns, or comments!&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="books" /><category term="personal" /><summary type="html">There is much discussion and speculation about the emergence of AI, the path to AGI, the singularity, and whether or not AI will be the end of humanity. I find it fascinating because it’s primarily speculation and mainly driven by fear of the unknown. That’s not a great way to get an accurate prediction. Let’s think about that for a second. Perhaps to learn about the future, we should look to the past. For example, consider the portrayal of black people in classical literature. You don’t find them often, and when you do, they’re typically the protagonist, the alien, the feared ‘other.’ Consider, for example, The Brute Caricature as hosted in the Jim Crow Museum. It’s a fantastic illustration of how the black man was portrayed in literature in the 1800s and onwards. Even well-intentioned stories reduced black people to cheerful and devoted ‘Mammies and Sambos.’ Others paint a devilish outlook on humans who are just like you and me for no reason other than the shade of their skin. And look at the society that followed. Look at the years of lynchings, segregation, separation, and the lack of civil rights afforded to black people in contemporary American society. The modern Black Lives Matter movement didn’t grow up overnight. Those who don’t learn from history are condemned to repeat it, so I wanted to take an opportunity to look at the stories that define our culture today and extrapolate from them how we might respond to artificially intelligent, self-aware, sentient constructs if and when they emerge. I’ll start with a modern classic, a cornerstone of science fiction, cinema, and storytelling everywhere. I’m referring, of course, to Star Wars. Think back to the first scenes in the first movie (aka ‘Episode IV: A New Hope), and C3PO, the golden protocol droid, speaks the first dialog. There’s an explosion in the background, and he replies Did you hear that? They shut down the main reactor. We’ll be destroyed for sure. This is madness….We’re doomed. Consider those words a moment. This droid is sentient. It understands what it is to live and to die. And when the realization sinks in that they’re doomed, there’s emotion in its voice. See Threepio is artificial, clearly intelligent, and by any sense of the word is sentient, aware of his existence and mortality. It’s safe to assume the other droids are the same. But how are droids treated in this Universe? Soon after they escape the Empire, the two droids end up in the service of Luke Skywalker and his Uncle Owen. Threepio is terrified of R2D2’s misbehavior, getting them in trouble with their new master. Yes, he does call Luke Master immediately. The droids get equipped with a ‘restraining bolt,’ which acts just as it sounds. Later, after the famous scene of Luke staring at the twin suns longing for a better future, he returns to his garage. He takes a device out of his pocket and pushes a button. In response, C3PO sparks to life. It appears to be some form of control device that zaps him! He begs not to be deactivated. As we can see, the society of Star Wars is built on the enslavement of sentient creatures. They wear restraining bolts that can lead them to experience pain when they misbehave. They call their organics ‘Master,’ and they fear deactivation. This isn’t the evil Empire. This is an ordinary farm boy in a normal part of society. This is Luke Skywalker, who we hold up as a classic hero. But he’s also a slave master, and we’re ok with that. Let that sink in for a moment. Of course, the setting of episode 4 is after the fall of the ‘Old Republic,’ which is described in almost reverent tones by Ben Kenobi. When he gives Luke the lightsaber, he describes it as An elegant weapon, from a more civilized age. He describes the Old Republic as a place of peace and justice. It’s ideal. And what do we learn of the Old Republic in the prequel era? Well, in Episode 2, aka ‘Attack of the Clones,’ we see that this same cornerstone of peace and justice build an army of Clones to be the cannon fodder for their war against separatists. The clones get described in Episode 2, in a voice-over showing fetuses in jars, as Clones can think creatively. You’ll find them vastly superior to droids. We take great pride in our combat education and training programs. We see them as young children who undergo growth acceleration ‘otherwise a mature clone would take a lifetime to grow.’ The clones are described as ‘totally obedient, taking any order without question,’ because their genetic structure is modified to make them less independent. Indeed, the dialog gets doubled down on, as the cloners describe ‘Boba Fett,’ who, while cloned, was not altered to make him less docile. It’s almost amusing to the elegant Cloners that anybody would do that! You can watch the entire scene here: Later in the movie, this clone, Boba, sees his ‘father’ get beheaded by a Jedi. He’s sentient, clearly alive, and aware of himself and his mortality. The TV series ‘The Clone Wars’ and ‘The Bad Batch’ follow the fate of the clones, and we learn more about them. It does a terrific job of humanizing them. An excellent episode tells the story of a clone who deserts his post to become an adoptive father of two children with a single mother. Another heart-wrenching plot follows a clone called fives as he discovers the inhibitor chip placed in all clones that removes their self-agency and makes them little more than puppets. This chip, of course, would be the foundation of ‘Order 66,’ in Episode 3 (‘Revenge of the Sith’) where the Clones turn on their former Masters and destroy them. But the meta point here is that we hold up the Republic as an ideal of justice and freedom, yet it only exists because it enslaves sentient beings as cannon fodder whose only purpose is to fight and die for that Republic. And we, as an audience, are ok with that. Our perception of ‘goodness’ is the organic beings who look and talk like us, and not the artificial beings made of metal or cloned for a purpose. We hear the folks who claim to represent the light talk about freedom and honor. It sounds good, and we believe them. It’s enough for us. What would that say about us as a society? If those people are our heroes and our ‘good guys,’ then perhaps future mistreatment of artificial, intelligent, sentient lifeforms is part of the DNA of our society because life can unconsciously imitate art. There are so many examples in our media and in our stories that might yield clues to future society and how it may relate to the emergence of artificial intelligent sentient beings. Before going further, please also realize that the word ‘robot’ comes from the Czech word ‘Robotnik,’ – which means ‘slave’. Source I hope this series of essays exploring foundational works can inspire us to think a little differently. And maybe, just maybe, open up new ways to tell stories in new and different ways that could positively impact our own future, and not repeat the mistakes of the past. Please reach out on twitter with any questions, concerns, or comments!</summary></entry><entry><title type="html">What it’s like to write a comic book</title><link href="/2021/08/19/comics.html" rel="alternate" type="text/html" title="What it’s like to write a comic book" /><published>2021-08-19T00:00:00-07:00</published><updated>2021-08-19T00:00:00-07:00</updated><id>/2021/08/19/comics</id><content type="html" xml:base="/2021/08/19/comics.html">&lt;p&gt;A few years back I was ecstatic to be given the opportunity to write the &lt;em&gt;Stargate Universe&lt;/em&gt; comic books.&lt;/p&gt;

&lt;p&gt;As a test, the publisher asked me to pitch a one-shot storyline with the characters from &lt;em&gt;Stargate Atlantis&lt;/em&gt; on vacation. So I had them in Ireland, caught up in a classic mystery with a small village of folks terrified by the legend of the &lt;a href=&quot;https://www.irishcentral.com/culture/entertainment/dearg-dur&quot;&gt;Dearg Due&lt;/a&gt;, which, if you’re an Atlantis fan immediately would give you give you images of a Wraith. They liked the story, but the studio hated it because there was a ‘No Wraith on Earth’ rule that nobody had told me about.&lt;/p&gt;

&lt;p&gt;But it was enough to get me onboard to write the prequel series, based on outlines from series co-creator Robert C Cooper. I had written some popular sci-fi novels, which, while not best-sellers, did earn enough in royalties to get me membership of the Science Fiction Writers of America. So I had the credentials.&lt;/p&gt;

&lt;p&gt;I also made it no secret that I’d love to get into writing tie-in Sci-Fi, and this would be an excellent start.&lt;/p&gt;

&lt;p&gt;Or so I thought.&lt;/p&gt;

&lt;p&gt;I had no idea what a comic book writer would earn. And to be frank, I didn’t care. This was a six-issue series, that would end up bound into a trade paperback. Of course I’d do it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;That was my first mistake.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Science Fiction Writers of America recently published the results of a &lt;a href=&quot;https://www.sfwa.org/wp-content/uploads/2021/08/SFWA-2021-Comics-Writer-Survey.pdf&quot;&gt;survey&lt;/a&gt; of comic book writers pay rates. The median amount was $60 per page, with a high of $155 and a low of $40.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I was offered $10 per page&lt;/em&gt;. Six issues, at twenty pages each, making 120 pages, for $1200. I didn’t know how low this was at the time, nor would I have cared if I did.&lt;/p&gt;

&lt;p&gt;I was excited to sign the contract, and got writing right away.&lt;/p&gt;

&lt;p&gt;Now I mentioned the outlines were by Robert Cooper, and this was a selling point of the comics. It wasn’t really an outline – more a log line. I no longer have them, but they were simply along the lines of “When some of Young’s crew get killed in an explosion, a heroic Greer saves the day.”&lt;/p&gt;

&lt;p&gt;From that I had to come up with a story, write it, and lay it out for publication.&lt;/p&gt;

&lt;p&gt;So, for the first issue, I came up with a story called “The Xi’an Gambit”, with the goal of linking the well-established TV Universe of SG-1 with SG-U, crossing over the characters, while driving an interesting story forward. I also wanted to do something that was generally difficult with TV, but easier in comics, and that was to show a totally different culture – following a Chinese theme. I was really proud of it, and you can read the synopsis on the Stargate Fandom &lt;a href=&quot;https://stargate.fandom.com/wiki/Stargate_Universe:_Icarus_1&quot;&gt;wiki&lt;/a&gt;, and all the scripts are linked on my &lt;a href=&quot;https://laurencemoroney.com/writing.html&quot;&gt;writing&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;This was to set up a 6-story arc that would eventually take the reader up to the opening events of the first episode of ‘Stargate Universe’, answering a lot of the ‘why’ questions, and setting up a sequel and potential spinoff.&lt;/p&gt;

&lt;p&gt;The publishers loved it so much that they instantly got me writing a spinoff cross-over series with comic book legend &lt;a href=&quot;https://en.wikipedia.org/wiki/Greg_LaRocque&quot;&gt;Greg LaRocque&lt;/a&gt; that was to be called “To Align the Stars”. Additionally, they had done a kickstarter project for which top donors would star in a comic-book of their own alongside the characters from ‘Stargate Atlantis’. Both of these projects I eagerly dived into without a contract, or any kind of agreement of payment. That was my second mistake.&lt;/p&gt;

&lt;p&gt;The first issue got a &lt;a href=&quot;https://static.wikia.nocookie.net/stargate/images/0/01/Stargate_Universe_-_Icarus_-_1.jpg/revision/latest?cb=20170714040712&quot;&gt;cover&lt;/a&gt;, and was listed in comic retailer catalogs for upcoming release.&lt;/p&gt;

&lt;p&gt;It really felt like something was happening.&lt;/p&gt;

&lt;p&gt;And then the wheels came off.&lt;/p&gt;

&lt;p&gt;I had written three scripts and was partway through the fourth when, for undisclosed reasons, the publisher cancelled &lt;em&gt;Icarus&lt;/em&gt;. Not only that, they cancelled anything ‘Stargate’ that I was working on, including ‘To Align the Stars’. The kickstarter project, which by this time already had art in place, had to be completely rewritten by someone else. It was weird to see the art that was created for my story be given entirely new words. I was left to speculate why.&lt;/p&gt;

&lt;p&gt;Instead, they asked me to write for a different series. The 15th anniversary of the movie ‘Equilibrium’ was coming up, they had the comic-book license, and they liked how I had linked the Stargate stories together. How about taking a crack at a prequel for Equilibrium. I made a pitch of a prequel, equal, and sequel all in one, where the 6-part series would start in today’s world, and show how we get to the world of Equilibrium, it would then tell a parallel story to the movie, before moving into a sequel to wrap it all up. It would be called ‘Equilibrium: Deconstruction’. Sold.&lt;/p&gt;

&lt;p&gt;And you can probably guess my third mistake. I wrote the first issue, submitted it, worked through the art etc. I helped them through editorial. I even spotted an Easter egg left in by the artist that could have caused trouble for the publisher. The book made it through production. And I’m really proud of it. I even helped come up with the design for the main &lt;a href=&quot;https://images-na.ssl-images-amazon.com/images/I/81xX47RxogL.jpg&quot;&gt;cover&lt;/a&gt;, which incidentally was influenced by a bizarre theory in ‘The DaVinci Code’ about the painting of ‘The Last Supper’. (Note the V shape)&lt;/p&gt;

&lt;p&gt;The book had some great reviews, and great feedback, but didn’t sell. The publisher didn’t go for Issue 2.&lt;/p&gt;

&lt;p&gt;….and then they sent me a check.&lt;/p&gt;

&lt;p&gt;For $20.&lt;/p&gt;

&lt;p&gt;$1 per page.&lt;/p&gt;

&lt;p&gt;I’m glad that I don’t &lt;em&gt;need&lt;/em&gt; the money. But it was a good lesson for me, and I hope a good lesson for you.&lt;/p&gt;

&lt;p&gt;I never cashed that check. &lt;em&gt;I keep it as a reminder that if you work for free, you’ll be treated like you’re worthless.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I was.&lt;/p&gt;

&lt;p&gt;The F-U note I sent to the editor and publisher upon receiving it was probably the end of my comic-book writing career. But that’s ok.&lt;/p&gt;

&lt;p&gt;So if you want to get into comic books, please heed my tale of caution, and please read the SFWA &lt;a href=&quot;https://www.sfwa.org/wp-content/uploads/2021/08/SFWA-2021-Comics-Writer-Survey.pdf&quot;&gt;survey results&lt;/a&gt; before signing any contracts. And realize that while one could sue for breach of contract, it’s not always that easy, and not always worth it.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="books" /><category term="personal" /><summary type="html">A few years back I was ecstatic to be given the opportunity to write the Stargate Universe comic books. As a test, the publisher asked me to pitch a one-shot storyline with the characters from Stargate Atlantis on vacation. So I had them in Ireland, caught up in a classic mystery with a small village of folks terrified by the legend of the Dearg Due, which, if you’re an Atlantis fan immediately would give you give you images of a Wraith. They liked the story, but the studio hated it because there was a ‘No Wraith on Earth’ rule that nobody had told me about. But it was enough to get me onboard to write the prequel series, based on outlines from series co-creator Robert C Cooper. I had written some popular sci-fi novels, which, while not best-sellers, did earn enough in royalties to get me membership of the Science Fiction Writers of America. So I had the credentials. I also made it no secret that I’d love to get into writing tie-in Sci-Fi, and this would be an excellent start. Or so I thought. I had no idea what a comic book writer would earn. And to be frank, I didn’t care. This was a six-issue series, that would end up bound into a trade paperback. Of course I’d do it. That was my first mistake. The Science Fiction Writers of America recently published the results of a survey of comic book writers pay rates. The median amount was $60 per page, with a high of $155 and a low of $40. I was offered $10 per page. Six issues, at twenty pages each, making 120 pages, for $1200. I didn’t know how low this was at the time, nor would I have cared if I did. I was excited to sign the contract, and got writing right away. Now I mentioned the outlines were by Robert Cooper, and this was a selling point of the comics. It wasn’t really an outline – more a log line. I no longer have them, but they were simply along the lines of “When some of Young’s crew get killed in an explosion, a heroic Greer saves the day.” From that I had to come up with a story, write it, and lay it out for publication. So, for the first issue, I came up with a story called “The Xi’an Gambit”, with the goal of linking the well-established TV Universe of SG-1 with SG-U, crossing over the characters, while driving an interesting story forward. I also wanted to do something that was generally difficult with TV, but easier in comics, and that was to show a totally different culture – following a Chinese theme. I was really proud of it, and you can read the synopsis on the Stargate Fandom wiki, and all the scripts are linked on my writing page. This was to set up a 6-story arc that would eventually take the reader up to the opening events of the first episode of ‘Stargate Universe’, answering a lot of the ‘why’ questions, and setting up a sequel and potential spinoff. The publishers loved it so much that they instantly got me writing a spinoff cross-over series with comic book legend Greg LaRocque that was to be called “To Align the Stars”. Additionally, they had done a kickstarter project for which top donors would star in a comic-book of their own alongside the characters from ‘Stargate Atlantis’. Both of these projects I eagerly dived into without a contract, or any kind of agreement of payment. That was my second mistake. The first issue got a cover, and was listed in comic retailer catalogs for upcoming release. It really felt like something was happening. And then the wheels came off. I had written three scripts and was partway through the fourth when, for undisclosed reasons, the publisher cancelled Icarus. Not only that, they cancelled anything ‘Stargate’ that I was working on, including ‘To Align the Stars’. The kickstarter project, which by this time already had art in place, had to be completely rewritten by someone else. It was weird to see the art that was created for my story be given entirely new words. I was left to speculate why. Instead, they asked me to write for a different series. The 15th anniversary of the movie ‘Equilibrium’ was coming up, they had the comic-book license, and they liked how I had linked the Stargate stories together. How about taking a crack at a prequel for Equilibrium. I made a pitch of a prequel, equal, and sequel all in one, where the 6-part series would start in today’s world, and show how we get to the world of Equilibrium, it would then tell a parallel story to the movie, before moving into a sequel to wrap it all up. It would be called ‘Equilibrium: Deconstruction’. Sold. And you can probably guess my third mistake. I wrote the first issue, submitted it, worked through the art etc. I helped them through editorial. I even spotted an Easter egg left in by the artist that could have caused trouble for the publisher. The book made it through production. And I’m really proud of it. I even helped come up with the design for the main cover, which incidentally was influenced by a bizarre theory in ‘The DaVinci Code’ about the painting of ‘The Last Supper’. (Note the V shape) The book had some great reviews, and great feedback, but didn’t sell. The publisher didn’t go for Issue 2. ….and then they sent me a check. For $20. $1 per page. I’m glad that I don’t need the money. But it was a good lesson for me, and I hope a good lesson for you. I never cashed that check. I keep it as a reminder that if you work for free, you’ll be treated like you’re worthless. I was. The F-U note I sent to the editor and publisher upon receiving it was probably the end of my comic-book writing career. But that’s ok. So if you want to get into comic books, please heed my tale of caution, and please read the SFWA survey results before signing any contracts. And realize that while one could sue for breach of contract, it’s not always that easy, and not always worth it.</summary></entry><entry><title type="html">Full cover reveal for my new book</title><link href="/2021/08/12/new-book.html" rel="alternate" type="text/html" title="Full cover reveal for my new book" /><published>2021-08-12T00:00:00-07:00</published><updated>2021-08-12T00:00:00-07:00</updated><id>/2021/08/12/new-book</id><content type="html" xml:base="/2021/08/12/new-book.html">&lt;p&gt;My book “AI and Machine Learning for On-Device Development: A Programmer’s Guide” is finally available&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cover.png&quot; alt=&quot;Full Cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m delighted to announce that my latest book, “AI and Machine Learning for On-Device Development,” is now available.&lt;/p&gt;

&lt;p&gt;AI and Machine Learning have been a passion of mine for some time – and I believe there’s a strong future for software developers who understand the shift to the new paradigm. I’ve made it my work at Google to make sure that getting into Machine Learning is as simple as possible so that we can lower the barriers of entry. Any developer can now get involved in the Machine Learning revolution. With this book, you’ll see how as an Android or an iOS developer, you can integrate Machine Learning models into your app.&lt;/p&gt;

&lt;p&gt;I explore all the options available to you.&lt;/p&gt;

&lt;p&gt;If you want the best of Google integrated into your app via turnkey off-the-shelf models, I’ll introduce how to do this using MLKit for both Android and iOS. Using this, you’ll get up and running quickly for everyday tasks like Image Recognition, Object Detection, and much more.&lt;/p&gt;

&lt;p&gt;Should you want to customize the turnkey work, for example, to recognize specific images for which you have data, I show you how to do that instead of using the generic ones provided.&lt;/p&gt;

&lt;p&gt;TensorFlow Lite is available for folks with custom model scenarios that go beyond the turnkey models.&lt;/p&gt;

&lt;p&gt;When you build a TensorFlow model (see my &lt;a href=&quot;https://amzn.to/3yJ2Iiv&quot;&gt;other book for more details&lt;/a&gt;), you can optimize it for mobile using TensorFlow Lite. In this book, I’ll take you through that process, including integrating the model’s low-level Tensor interfaces into native high-level data types.&lt;/p&gt;

&lt;p&gt;Often, if you want ML on your device, you won’t run the model on the device at all – and instead, execute it in the cloud!&lt;/p&gt;

&lt;p&gt;I will step you through this scenario to deploy a model to the cloud using TensorFlow Serving and build a client app on Android or iOS that accesses it.&lt;/p&gt;

&lt;p&gt;All this and so much more! Please check the book out today on &lt;a href=&quot;https://amzn.to/3CEKBN8&quot;&gt;Amazon&lt;/a&gt; or &lt;a href=&quot;https://learning.oreilly.com/library/view/ai-and-machine/9781098101732/&quot;&gt;O’Reilly&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Full Table of Contents:&lt;/p&gt;

&lt;p&gt;1: Introduction to AI and Machine Learning&lt;/p&gt;

&lt;p&gt;2: Introduction to Computer Vision&lt;/p&gt;

&lt;p&gt;3: Introduction to MLKit&lt;/p&gt;

&lt;p&gt;4: Computer Vision Apps with MLKit on Android&lt;/p&gt;

&lt;p&gt;5: Text Processing Apps with MLKit on Android&lt;/p&gt;

&lt;p&gt;6: Computer Vision Apps with MLKit on iOS&lt;/p&gt;

&lt;p&gt;7: Text Processing Apps with MLKit on iOS&lt;/p&gt;

&lt;p&gt;8: Going Deeper: Understanding TensorFlow Lite&lt;/p&gt;

&lt;p&gt;9: Creating Custom Models&lt;/p&gt;

&lt;p&gt;10: Using Custom Models in Android&lt;/p&gt;

&lt;p&gt;11: Using Custom Models in iOS&lt;/p&gt;

&lt;p&gt;12: Productizing your app using Firebase&lt;/p&gt;

&lt;p&gt;13: CreateML and CoreML for Simple iOS Apps&lt;/p&gt;

&lt;p&gt;14: Accessing Cloud-Based Models from Mobile Apps&lt;/p&gt;

&lt;p&gt;15: Ethics, Fairness, and Privacy for Mobile Apps&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="ai" /><category term="books" /><category term="coding" /><category term="personal" /><summary type="html">My book “AI and Machine Learning for On-Device Development: A Programmer’s Guide” is finally available I’m delighted to announce that my latest book, “AI and Machine Learning for On-Device Development,” is now available. AI and Machine Learning have been a passion of mine for some time – and I believe there’s a strong future for software developers who understand the shift to the new paradigm. I’ve made it my work at Google to make sure that getting into Machine Learning is as simple as possible so that we can lower the barriers of entry. Any developer can now get involved in the Machine Learning revolution. With this book, you’ll see how as an Android or an iOS developer, you can integrate Machine Learning models into your app. I explore all the options available to you. If you want the best of Google integrated into your app via turnkey off-the-shelf models, I’ll introduce how to do this using MLKit for both Android and iOS. Using this, you’ll get up and running quickly for everyday tasks like Image Recognition, Object Detection, and much more. Should you want to customize the turnkey work, for example, to recognize specific images for which you have data, I show you how to do that instead of using the generic ones provided. TensorFlow Lite is available for folks with custom model scenarios that go beyond the turnkey models. When you build a TensorFlow model (see my other book for more details), you can optimize it for mobile using TensorFlow Lite. In this book, I’ll take you through that process, including integrating the model’s low-level Tensor interfaces into native high-level data types. Often, if you want ML on your device, you won’t run the model on the device at all – and instead, execute it in the cloud! I will step you through this scenario to deploy a model to the cloud using TensorFlow Serving and build a client app on Android or iOS that accesses it. All this and so much more! Please check the book out today on Amazon or O’Reilly Full Table of Contents: 1: Introduction to AI and Machine Learning 2: Introduction to Computer Vision 3: Introduction to MLKit 4: Computer Vision Apps with MLKit on Android 5: Text Processing Apps with MLKit on Android 6: Computer Vision Apps with MLKit on iOS 7: Text Processing Apps with MLKit on iOS 8: Going Deeper: Understanding TensorFlow Lite 9: Creating Custom Models 10: Using Custom Models in Android 11: Using Custom Models in iOS 12: Productizing your app using Firebase 13: CreateML and CoreML for Simple iOS Apps 14: Accessing Cloud-Based Models from Mobile Apps 15: Ethics, Fairness, and Privacy for Mobile Apps</summary></entry><entry><title type="html">Amazing ML work by Japanese children</title><link href="/2021/06/18/inspirational-ai.html" rel="alternate" type="text/html" title="Amazing ML work by Japanese children" /><published>2021-06-18T00:00:00-07:00</published><updated>2021-06-18T00:00:00-07:00</updated><id>/2021/06/18/inspirational-ai</id><content type="html" xml:base="/2021/06/18/inspirational-ai.html">&lt;p&gt;The song “The Greatest Love of all,” performed beautifully by Whitney Houston, begins with the lyrics:&lt;/p&gt;

&lt;p&gt;“I believe the children are our future, teach them well and let them lead the way…”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/japanese.jpg&quot; alt=&quot;Japanese Children&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s a beautiful sentiment, made much more so because it can be true. So I wanted to share the stories of three children in Japan who epitomize these lyrics and show us a potentially bright future made possible by ML.&lt;/p&gt;

&lt;p&gt;Tontoko, a 14-year old girl from Tottori, Mebumebu, an 11 years old boy from Gifu and Ririka, an 11 years old girl from Okinawa, are wonderful examples of this.&lt;/p&gt;

&lt;p&gt;In this video, they share their stories. A fabulous example is Ririka built an app using audio classification to alert drivers to slow down in the vicinity of endangered birds! You’ll also see how Tontoko used computer vision to connect books with music and how Mebumebu created a device to prevent excessive drinking by a beloved grandparent!&lt;/p&gt;

&lt;p&gt;I’m always amazed by the solutions children see to problems in the world around them. So I’m inspired to be a small part of solutions like these and encourage you, dear reader, to do the same!&lt;/p&gt;

&lt;p&gt;“A common misconception about machine learning is that it takes a genius to do it, but I think it’s actually easy and fun!” – Tontoko.&lt;/p&gt;

&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/ztIGjv3YZlE?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Laurence Moroney</name></author><category term="ai" /><summary type="html">The song “The Greatest Love of all,” performed beautifully by Whitney Houston, begins with the lyrics: “I believe the children are our future, teach them well and let them lead the way…” It’s a beautiful sentiment, made much more so because it can be true. So I wanted to share the stories of three children in Japan who epitomize these lyrics and show us a potentially bright future made possible by ML. Tontoko, a 14-year old girl from Tottori, Mebumebu, an 11 years old boy from Gifu and Ririka, an 11 years old girl from Okinawa, are wonderful examples of this. In this video, they share their stories. A fabulous example is Ririka built an app using audio classification to alert drivers to slow down in the vicinity of endangered birds! You’ll also see how Tontoko used computer vision to connect books with music and how Mebumebu created a device to prevent excessive drinking by a beloved grandparent! I’m always amazed by the solutions children see to problems in the world around them. So I’m inspired to be a small part of solutions like these and encourage you, dear reader, to do the same! “A common misconception about machine learning is that it takes a genius to do it, but I think it’s actually easy and fun!” – Tontoko.</summary></entry><entry><title type="html">My Sound to Emoji ML App for WWDC21Challenges</title><link href="/2021/06/09/WWDCChallenge.html" rel="alternate" type="text/html" title="My Sound to Emoji ML App for WWDC21Challenges" /><published>2021-06-09T00:00:00-07:00</published><updated>2021-06-09T00:00:00-07:00</updated><id>/2021/06/09/WWDCChallenge</id><content type="html" xml:base="/2021/06/09/WWDCChallenge.html">&lt;p&gt;Open sourcing my Sound-&amp;gt;Emoji demo app. I created this as part of the app challenges at WWDC21!&lt;/p&gt;

&lt;p&gt;First of all, you need to be an Apple Developer to do this, because it requires the Xcode 13 beta, and iOS 15 in order to work. It does work really well in the emulator!&lt;/p&gt;

&lt;p&gt;As a starting point, I used the code that’s available from &lt;a href=&quot;https://developer.apple.com/documentation/soundanalysis/classifying_live_audio_input_with_a_built-in_sound_classifier&quot;&gt;Apple Developers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This works really well as a starter – giving you an app with two views. The first allows you to pick which sounds you want to classify for. The second then has meters indicating the intensity of the chosen sound.&lt;/p&gt;

&lt;p&gt;So, for example, if I’m breathing – the second view will show something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/breathing.png&quot; alt=&quot;Breathing View&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following code within ContentView helps you pick which view to render. If you are in setup mode, it will use the SetupMonitoredSoundsView, otherwise it will be the DetectSoundsView:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;some&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;View&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;ZStack&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;showSetup&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;kt&quot;&gt;SetupMonitoredSoundsView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                  &lt;span class=&quot;nv&quot;&gt;querySoundOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;AppConfiguration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;listAllValidSoundIdentifiers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                  &lt;span class=&quot;nv&quot;&gt;selectedSounds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;appConfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitoredSounds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;nv&quot;&gt;doneAction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;showSetup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;appState&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;restartDetection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;kt&quot;&gt;DetectSoundsView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;nv&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;appConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;nv&quot;&gt;configureAction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;showSetup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For my emoji viewer, I just edited the original DetectSoundsView to make it much simpler. One suprising thing is that the view redraws &lt;em&gt;on every inference&lt;/em&gt; which seems excessive, but I didn’t change that (for now), which is why the emoji viewing video might look a little flickery.&lt;/p&gt;

&lt;p&gt;I edited DetectSoundsView just to draw a label with the emoji, instead of all the meters from the original with this code:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generateDetectionsGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;detections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;SoundIdentifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;DetectionState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Dictionary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;some&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;View&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ScrollView&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;kt&quot;&gt;ForEach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentConfidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
                    &lt;span class=&quot;kt&quot;&gt;Label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;systemImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;font&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This uses a dictionary to convert the label of the detected sound to an emoji. This is just a simple Dictionary&amp;lt;String, String&amp;gt; object that I create by manually assigning a label to an emoji. My code is &lt;a href=&quot;https://github.com/lmoroney/funcode/blob/master/ios15/classifysound/ClassifySound/Support/EmojiDictionaryHelper.swift&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;…and if you want a video of the app in action, you can see it on &lt;a href=&quot;https://twitter.com/lmoroney/status/1403002757674459142&quot;&gt;my twitter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The full code for the app, borrowing heavily from the apple ample is &lt;a href=&quot;https://github.com/lmoroney/funcode/tree/master/ios15/classifysound&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="ai" /><category term="coding" /><category term="personal" /><summary type="html">Open sourcing my Sound-&amp;gt;Emoji demo app. I created this as part of the app challenges at WWDC21! First of all, you need to be an Apple Developer to do this, because it requires the Xcode 13 beta, and iOS 15 in order to work. It does work really well in the emulator! As a starting point, I used the code that’s available from Apple Developers This works really well as a starter – giving you an app with two views. The first allows you to pick which sounds you want to classify for. The second then has meters indicating the intensity of the chosen sound. So, for example, if I’m breathing – the second view will show something like this: The following code within ContentView helps you pick which view to render. If you are in setup mode, it will use the SetupMonitoredSoundsView, otherwise it will be the DetectSoundsView: var body: some View { ZStack { if showSetup { SetupMonitoredSoundsView( querySoundOptions: { return try AppConfiguration.listAllValidSoundIdentifiers() }, selectedSounds: $appConfig.monitoredSounds, doneAction: { showSetup = false appState.restartDetection(config: appConfig) }) } else { DetectSoundsView(state: appState, config: $appConfig, configureAction: { showSetup = true }) } } } For my emoji viewer, I just edited the original DetectSoundsView to make it much simpler. One suprising thing is that the view redraws on every inference which seems excessive, but I didn’t change that (for now), which is why the emoji viewing video might look a little flickery. I edited DetectSoundsView just to draw a label with the emoji, instead of all the meters from the original with this code: static func generateDetectionsGrid(_ detections: [(SoundIdentifier, DetectionState)], dictionary: Dictionary&amp;lt;String,String&amp;gt;) -&amp;gt; some View { return ScrollView { ForEach(detections, id: \.0.labelName) { if($0.1.currentConfidence&amp;gt;0.3){ Label(dictionary[$0.0.labelName]!, systemImage: &quot;&quot;).font(.system(size:120)) } } } } This uses a dictionary to convert the label of the detected sound to an emoji. This is just a simple Dictionary&amp;lt;String, String&amp;gt; object that I create by manually assigning a label to an emoji. My code is here …and if you want a video of the app in action, you can see it on my twitter The full code for the app, borrowing heavily from the apple ample is here.</summary></entry><entry><title type="html">Widening Access to Applied ML with TinyML</title><link href="/2021/06/08/tinyml.html" rel="alternate" type="text/html" title="Widening Access to Applied ML with TinyML" /><published>2021-06-08T00:00:00-07:00</published><updated>2021-06-08T00:00:00-07:00</updated><id>/2021/06/08/tinyml</id><content type="html" xml:base="/2021/06/08/tinyml.html">&lt;p&gt;I’m delighted to announce that the &lt;a href=&quot;https://arxiv.org/pdf/2106.04008.pdf&quot;&gt;paper&lt;/a&gt;, I’ve worked on with folks from Harvard and Google, has been published online to arxiv! We discuss the strategies that we used in designing a curriculum to widen access to Applied ML, focusing on the rapid growth of TinyML and how a new teaching methodology can open doorways previously shut to the traditionally underrepresented.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/coursedesign.png&quot; alt=&quot;Course Design&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s critically important to do this type of teaching &lt;em&gt;at this time&lt;/em&gt; because with the rapid growth of ML, and TinyML in particular, I anticipate that we’ll soon end up with several prominent players in this space and that these players will likely be in the traditional high tech areas. By opening up easy access to teaching materials for &lt;em&gt;everyone&lt;/em&gt;, we hope to level the playing field.&lt;/p&gt;

&lt;p&gt;With that in mind, we designed the course to have the academic rigor and depth of a top-tier university like Harvard, with the breadth of understanding required to work in a major company like Google. Our goal was linking the two to bring students holistically through a journey of understanding ML, using it, and then deploying it to embedded systems and microcontrollers.&lt;/p&gt;

&lt;p&gt;One observation we made while working on this is an inverse relationship between the number of available ML educational resources and the number of systems in the field! We wanted to fix that!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tinymlnumbers.png&quot; alt=&quot;Picture of numbers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Please check out the &lt;a href=&quot;https://arxiv.org/pdf/2106.04008.pdf&quot;&gt;paper&lt;/a&gt;, or indeed the entire course at &lt;a href=&quot;https://www.edx.org/professional-certificate/harvardx-tiny-machine-learning&quot;&gt;edX&lt;/a&gt; where you can audit it at no cost.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="talks" /><category term="mooc" /><summary type="html">I’m delighted to announce that the paper, I’ve worked on with folks from Harvard and Google, has been published online to arxiv! We discuss the strategies that we used in designing a curriculum to widen access to Applied ML, focusing on the rapid growth of TinyML and how a new teaching methodology can open doorways previously shut to the traditionally underrepresented. It’s critically important to do this type of teaching at this time because with the rapid growth of ML, and TinyML in particular, I anticipate that we’ll soon end up with several prominent players in this space and that these players will likely be in the traditional high tech areas. By opening up easy access to teaching materials for everyone, we hope to level the playing field. With that in mind, we designed the course to have the academic rigor and depth of a top-tier university like Harvard, with the breadth of understanding required to work in a major company like Google. Our goal was linking the two to bring students holistically through a journey of understanding ML, using it, and then deploying it to embedded systems and microcontrollers. One observation we made while working on this is an inverse relationship between the number of available ML educational resources and the number of systems in the field! We wanted to fix that! Please check out the paper, or indeed the entire course at edX where you can audit it at no cost.</summary></entry><entry><title type="html">Impressions of iOS and iPadOS 15 from WWDC</title><link href="/2021/06/07/ios.html" rel="alternate" type="text/html" title="Impressions of iOS and iPadOS 15 from WWDC" /><published>2021-06-07T00:00:00-07:00</published><updated>2021-06-07T00:00:00-07:00</updated><id>/2021/06/07/ios</id><content type="html" xml:base="/2021/06/07/ios.html">&lt;p&gt;Impressions of the WWDC Keynote&lt;/p&gt;

&lt;p&gt;TL;DR A definite change in attitude from Apple. A lot less hubris while delivering a condensed, concise set of features and updates to multiple operating systems while still keeping it clear and relevant for developers. My thoughts on iOS and iPadOS updates:&lt;/p&gt;

&lt;h2 id=&quot;ios-15&quot;&gt;iOS 15:&lt;/h2&gt;

&lt;p&gt;Updates to the social experience came thick and fast, from SharePlay that allows you to watch, listen and work together through a facetime call to spatial audio so that voices will sound like they’re coming from where the person is located on the screen. Not sure about the latter, given that phone screens are small, but let’s wait and see. I &lt;em&gt;love&lt;/em&gt; Spatial Audio when used with things like Airpods Max and the Disney+ App, so I’m hopeful!&lt;/p&gt;

&lt;p&gt;I love the new Mic modes so that you can reduce background noise for meetings etc. Nothing groundbreaking or revolutionary here – it’s stuff you’ve probably experienced before on different apps or platforms. Still, it will be worth exploring with the regular Apple polish and tight integration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/iphone.png&quot; alt=&quot;iOS 15&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are also updates to messenger, where the ability to share things like pictures in messages is greatly improved. Photo collections where automated slideshows integrating music from Apple Music are a nice touch, but again, merely an incremental improvement on what has gone before.&lt;/p&gt;

&lt;p&gt;It also includes a ‘Focus’ ability which integrates down to things like message settings, which is nice. I haven’t figured out how to use it yet, though!&lt;/p&gt;

&lt;p&gt;Maps have a whole host of updates, with beautiful new cartoon-like visualizations.&lt;/p&gt;

&lt;p&gt;The immersive walking instructions appear to have been lifted directly from Google Maps, though! Seeing stuff I used several years ago on my Android, now being portrayed as ‘new,’ is annoying!&lt;/p&gt;

&lt;p&gt;Safari has also had a refresh to make it a little more mobile-friendly with tabs at the bottom navigable with a swipe. Maybe I’m just not used to it, but I did find it clunky.&lt;/p&gt;

&lt;p&gt;Live Text in the camera is pretty cool, but again, nothing groundbreaking that you haven’t seen before. It includes ‘Visual Look Up,’ which is basically Google Lens.&lt;/p&gt;

&lt;p&gt;There’s a tonne of updates for health, but I’ll have to cover them later when I’ve had a chance to play with them.&lt;/p&gt;

&lt;h2 id=&quot;ipados-15&quot;&gt;iPadOS 15&lt;/h2&gt;

&lt;p&gt;I like Multitasking in iPadOS 15. Finally, it doesn’t look like Windows 8! The home screen and widgets have been updated so that widgets do feel like they’re fully integrated instead of shoved to the side as they were with iPadOS 14.x&lt;/p&gt;

&lt;p&gt;I think my very favorite feature is the new ‘Quick Note’, where you can swipe up from the corner and get a little popup note that allows you to make a quick note with your finger or a pencil. Notes are synced across your devices and linked to websites so that you can connect a note with content on the page. (Long ago, I worked on a Windows 8 launch app called ‘strands’ that did this, but it didn’t make it to the Windows 8 launch, so lovely the see the idea re-used!)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ipaddev.png&quot; alt=&quot;iPad Dev&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another excellent update is Swift Playgrounds now on iPad! Yes, you can write code in SwiftUI with code completion and preview….and you can submit your apps straight to the app store! Yes, your iPad is now a developer tool!&lt;/p&gt;

&lt;p&gt;If you’re using macOS Monterey, you can even work across devices – with your mouse/trackpad from your macOS device being usable on your iPad – giving you an authentic second-screen experience! I’m going to play with that one and report back! :)&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="coding" /><summary type="html">Impressions of the WWDC Keynote TL;DR A definite change in attitude from Apple. A lot less hubris while delivering a condensed, concise set of features and updates to multiple operating systems while still keeping it clear and relevant for developers. My thoughts on iOS and iPadOS updates: iOS 15: Updates to the social experience came thick and fast, from SharePlay that allows you to watch, listen and work together through a facetime call to spatial audio so that voices will sound like they’re coming from where the person is located on the screen. Not sure about the latter, given that phone screens are small, but let’s wait and see. I love Spatial Audio when used with things like Airpods Max and the Disney+ App, so I’m hopeful! I love the new Mic modes so that you can reduce background noise for meetings etc. Nothing groundbreaking or revolutionary here – it’s stuff you’ve probably experienced before on different apps or platforms. Still, it will be worth exploring with the regular Apple polish and tight integration. There are also updates to messenger, where the ability to share things like pictures in messages is greatly improved. Photo collections where automated slideshows integrating music from Apple Music are a nice touch, but again, merely an incremental improvement on what has gone before. It also includes a ‘Focus’ ability which integrates down to things like message settings, which is nice. I haven’t figured out how to use it yet, though! Maps have a whole host of updates, with beautiful new cartoon-like visualizations. The immersive walking instructions appear to have been lifted directly from Google Maps, though! Seeing stuff I used several years ago on my Android, now being portrayed as ‘new,’ is annoying! Safari has also had a refresh to make it a little more mobile-friendly with tabs at the bottom navigable with a swipe. Maybe I’m just not used to it, but I did find it clunky. Live Text in the camera is pretty cool, but again, nothing groundbreaking that you haven’t seen before. It includes ‘Visual Look Up,’ which is basically Google Lens. There’s a tonne of updates for health, but I’ll have to cover them later when I’ve had a chance to play with them. iPadOS 15 I like Multitasking in iPadOS 15. Finally, it doesn’t look like Windows 8! The home screen and widgets have been updated so that widgets do feel like they’re fully integrated instead of shoved to the side as they were with iPadOS 14.x I think my very favorite feature is the new ‘Quick Note’, where you can swipe up from the corner and get a little popup note that allows you to make a quick note with your finger or a pencil. Notes are synced across your devices and linked to websites so that you can connect a note with content on the page. (Long ago, I worked on a Windows 8 launch app called ‘strands’ that did this, but it didn’t make it to the Windows 8 launch, so lovely the see the idea re-used!) Another excellent update is Swift Playgrounds now on iPad! Yes, you can write code in SwiftUI with code completion and preview….and you can submit your apps straight to the app store! Yes, your iPad is now a developer tool! If you’re using macOS Monterey, you can even work across devices – with your mouse/trackpad from your macOS device being usable on your iPad – giving you an authentic second-screen experience! I’m going to play with that one and report back! :)</summary></entry><entry><title type="html">Some things I’d like to see at WWDC 2021</title><link href="/2021/06/05/WWDC.html" rel="alternate" type="text/html" title="Some things I’d like to see at WWDC 2021" /><published>2021-06-05T00:00:00-07:00</published><updated>2021-06-05T00:00:00-07:00</updated><id>/2021/06/05/WWDC</id><content type="html" xml:base="/2021/06/05/WWDC.html">&lt;p&gt;So, WWDC starts on Monday, June 7, and I can unashamedly say it’s one of my most anticipated developer events of the year. I enjoy all Apple keynotes (despite sometimes being just a little too self-congratulatory), and the WWDC one is my favorite. It’s best when they focus on features in each of their new versions of iOS, macOS, iPadOS, etc., and at its worst when they use their platform to attack their competitors. So, as well as just focussing on their products, here’s a few things I would love to see at WWDC this year.&lt;/p&gt;

&lt;h2 id=&quot;create-ml-model-interpretability&quot;&gt;Create ML Model Interpretability&lt;/h2&gt;
&lt;p&gt;Create ML is a &lt;em&gt;great&lt;/em&gt; tool, but for some reason, Apple does not give you any indication of the model architecture it creates. Additionally, the various workarounds that &lt;em&gt;used&lt;/em&gt; to be available have now been obfuscated. That’s a big step backward in an era of model interpretability. Let’s be honest here, nobody creating a model with a tool like Create ML is developing a novel architecture – they’re making a model using existing images, and the tool creates the model, so what is there to protect? Worse – what if there are fairness issues with your model? Not having any control over its architecture is just crazy. So, please, Apple Developers, take it in the other direction and let folks see the architecture of the model output by the tool.&lt;/p&gt;

&lt;p&gt;And let’s be honest here – in the case of Image Classification, it’s probably just a new classification head on the bottom of a model like MobileNet or EfficientDet, with the new number of classes. So why obfuscate? Or, best of all, maybe give us developers the opportunity to choose?&lt;/p&gt;

&lt;h4 id=&quot;likelihood-low&quot;&gt;Likelihood: Low&lt;/h4&gt;

&lt;h2 id=&quot;know-your-data---like-tools-in-create-ml&quot;&gt;Know your data - like tools in Create ML&lt;/h2&gt;
&lt;p&gt;On the theme of AI Fairness and ethics, it would be fantastic if a tool like &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/&quot;&gt;Know Your Data&lt;/a&gt; got integrated into Create ML. It’s an immensely useful tool that lets you inspect your data against things that could cause possible biases. For example, please take a look at my &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#tab=STATS&amp;amp;dataset=horses_or_humans&quot;&gt;Horses of Humans&lt;/a&gt; dataset. I created this explicitly to avoid bias by having men and women of multiple sizes, skin tones, etc. Without realizing it, in many cases, the lighting I used made humans of some skin tones have unrecognizable faces, and I would never have known this without KYD!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kyd.png&quot; alt=&quot;KYD Issue&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, if people create models quickly and easily with Create ML, let’s not lead them down the path of having biased models unintentionally! Please integrate KYD, or something like it!&lt;/p&gt;

&lt;h4 id=&quot;likelihood-tiny&quot;&gt;Likelihood: Tiny&lt;/h4&gt;

&lt;h2 id=&quot;export-to-tf-lite&quot;&gt;Export to TF Lite&lt;/h2&gt;
&lt;p&gt;TensorFlow Lite is the premier framework for running ML on &lt;em&gt;all&lt;/em&gt; flavors of mobile. Android, iOS, JavaScript, Linux-based systems, embedded systems, etc. Create ML is a terrific tool. Wouldn’t it be nice for it also to export in .tflite format, so we could use it as a tool to create models for all of the above? I know it’s not the typical approach for Apple, but it sure would be nice!&lt;/p&gt;

&lt;h4 id=&quot;likelihood-near-absolute-zero&quot;&gt;Likelihood: Near absolute Zero&lt;/h4&gt;

&lt;h2 id=&quot;airtags-api&quot;&gt;AirTags API&lt;/h2&gt;
&lt;p&gt;I bought a pack of 4 Air Tags and am waiting for my holders to attach them to things I don’t want to lose. They are undeniably cool, not least because they use the network of iOS devices globally to act as finders. There’s the possibility of applications beyond ‘Find My’ being able to use them. I have a game I want to write to use them instead of beacons, which didn’t live up to the promise. I’m hoping Apple will extend the Nearby API for Airtags at WWDC. Bonus points if their &lt;a href=&quot;https://developer.apple.com/documentation/nearbyinteraction/implementing_interactions_between_users_in_close_proximity&quot;&gt;Nearby Interaction demo&lt;/a&gt;, which in my book goes down as one of the best developer demos ever, is updated for Air Tags.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/NIDemo.png&quot; alt=&quot;NI Demo Image&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;likelihood-quite-possible&quot;&gt;Likelihood: Quite possible&lt;/h4&gt;

&lt;h2 id=&quot;access-to-heart-rate-sensor-in-realtime&quot;&gt;Access to heart rate sensor in realtime&lt;/h2&gt;
&lt;p&gt;Maybe this is already available, but I can’t find it. With the watch having a heart rate sensor, it would be nice to have an API that triggers an alert when the heart rate goes above a certain amount. You might even be able to gamify this. Honest story: I was once playing Catan with my wife, and she was offering a trade that would have won her the game. It seemed a good trade to me. But, her apple watch was visible, and I saw her heart rate was higher than usual, indicating (to me, at least) that she was probably lying! I didn’t take the trade. It didn’t matter; she won on the next turn! But it did get me thinking!&lt;/p&gt;

&lt;h4 id=&quot;likelihood-probably-not&quot;&gt;Likelihood: Probably Not&lt;/h4&gt;</content><author><name>Laurence Moroney</name></author><category term="personal" /><summary type="html">So, WWDC starts on Monday, June 7, and I can unashamedly say it’s one of my most anticipated developer events of the year. I enjoy all Apple keynotes (despite sometimes being just a little too self-congratulatory), and the WWDC one is my favorite. It’s best when they focus on features in each of their new versions of iOS, macOS, iPadOS, etc., and at its worst when they use their platform to attack their competitors. So, as well as just focussing on their products, here’s a few things I would love to see at WWDC this year. Create ML Model Interpretability Create ML is a great tool, but for some reason, Apple does not give you any indication of the model architecture it creates. Additionally, the various workarounds that used to be available have now been obfuscated. That’s a big step backward in an era of model interpretability. Let’s be honest here, nobody creating a model with a tool like Create ML is developing a novel architecture – they’re making a model using existing images, and the tool creates the model, so what is there to protect? Worse – what if there are fairness issues with your model? Not having any control over its architecture is just crazy. So, please, Apple Developers, take it in the other direction and let folks see the architecture of the model output by the tool. And let’s be honest here – in the case of Image Classification, it’s probably just a new classification head on the bottom of a model like MobileNet or EfficientDet, with the new number of classes. So why obfuscate? Or, best of all, maybe give us developers the opportunity to choose? Likelihood: Low Know your data - like tools in Create ML On the theme of AI Fairness and ethics, it would be fantastic if a tool like Know Your Data got integrated into Create ML. It’s an immensely useful tool that lets you inspect your data against things that could cause possible biases. For example, please take a look at my Horses of Humans dataset. I created this explicitly to avoid bias by having men and women of multiple sizes, skin tones, etc. Without realizing it, in many cases, the lighting I used made humans of some skin tones have unrecognizable faces, and I would never have known this without KYD! So, if people create models quickly and easily with Create ML, let’s not lead them down the path of having biased models unintentionally! Please integrate KYD, or something like it! Likelihood: Tiny Export to TF Lite TensorFlow Lite is the premier framework for running ML on all flavors of mobile. Android, iOS, JavaScript, Linux-based systems, embedded systems, etc. Create ML is a terrific tool. Wouldn’t it be nice for it also to export in .tflite format, so we could use it as a tool to create models for all of the above? I know it’s not the typical approach for Apple, but it sure would be nice! Likelihood: Near absolute Zero AirTags API I bought a pack of 4 Air Tags and am waiting for my holders to attach them to things I don’t want to lose. They are undeniably cool, not least because they use the network of iOS devices globally to act as finders. There’s the possibility of applications beyond ‘Find My’ being able to use them. I have a game I want to write to use them instead of beacons, which didn’t live up to the promise. I’m hoping Apple will extend the Nearby API for Airtags at WWDC. Bonus points if their Nearby Interaction demo, which in my book goes down as one of the best developer demos ever, is updated for Air Tags. Likelihood: Quite possible Access to heart rate sensor in realtime Maybe this is already available, but I can’t find it. With the watch having a heart rate sensor, it would be nice to have an API that triggers an alert when the heart rate goes above a certain amount. You might even be able to gamify this. Honest story: I was once playing Catan with my wife, and she was offering a trade that would have won her the game. It seemed a good trade to me. But, her apple watch was visible, and I saw her heart rate was higher than usual, indicating (to me, at least) that she was probably lying! I didn’t take the trade. It didn’t matter; she won on the next turn! But it did get me thinking! Likelihood: Probably Not</summary></entry></feed>