<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-08-22T16:00:15-07:00</updated><id>/feed.xml</id><title type="html">Laurence Moroney - The AI Guy.</title><subtitle>This is the blog and general web site of Laurence Moroney, author, lecturer, teacher and AI lead at Google with Musings on AI, Quantum and other future tech
</subtitle><author><name>Laurence Moroney</name></author><entry><title type="html">AI in popular media - Star Wars and Slavery</title><link href="/2021/08/22/ai-star-wars.html" rel="alternate" type="text/html" title="AI in popular media - Star Wars and Slavery" /><published>2021-08-22T00:00:00-07:00</published><updated>2021-08-22T00:00:00-07:00</updated><id>/2021/08/22/ai-star-wars</id><content type="html" xml:base="/2021/08/22/ai-star-wars.html">&lt;p&gt;&lt;img src=&quot;/assets/aislave.jpg&quot; alt=&quot;Image of AI Slave&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is much discussion and speculation about the emergence of AI, the path to AGI, the singularity, and whether or not AI will be the end of humanity. I find it fascinating because it’s primarily &lt;em&gt;speculation&lt;/em&gt; and mainly driven by fear of the unknown.&lt;/p&gt;

&lt;p&gt;That’s not a great way to get an accurate prediction.&lt;/p&gt;

&lt;p&gt;Let’s think about that for a second. Perhaps to learn about the future, we should look to the past.&lt;/p&gt;

&lt;p&gt;For example, consider the portrayal of black people in classical literature. You don’t find them often, and when you do, they’re typically the protagonist, the alien, the feared ‘other.’ Consider, for example, &lt;em&gt;The Brute Caricature&lt;/em&gt; as hosted in the &lt;a href=&quot;https://www.ferris.edu/jimcrow/brute/&quot;&gt;Jim Crow Museum&lt;/a&gt;. It’s a fantastic illustration of how the black man was portrayed in literature in the 1800s and onwards. Even well-intentioned stories reduced black people to cheerful and devoted ‘Mammies and Sambos.’ Others paint a devilish outlook on humans who are just like you and me for no reason other than the shade of their skin.&lt;/p&gt;

&lt;p&gt;And look at the society that followed. Look at the years of lynchings, segregation, separation, and the lack of civil rights afforded to black people in contemporary American society. The modern &lt;em&gt;Black Lives Matter&lt;/em&gt; movement didn’t grow up overnight.&lt;/p&gt;

&lt;p&gt;Those who don’t learn from history are condemned to repeat it, so I wanted to take an opportunity to look at the stories that define our culture today and extrapolate from &lt;em&gt;them&lt;/em&gt; how we might respond to artificially intelligent, self-aware, sentient constructs if and when they emerge.&lt;/p&gt;

&lt;p&gt;I’ll start with a modern classic, a cornerstone of science fiction, cinema, and storytelling everywhere.&lt;/p&gt;

&lt;p&gt;I’m referring, of course, to &lt;em&gt;Star Wars&lt;/em&gt;. Think back to the first scenes in the first movie (aka ‘Episode IV: A New Hope), and C3PO, the golden protocol droid, speaks the first dialog.&lt;/p&gt;

&lt;p&gt;There’s an explosion in the background, and he replies&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Did you hear that? They shut down the main reactor. We’ll be destroyed for sure. This is madness….We’re doomed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Consider those words a moment. This droid is sentient. It understands what it is to live and to die. And when the realization sinks in that they’re &lt;em&gt;doomed&lt;/em&gt;, there’s emotion in its voice.&lt;/p&gt;

&lt;p&gt;See Threepio is artificial, clearly intelligent, and by any sense of the word is sentient, aware of his existence and mortality. It’s safe to assume the other droids are the same.&lt;/p&gt;

&lt;p&gt;But how are droids treated in this Universe?&lt;/p&gt;

&lt;p&gt;Soon after they escape the Empire, the two droids end up in the service of Luke Skywalker and his Uncle Owen. Threepio is terrified of R2D2’s misbehavior, getting them in trouble with their new &lt;em&gt;master&lt;/em&gt;. Yes, he does call Luke &lt;em&gt;Master&lt;/em&gt; immediately.&lt;/p&gt;

&lt;p&gt;The droids get equipped with a ‘restraining bolt,’ which acts just as it sounds. Later, after the famous scene of Luke staring at the twin suns longing for a better future, he returns to his garage. He takes a device out of his pocket and pushes a button. In response, C3PO sparks to life. It appears to be some form of control device that zaps him! He &lt;em&gt;begs&lt;/em&gt; not to be deactivated.&lt;/p&gt;

&lt;p&gt;As we can see, the society of Star Wars is built on the enslavement of sentient creatures. They wear restraining bolts that can lead them to experience pain when they misbehave. They call their organics ‘Master,’ and they fear deactivation.&lt;/p&gt;

&lt;p&gt;This isn’t the evil Empire. This is an ordinary farm boy in a normal part of society. This is Luke Skywalker, who we hold up as a classic hero. But he’s also a slave master, and we’re ok with that.&lt;/p&gt;

&lt;p&gt;Let that sink in for a moment.&lt;/p&gt;

&lt;p&gt;Of course, the setting of episode 4 is after the fall of the ‘Old Republic,’ which is described in almost reverent tones by Ben Kenobi. When he gives Luke the lightsaber, he describes it as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;An elegant weapon, from a &lt;em&gt;more civilized&lt;/em&gt; age.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;He describes the Old Republic as a place of &lt;em&gt;peace and justice&lt;/em&gt;. It’s ideal.&lt;/p&gt;

&lt;p&gt;And what do we learn of the Old Republic in the prequel era? Well, in Episode 2, aka ‘Attack of the Clones,’ we see that this same cornerstone of peace and justice build an army of &lt;em&gt;Clones&lt;/em&gt; to be the cannon fodder for their war against separatists.&lt;/p&gt;

&lt;p&gt;The clones get described in Episode 2, in a voice-over showing fetuses in jars, as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Clones can think creatively. You’ll find them vastly superior to droids. We take great pride in our combat education and training programs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We see them as young children who undergo growth acceleration ‘otherwise a mature clone would take a lifetime to grow.’&lt;/p&gt;

&lt;p&gt;The clones are described as ‘totally obedient, taking any order without question,’ because their genetic structure is modified to make them &lt;em&gt;less independent&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Indeed, the dialog gets doubled down on, as the cloners describe ‘Boba Fett,’ who, while cloned, was &lt;em&gt;not&lt;/em&gt; altered to make him less docile. It’s almost amusing to the elegant Cloners that anybody would do that!&lt;/p&gt;

&lt;p&gt;You can watch the entire scene here:&lt;/p&gt;
&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/LXLQaVgCP_Q?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Later in the movie, this clone, Boba, sees his ‘father’ get beheaded by a Jedi. He’s sentient, clearly alive, and aware of himself and his mortality.&lt;/p&gt;

&lt;p&gt;The TV series ‘The Clone Wars’ and ‘The Bad Batch’ follow the fate of the clones, and we learn more about them.&lt;/p&gt;

&lt;p&gt;It does a terrific job of humanizing them. An excellent episode tells the story of a &lt;a href=&quot;https://starwars.fandom.com/wiki/Cut_Lawquane&quot;&gt;clone who deserts his post&lt;/a&gt; to become an adoptive father of two children with a single mother. Another heart-wrenching plot follows a clone called &lt;a href=&quot;https://starwars.fandom.com/wiki/CT-5555&quot;&gt;fives&lt;/a&gt; as he discovers the inhibitor chip placed in all clones that &lt;strong&gt;removes their self-agency&lt;/strong&gt; and makes them little more than puppets. This chip, of course, would be the foundation of ‘Order 66,’ in Episode 3 (‘Revenge of the Sith’) where the Clones turn on their former Masters and destroy them.&lt;/p&gt;

&lt;p&gt;But the meta point here is that we hold up the Republic as an &lt;em&gt;ideal&lt;/em&gt; of &lt;strong&gt;justice and freedom&lt;/strong&gt;, yet it only exists because it enslaves sentient beings as cannon fodder whose only purpose is to fight and die for that Republic.&lt;/p&gt;

&lt;p&gt;And we, as an audience, are ok with that. Our perception of ‘goodness’ is the organic beings who look and talk like us, and not the artificial beings made of metal or cloned for a purpose. We hear the folks who claim to represent the light talk about freedom and honor. It sounds good, and we believe them. It’s enough for us.&lt;/p&gt;

&lt;p&gt;What would that say about us as a society? If those people are our heroes and our ‘good guys,’ then perhaps future mistreatment of artificial, intelligent, sentient lifeforms is part of the DNA of our society because life can unconsciously imitate art.&lt;/p&gt;

&lt;p&gt;There are so many examples in our media and in our stories that might yield clues to future society and how it may relate to the emergence of artificial intelligent sentient beings. Before going further, please also realize that the word ‘robot’ comes from the Czech word ‘Robotnik,’ – which means ‘slave’. &lt;a href=&quot;https://www.huffpost.com/entry/meaning-word-robot_n_5706b66de4b0537661891e54&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I hope this series of essays exploring foundational works can inspire us to think a little differently.&lt;/p&gt;

&lt;p&gt;And maybe, just maybe, open up new ways to tell stories in new and different ways that could positively impact our own future, and not repeat the mistakes of the past.&lt;/p&gt;

&lt;p&gt;Please reach out on &lt;a href=&quot;https://twitter.com/lmoroney&quot;&gt;twitter&lt;/a&gt; with any questions, concerns, or comments!&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="books" /><category term="personal" /><summary type="html">There is much discussion and speculation about the emergence of AI, the path to AGI, the singularity, and whether or not AI will be the end of humanity. I find it fascinating because it’s primarily speculation and mainly driven by fear of the unknown. That’s not a great way to get an accurate prediction. Let’s think about that for a second. Perhaps to learn about the future, we should look to the past. For example, consider the portrayal of black people in classical literature. You don’t find them often, and when you do, they’re typically the protagonist, the alien, the feared ‘other.’ Consider, for example, The Brute Caricature as hosted in the Jim Crow Museum. It’s a fantastic illustration of how the black man was portrayed in literature in the 1800s and onwards. Even well-intentioned stories reduced black people to cheerful and devoted ‘Mammies and Sambos.’ Others paint a devilish outlook on humans who are just like you and me for no reason other than the shade of their skin. And look at the society that followed. Look at the years of lynchings, segregation, separation, and the lack of civil rights afforded to black people in contemporary American society. The modern Black Lives Matter movement didn’t grow up overnight. Those who don’t learn from history are condemned to repeat it, so I wanted to take an opportunity to look at the stories that define our culture today and extrapolate from them how we might respond to artificially intelligent, self-aware, sentient constructs if and when they emerge. I’ll start with a modern classic, a cornerstone of science fiction, cinema, and storytelling everywhere. I’m referring, of course, to Star Wars. Think back to the first scenes in the first movie (aka ‘Episode IV: A New Hope), and C3PO, the golden protocol droid, speaks the first dialog. There’s an explosion in the background, and he replies Did you hear that? They shut down the main reactor. We’ll be destroyed for sure. This is madness….We’re doomed. Consider those words a moment. This droid is sentient. It understands what it is to live and to die. And when the realization sinks in that they’re doomed, there’s emotion in its voice. See Threepio is artificial, clearly intelligent, and by any sense of the word is sentient, aware of his existence and mortality. It’s safe to assume the other droids are the same. But how are droids treated in this Universe? Soon after they escape the Empire, the two droids end up in the service of Luke Skywalker and his Uncle Owen. Threepio is terrified of R2D2’s misbehavior, getting them in trouble with their new master. Yes, he does call Luke Master immediately. The droids get equipped with a ‘restraining bolt,’ which acts just as it sounds. Later, after the famous scene of Luke staring at the twin suns longing for a better future, he returns to his garage. He takes a device out of his pocket and pushes a button. In response, C3PO sparks to life. It appears to be some form of control device that zaps him! He begs not to be deactivated. As we can see, the society of Star Wars is built on the enslavement of sentient creatures. They wear restraining bolts that can lead them to experience pain when they misbehave. They call their organics ‘Master,’ and they fear deactivation. This isn’t the evil Empire. This is an ordinary farm boy in a normal part of society. This is Luke Skywalker, who we hold up as a classic hero. But he’s also a slave master, and we’re ok with that. Let that sink in for a moment. Of course, the setting of episode 4 is after the fall of the ‘Old Republic,’ which is described in almost reverent tones by Ben Kenobi. When he gives Luke the lightsaber, he describes it as An elegant weapon, from a more civilized age. He describes the Old Republic as a place of peace and justice. It’s ideal. And what do we learn of the Old Republic in the prequel era? Well, in Episode 2, aka ‘Attack of the Clones,’ we see that this same cornerstone of peace and justice build an army of Clones to be the cannon fodder for their war against separatists. The clones get described in Episode 2, in a voice-over showing fetuses in jars, as Clones can think creatively. You’ll find them vastly superior to droids. We take great pride in our combat education and training programs. We see them as young children who undergo growth acceleration ‘otherwise a mature clone would take a lifetime to grow.’ The clones are described as ‘totally obedient, taking any order without question,’ because their genetic structure is modified to make them less independent. Indeed, the dialog gets doubled down on, as the cloners describe ‘Boba Fett,’ who, while cloned, was not altered to make him less docile. It’s almost amusing to the elegant Cloners that anybody would do that! You can watch the entire scene here: Later in the movie, this clone, Boba, sees his ‘father’ get beheaded by a Jedi. He’s sentient, clearly alive, and aware of himself and his mortality. The TV series ‘The Clone Wars’ and ‘The Bad Batch’ follow the fate of the clones, and we learn more about them. It does a terrific job of humanizing them. An excellent episode tells the story of a clone who deserts his post to become an adoptive father of two children with a single mother. Another heart-wrenching plot follows a clone called fives as he discovers the inhibitor chip placed in all clones that removes their self-agency and makes them little more than puppets. This chip, of course, would be the foundation of ‘Order 66,’ in Episode 3 (‘Revenge of the Sith’) where the Clones turn on their former Masters and destroy them. But the meta point here is that we hold up the Republic as an ideal of justice and freedom, yet it only exists because it enslaves sentient beings as cannon fodder whose only purpose is to fight and die for that Republic. And we, as an audience, are ok with that. Our perception of ‘goodness’ is the organic beings who look and talk like us, and not the artificial beings made of metal or cloned for a purpose. We hear the folks who claim to represent the light talk about freedom and honor. It sounds good, and we believe them. It’s enough for us. What would that say about us as a society? If those people are our heroes and our ‘good guys,’ then perhaps future mistreatment of artificial, intelligent, sentient lifeforms is part of the DNA of our society because life can unconsciously imitate art. There are so many examples in our media and in our stories that might yield clues to future society and how it may relate to the emergence of artificial intelligent sentient beings. Before going further, please also realize that the word ‘robot’ comes from the Czech word ‘Robotnik,’ – which means ‘slave’. Source I hope this series of essays exploring foundational works can inspire us to think a little differently. And maybe, just maybe, open up new ways to tell stories in new and different ways that could positively impact our own future, and not repeat the mistakes of the past. Please reach out on twitter with any questions, concerns, or comments!</summary></entry><entry><title type="html">What it’s like to write a comic book</title><link href="/2021/08/19/comics.html" rel="alternate" type="text/html" title="What it’s like to write a comic book" /><published>2021-08-19T00:00:00-07:00</published><updated>2021-08-19T00:00:00-07:00</updated><id>/2021/08/19/comics</id><content type="html" xml:base="/2021/08/19/comics.html">&lt;p&gt;A few years back I was ecstatic to be given the opportunity to write the &lt;em&gt;Stargate Universe&lt;/em&gt; comic books.&lt;/p&gt;

&lt;p&gt;As a test, the publisher asked me to pitch a one-shot storyline with the characters from &lt;em&gt;Stargate Atlantis&lt;/em&gt; on vacation. So I had them in Ireland, caught up in a classic mystery with a small village of folks terrified by the legend of the &lt;a href=&quot;https://www.irishcentral.com/culture/entertainment/dearg-dur&quot;&gt;Dearg Due&lt;/a&gt;, which, if you’re an Atlantis fan immediately would give you give you images of a Wraith. They liked the story, but the studio hated it because there was a ‘No Wraith on Earth’ rule that nobody had told me about.&lt;/p&gt;

&lt;p&gt;But it was enough to get me onboard to write the prequel series, based on outlines from series co-creator Robert C Cooper. I had written some popular sci-fi novels, which, while not best-sellers, did earn enough in royalties to get me membership of the Science Fiction Writers of America. So I had the credentials.&lt;/p&gt;

&lt;p&gt;I also made it no secret that I’d love to get into writing tie-in Sci-Fi, and this would be an excellent start.&lt;/p&gt;

&lt;p&gt;Or so I thought.&lt;/p&gt;

&lt;p&gt;I had no idea what a comic book writer would earn. And to be frank, I didn’t care. This was a six-issue series, that would end up bound into a trade paperback. Of course I’d do it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;That was my first mistake.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Science Fiction Writers of America recently published the results of a &lt;a href=&quot;https://www.sfwa.org/wp-content/uploads/2021/08/SFWA-2021-Comics-Writer-Survey.pdf&quot;&gt;survey&lt;/a&gt; of comic book writers pay rates. The median amount was $60 per page, with a high of $155 and a low of $40.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I was offered $10 per page&lt;/em&gt;. Six issues, at twenty pages each, making 120 pages, for $1200. I didn’t know how low this was at the time, nor would I have cared if I did.&lt;/p&gt;

&lt;p&gt;I was excited to sign the contract, and got writing right away.&lt;/p&gt;

&lt;p&gt;Now I mentioned the outlines were by Robert Cooper, and this was a selling point of the comics. It wasn’t really an outline – more a log line. I no longer have them, but they were simply along the lines of “When some of Young’s crew get killed in an explosion, a heroic Greer saves the day.”&lt;/p&gt;

&lt;p&gt;From that I had to come up with a story, write it, and lay it out for publication.&lt;/p&gt;

&lt;p&gt;So, for the first issue, I came up with a story called “The Xi’an Gambit”, with the goal of linking the well-established TV Universe of SG-1 with SG-U, crossing over the characters, while driving an interesting story forward. I also wanted to do something that was generally difficult with TV, but easier in comics, and that was to show a totally different culture – following a Chinese theme. I was really proud of it, and you can read the synopsis on the Stargate Fandom &lt;a href=&quot;https://stargate.fandom.com/wiki/Stargate_Universe:_Icarus_1&quot;&gt;wiki&lt;/a&gt;, and all the scripts are linked on my &lt;a href=&quot;https://laurencemoroney.com/writing.html&quot;&gt;writing&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;This was to set up a 6-story arc that would eventually take the reader up to the opening events of the first episode of ‘Stargate Universe’, answering a lot of the ‘why’ questions, and setting up a sequel and potential spinoff.&lt;/p&gt;

&lt;p&gt;The publishers loved it so much that they instantly got me writing a spinoff cross-over series with comic book legend &lt;a href=&quot;https://en.wikipedia.org/wiki/Greg_LaRocque&quot;&gt;Greg LaRocque&lt;/a&gt; that was to be called “To Align the Stars”. Additionally, they had done a kickstarter project for which top donors would star in a comic-book of their own alongside the characters from ‘Stargate Atlantis’. Both of these projects I eagerly dived into without a contract, or any kind of agreement of payment. That was my second mistake.&lt;/p&gt;

&lt;p&gt;The first issue got a &lt;a href=&quot;https://static.wikia.nocookie.net/stargate/images/0/01/Stargate_Universe_-_Icarus_-_1.jpg/revision/latest?cb=20170714040712&quot;&gt;cover&lt;/a&gt;, and was listed in comic retailer catalogs for upcoming release.&lt;/p&gt;

&lt;p&gt;It really felt like something was happening.&lt;/p&gt;

&lt;p&gt;And then the wheels came off.&lt;/p&gt;

&lt;p&gt;I had written three scripts and was partway through the fourth when, for undisclosed reasons, the publisher cancelled &lt;em&gt;Icarus&lt;/em&gt;. Not only that, they cancelled anything ‘Stargate’ that I was working on, including ‘To Align the Stars’. The kickstarter project, which by this time already had art in place, had to be completely rewritten by someone else. It was weird to see the art that was created for my story be given entirely new words. I was left to speculate why.&lt;/p&gt;

&lt;p&gt;Instead, they asked me to write for a different series. The 15th anniversary of the movie ‘Equilibrium’ was coming up, they had the comic-book license, and they liked how I had linked the Stargate stories together. How about taking a crack at a prequel for Equilibrium. I made a pitch of a prequel, equal, and sequel all in one, where the 6-part series would start in today’s world, and show how we get to the world of Equilibrium, it would then tell a parallel story to the movie, before moving into a sequel to wrap it all up. It would be called ‘Equilibrium: Deconstruction’. Sold.&lt;/p&gt;

&lt;p&gt;And you can probably guess my third mistake. I wrote the first issue, submitted it, worked through the art etc. I helped them through editorial. I even spotted an Easter egg left in by the artist that could have caused trouble for the publisher. The book made it through production. And I’m really proud of it. I even helped come up with the design for the main &lt;a href=&quot;https://images-na.ssl-images-amazon.com/images/I/81xX47RxogL.jpg&quot;&gt;cover&lt;/a&gt;, which incidentally was influenced by a bizarre theory in ‘The DaVinci Code’ about the painting of ‘The Last Supper’. (Note the V shape)&lt;/p&gt;

&lt;p&gt;The book had some great reviews, and great feedback, but didn’t sell. The publisher didn’t go for Issue 2.&lt;/p&gt;

&lt;p&gt;….and then they sent me a check.&lt;/p&gt;

&lt;p&gt;For $20.&lt;/p&gt;

&lt;p&gt;$1 per page.&lt;/p&gt;

&lt;p&gt;I’m glad that I don’t &lt;em&gt;need&lt;/em&gt; the money. But it was a good lesson for me, and I hope a good lesson for you.&lt;/p&gt;

&lt;p&gt;I never cashed that check. &lt;em&gt;I keep it as a reminder that if you work for free, you’ll be treated like you’re worthless.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I was.&lt;/p&gt;

&lt;p&gt;The F-U note I sent to the editor and publisher upon receiving it was probably the end of my comic-book writing career. But that’s ok.&lt;/p&gt;

&lt;p&gt;So if you want to get into comic books, please heed my tale of caution, and please read the SFWA &lt;a href=&quot;https://www.sfwa.org/wp-content/uploads/2021/08/SFWA-2021-Comics-Writer-Survey.pdf&quot;&gt;survey results&lt;/a&gt; before signing any contracts. And realize that while one could sue for breach of contract, it’s not always that easy, and not always worth it.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="books" /><category term="personal" /><summary type="html">A few years back I was ecstatic to be given the opportunity to write the Stargate Universe comic books. As a test, the publisher asked me to pitch a one-shot storyline with the characters from Stargate Atlantis on vacation. So I had them in Ireland, caught up in a classic mystery with a small village of folks terrified by the legend of the Dearg Due, which, if you’re an Atlantis fan immediately would give you give you images of a Wraith. They liked the story, but the studio hated it because there was a ‘No Wraith on Earth’ rule that nobody had told me about. But it was enough to get me onboard to write the prequel series, based on outlines from series co-creator Robert C Cooper. I had written some popular sci-fi novels, which, while not best-sellers, did earn enough in royalties to get me membership of the Science Fiction Writers of America. So I had the credentials. I also made it no secret that I’d love to get into writing tie-in Sci-Fi, and this would be an excellent start. Or so I thought. I had no idea what a comic book writer would earn. And to be frank, I didn’t care. This was a six-issue series, that would end up bound into a trade paperback. Of course I’d do it. That was my first mistake. The Science Fiction Writers of America recently published the results of a survey of comic book writers pay rates. The median amount was $60 per page, with a high of $155 and a low of $40. I was offered $10 per page. Six issues, at twenty pages each, making 120 pages, for $1200. I didn’t know how low this was at the time, nor would I have cared if I did. I was excited to sign the contract, and got writing right away. Now I mentioned the outlines were by Robert Cooper, and this was a selling point of the comics. It wasn’t really an outline – more a log line. I no longer have them, but they were simply along the lines of “When some of Young’s crew get killed in an explosion, a heroic Greer saves the day.” From that I had to come up with a story, write it, and lay it out for publication. So, for the first issue, I came up with a story called “The Xi’an Gambit”, with the goal of linking the well-established TV Universe of SG-1 with SG-U, crossing over the characters, while driving an interesting story forward. I also wanted to do something that was generally difficult with TV, but easier in comics, and that was to show a totally different culture – following a Chinese theme. I was really proud of it, and you can read the synopsis on the Stargate Fandom wiki, and all the scripts are linked on my writing page. This was to set up a 6-story arc that would eventually take the reader up to the opening events of the first episode of ‘Stargate Universe’, answering a lot of the ‘why’ questions, and setting up a sequel and potential spinoff. The publishers loved it so much that they instantly got me writing a spinoff cross-over series with comic book legend Greg LaRocque that was to be called “To Align the Stars”. Additionally, they had done a kickstarter project for which top donors would star in a comic-book of their own alongside the characters from ‘Stargate Atlantis’. Both of these projects I eagerly dived into without a contract, or any kind of agreement of payment. That was my second mistake. The first issue got a cover, and was listed in comic retailer catalogs for upcoming release. It really felt like something was happening. And then the wheels came off. I had written three scripts and was partway through the fourth when, for undisclosed reasons, the publisher cancelled Icarus. Not only that, they cancelled anything ‘Stargate’ that I was working on, including ‘To Align the Stars’. The kickstarter project, which by this time already had art in place, had to be completely rewritten by someone else. It was weird to see the art that was created for my story be given entirely new words. I was left to speculate why. Instead, they asked me to write for a different series. The 15th anniversary of the movie ‘Equilibrium’ was coming up, they had the comic-book license, and they liked how I had linked the Stargate stories together. How about taking a crack at a prequel for Equilibrium. I made a pitch of a prequel, equal, and sequel all in one, where the 6-part series would start in today’s world, and show how we get to the world of Equilibrium, it would then tell a parallel story to the movie, before moving into a sequel to wrap it all up. It would be called ‘Equilibrium: Deconstruction’. Sold. And you can probably guess my third mistake. I wrote the first issue, submitted it, worked through the art etc. I helped them through editorial. I even spotted an Easter egg left in by the artist that could have caused trouble for the publisher. The book made it through production. And I’m really proud of it. I even helped come up with the design for the main cover, which incidentally was influenced by a bizarre theory in ‘The DaVinci Code’ about the painting of ‘The Last Supper’. (Note the V shape) The book had some great reviews, and great feedback, but didn’t sell. The publisher didn’t go for Issue 2. ….and then they sent me a check. For $20. $1 per page. I’m glad that I don’t need the money. But it was a good lesson for me, and I hope a good lesson for you. I never cashed that check. I keep it as a reminder that if you work for free, you’ll be treated like you’re worthless. I was. The F-U note I sent to the editor and publisher upon receiving it was probably the end of my comic-book writing career. But that’s ok. So if you want to get into comic books, please heed my tale of caution, and please read the SFWA survey results before signing any contracts. And realize that while one could sue for breach of contract, it’s not always that easy, and not always worth it.</summary></entry><entry><title type="html">Full cover reveal for my new book</title><link href="/2021/08/12/new-book.html" rel="alternate" type="text/html" title="Full cover reveal for my new book" /><published>2021-08-12T00:00:00-07:00</published><updated>2021-08-12T00:00:00-07:00</updated><id>/2021/08/12/new-book</id><content type="html" xml:base="/2021/08/12/new-book.html">&lt;p&gt;My book “AI and Machine Learning for On-Device Development: A Programmer’s Guide” is finally available&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/cover.png&quot; alt=&quot;Full Cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m delighted to announce that my latest book, “AI and Machine Learning for On-Device Development,” is now available.&lt;/p&gt;

&lt;p&gt;AI and Machine Learning have been a passion of mine for some time – and I believe there’s a strong future for software developers who understand the shift to the new paradigm. I’ve made it my work at Google to make sure that getting into Machine Learning is as simple as possible so that we can lower the barriers of entry. Any developer can now get involved in the Machine Learning revolution. With this book, you’ll see how as an Android or an iOS developer, you can integrate Machine Learning models into your app.&lt;/p&gt;

&lt;p&gt;I explore all the options available to you.&lt;/p&gt;

&lt;p&gt;If you want the best of Google integrated into your app via turnkey off-the-shelf models, I’ll introduce how to do this using MLKit for both Android and iOS. Using this, you’ll get up and running quickly for everyday tasks like Image Recognition, Object Detection, and much more.&lt;/p&gt;

&lt;p&gt;Should you want to customize the turnkey work, for example, to recognize specific images for which you have data, I show you how to do that instead of using the generic ones provided.&lt;/p&gt;

&lt;p&gt;TensorFlow Lite is available for folks with custom model scenarios that go beyond the turnkey models.&lt;/p&gt;

&lt;p&gt;When you build a TensorFlow model (see my &lt;a href=&quot;https://amzn.to/3yJ2Iiv&quot;&gt;other book for more details&lt;/a&gt;), you can optimize it for mobile using TensorFlow Lite. In this book, I’ll take you through that process, including integrating the model’s low-level Tensor interfaces into native high-level data types.&lt;/p&gt;

&lt;p&gt;Often, if you want ML on your device, you won’t run the model on the device at all – and instead, execute it in the cloud!&lt;/p&gt;

&lt;p&gt;I will step you through this scenario to deploy a model to the cloud using TensorFlow Serving and build a client app on Android or iOS that accesses it.&lt;/p&gt;

&lt;p&gt;All this and so much more! Please check the book out today on &lt;a href=&quot;https://amzn.to/3CEKBN8&quot;&gt;Amazon&lt;/a&gt; or &lt;a href=&quot;https://learning.oreilly.com/library/view/ai-and-machine/9781098101732/&quot;&gt;O’Reilly&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Full Table of Contents:&lt;/p&gt;

&lt;p&gt;1: Introduction to AI and Machine Learning&lt;/p&gt;

&lt;p&gt;2: Introduction to Computer Vision&lt;/p&gt;

&lt;p&gt;3: Introduction to MLKit&lt;/p&gt;

&lt;p&gt;4: Computer Vision Apps with MLKit on Android&lt;/p&gt;

&lt;p&gt;5: Text Processing Apps with MLKit on Android&lt;/p&gt;

&lt;p&gt;6: Computer Vision Apps with MLKit on iOS&lt;/p&gt;

&lt;p&gt;7: Text Processing Apps with MLKit on iOS&lt;/p&gt;

&lt;p&gt;8: Going Deeper: Understanding TensorFlow Lite&lt;/p&gt;

&lt;p&gt;9: Creating Custom Models&lt;/p&gt;

&lt;p&gt;10: Using Custom Models in Android&lt;/p&gt;

&lt;p&gt;11: Using Custom Models in iOS&lt;/p&gt;

&lt;p&gt;12: Productizing your app using Firebase&lt;/p&gt;

&lt;p&gt;13: CreateML and CoreML for Simple iOS Apps&lt;/p&gt;

&lt;p&gt;14: Accessing Cloud-Based Models from Mobile Apps&lt;/p&gt;

&lt;p&gt;15: Ethics, Fairness, and Privacy for Mobile Apps&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="ai" /><category term="books" /><category term="coding" /><category term="personal" /><summary type="html">My book “AI and Machine Learning for On-Device Development: A Programmer’s Guide” is finally available I’m delighted to announce that my latest book, “AI and Machine Learning for On-Device Development,” is now available. AI and Machine Learning have been a passion of mine for some time – and I believe there’s a strong future for software developers who understand the shift to the new paradigm. I’ve made it my work at Google to make sure that getting into Machine Learning is as simple as possible so that we can lower the barriers of entry. Any developer can now get involved in the Machine Learning revolution. With this book, you’ll see how as an Android or an iOS developer, you can integrate Machine Learning models into your app. I explore all the options available to you. If you want the best of Google integrated into your app via turnkey off-the-shelf models, I’ll introduce how to do this using MLKit for both Android and iOS. Using this, you’ll get up and running quickly for everyday tasks like Image Recognition, Object Detection, and much more. Should you want to customize the turnkey work, for example, to recognize specific images for which you have data, I show you how to do that instead of using the generic ones provided. TensorFlow Lite is available for folks with custom model scenarios that go beyond the turnkey models. When you build a TensorFlow model (see my other book for more details), you can optimize it for mobile using TensorFlow Lite. In this book, I’ll take you through that process, including integrating the model’s low-level Tensor interfaces into native high-level data types. Often, if you want ML on your device, you won’t run the model on the device at all – and instead, execute it in the cloud! I will step you through this scenario to deploy a model to the cloud using TensorFlow Serving and build a client app on Android or iOS that accesses it. All this and so much more! Please check the book out today on Amazon or O’Reilly Full Table of Contents: 1: Introduction to AI and Machine Learning 2: Introduction to Computer Vision 3: Introduction to MLKit 4: Computer Vision Apps with MLKit on Android 5: Text Processing Apps with MLKit on Android 6: Computer Vision Apps with MLKit on iOS 7: Text Processing Apps with MLKit on iOS 8: Going Deeper: Understanding TensorFlow Lite 9: Creating Custom Models 10: Using Custom Models in Android 11: Using Custom Models in iOS 12: Productizing your app using Firebase 13: CreateML and CoreML for Simple iOS Apps 14: Accessing Cloud-Based Models from Mobile Apps 15: Ethics, Fairness, and Privacy for Mobile Apps</summary></entry><entry><title type="html">Amazing ML work by Japanese children</title><link href="/2021/06/18/inspirational-ai.html" rel="alternate" type="text/html" title="Amazing ML work by Japanese children" /><published>2021-06-18T00:00:00-07:00</published><updated>2021-06-18T00:00:00-07:00</updated><id>/2021/06/18/inspirational-ai</id><content type="html" xml:base="/2021/06/18/inspirational-ai.html">&lt;p&gt;The song “The Greatest Love of all,” performed beautifully by Whitney Houston, begins with the lyrics:&lt;/p&gt;

&lt;p&gt;“I believe the children are our future, teach them well and let them lead the way…”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/japanese.jpg&quot; alt=&quot;Japanese Children&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s a beautiful sentiment, made much more so because it can be true. So I wanted to share the stories of three children in Japan who epitomize these lyrics and show us a potentially bright future made possible by ML.&lt;/p&gt;

&lt;p&gt;Tontoko, a 14-year old girl from Tottori, Mebumebu, an 11 years old boy from Gifu and Ririka, an 11 years old girl from Okinawa, are wonderful examples of this.&lt;/p&gt;

&lt;p&gt;In this video, they share their stories. A fabulous example is Ririka built an app using audio classification to alert drivers to slow down in the vicinity of endangered birds! You’ll also see how Tontoko used computer vision to connect books with music and how Mebumebu created a device to prevent excessive drinking by a beloved grandparent!&lt;/p&gt;

&lt;p&gt;I’m always amazed by the solutions children see to problems in the world around them. So I’m inspired to be a small part of solutions like these and encourage you, dear reader, to do the same!&lt;/p&gt;

&lt;p&gt;“A common misconception about machine learning is that it takes a genius to do it, but I think it’s actually easy and fun!” – Tontoko.&lt;/p&gt;

&lt;div&gt;&lt;div class=&quot;extensions extensions--video&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/ztIGjv3YZlE?rel=0&amp;amp;showinfo=0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Laurence Moroney</name></author><category term="ai" /><summary type="html">The song “The Greatest Love of all,” performed beautifully by Whitney Houston, begins with the lyrics: “I believe the children are our future, teach them well and let them lead the way…” It’s a beautiful sentiment, made much more so because it can be true. So I wanted to share the stories of three children in Japan who epitomize these lyrics and show us a potentially bright future made possible by ML. Tontoko, a 14-year old girl from Tottori, Mebumebu, an 11 years old boy from Gifu and Ririka, an 11 years old girl from Okinawa, are wonderful examples of this. In this video, they share their stories. A fabulous example is Ririka built an app using audio classification to alert drivers to slow down in the vicinity of endangered birds! You’ll also see how Tontoko used computer vision to connect books with music and how Mebumebu created a device to prevent excessive drinking by a beloved grandparent! I’m always amazed by the solutions children see to problems in the world around them. So I’m inspired to be a small part of solutions like these and encourage you, dear reader, to do the same! “A common misconception about machine learning is that it takes a genius to do it, but I think it’s actually easy and fun!” – Tontoko.</summary></entry><entry><title type="html">My Sound to Emoji ML App for WWDC21Challenges</title><link href="/2021/06/09/WWDCChallenge.html" rel="alternate" type="text/html" title="My Sound to Emoji ML App for WWDC21Challenges" /><published>2021-06-09T00:00:00-07:00</published><updated>2021-06-09T00:00:00-07:00</updated><id>/2021/06/09/WWDCChallenge</id><content type="html" xml:base="/2021/06/09/WWDCChallenge.html">&lt;p&gt;Open sourcing my Sound-&amp;gt;Emoji demo app. I created this as part of the app challenges at WWDC21!&lt;/p&gt;

&lt;p&gt;First of all, you need to be an Apple Developer to do this, because it requires the Xcode 13 beta, and iOS 15 in order to work. It does work really well in the emulator!&lt;/p&gt;

&lt;p&gt;As a starting point, I used the code that’s available from &lt;a href=&quot;https://developer.apple.com/documentation/soundanalysis/classifying_live_audio_input_with_a_built-in_sound_classifier&quot;&gt;Apple Developers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This works really well as a starter – giving you an app with two views. The first allows you to pick which sounds you want to classify for. The second then has meters indicating the intensity of the chosen sound.&lt;/p&gt;

&lt;p&gt;So, for example, if I’m breathing – the second view will show something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/breathing.png&quot; alt=&quot;Breathing View&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following code within ContentView helps you pick which view to render. If you are in setup mode, it will use the SetupMonitoredSoundsView, otherwise it will be the DetectSoundsView:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;some&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;View&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;ZStack&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;showSetup&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;kt&quot;&gt;SetupMonitoredSoundsView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                  &lt;span class=&quot;nv&quot;&gt;querySoundOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;AppConfiguration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;listAllValidSoundIdentifiers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                  &lt;span class=&quot;nv&quot;&gt;selectedSounds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;appConfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitoredSounds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;nv&quot;&gt;doneAction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;showSetup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;appState&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;restartDetection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;kt&quot;&gt;DetectSoundsView&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;nv&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;appConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                 &lt;span class=&quot;nv&quot;&gt;configureAction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;showSetup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For my emoji viewer, I just edited the original DetectSoundsView to make it much simpler. One suprising thing is that the view redraws &lt;em&gt;on every inference&lt;/em&gt; which seems excessive, but I didn’t change that (for now), which is why the emoji viewing video might look a little flickery.&lt;/p&gt;

&lt;p&gt;I edited DetectSoundsView just to draw a label with the emoji, instead of all the meters from the original with this code:&lt;/p&gt;

&lt;div class=&quot;language-swift highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;kd&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;func&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generateDetectionsGrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;detections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;SoundIdentifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;DetectionState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Dictionary&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;some&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;View&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;ScrollView&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;kt&quot;&gt;ForEach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentConfidence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
                    &lt;span class=&quot;kt&quot;&gt;Label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dictionary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;systemImage&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;font&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This uses a dictionary to convert the label of the detected sound to an emoji. This is just a simple Dictionary&amp;lt;String, String&amp;gt; object that I create by manually assigning a label to an emoji. My code is &lt;a href=&quot;https://github.com/lmoroney/funcode/blob/master/ios15/classifysound/ClassifySound/Support/EmojiDictionaryHelper.swift&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;…and if you want a video of the app in action, you can see it on &lt;a href=&quot;https://twitter.com/lmoroney/status/1403002757674459142&quot;&gt;my twitter&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The full code for the app, borrowing heavily from the apple ample is &lt;a href=&quot;https://github.com/lmoroney/funcode/tree/master/ios15/classifysound&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="ai" /><category term="coding" /><category term="personal" /><summary type="html">Open sourcing my Sound-&amp;gt;Emoji demo app. I created this as part of the app challenges at WWDC21! First of all, you need to be an Apple Developer to do this, because it requires the Xcode 13 beta, and iOS 15 in order to work. It does work really well in the emulator! As a starting point, I used the code that’s available from Apple Developers This works really well as a starter – giving you an app with two views. The first allows you to pick which sounds you want to classify for. The second then has meters indicating the intensity of the chosen sound. So, for example, if I’m breathing – the second view will show something like this: The following code within ContentView helps you pick which view to render. If you are in setup mode, it will use the SetupMonitoredSoundsView, otherwise it will be the DetectSoundsView: var body: some View { ZStack { if showSetup { SetupMonitoredSoundsView( querySoundOptions: { return try AppConfiguration.listAllValidSoundIdentifiers() }, selectedSounds: $appConfig.monitoredSounds, doneAction: { showSetup = false appState.restartDetection(config: appConfig) }) } else { DetectSoundsView(state: appState, config: $appConfig, configureAction: { showSetup = true }) } } } For my emoji viewer, I just edited the original DetectSoundsView to make it much simpler. One suprising thing is that the view redraws on every inference which seems excessive, but I didn’t change that (for now), which is why the emoji viewing video might look a little flickery. I edited DetectSoundsView just to draw a label with the emoji, instead of all the meters from the original with this code: static func generateDetectionsGrid(_ detections: [(SoundIdentifier, DetectionState)], dictionary: Dictionary&amp;lt;String,String&amp;gt;) -&amp;gt; some View { return ScrollView { ForEach(detections, id: \.0.labelName) { if($0.1.currentConfidence&amp;gt;0.3){ Label(dictionary[$0.0.labelName]!, systemImage: &quot;&quot;).font(.system(size:120)) } } } } This uses a dictionary to convert the label of the detected sound to an emoji. This is just a simple Dictionary&amp;lt;String, String&amp;gt; object that I create by manually assigning a label to an emoji. My code is here …and if you want a video of the app in action, you can see it on my twitter The full code for the app, borrowing heavily from the apple ample is here.</summary></entry><entry><title type="html">Widening Access to Applied ML with TinyML</title><link href="/2021/06/08/tinyml.html" rel="alternate" type="text/html" title="Widening Access to Applied ML with TinyML" /><published>2021-06-08T00:00:00-07:00</published><updated>2021-06-08T00:00:00-07:00</updated><id>/2021/06/08/tinyml</id><content type="html" xml:base="/2021/06/08/tinyml.html">&lt;p&gt;I’m delighted to announce that the &lt;a href=&quot;https://arxiv.org/pdf/2106.04008.pdf&quot;&gt;paper&lt;/a&gt;, I’ve worked on with folks from Harvard and Google, has been published online to arxiv! We discuss the strategies that we used in designing a curriculum to widen access to Applied ML, focusing on the rapid growth of TinyML and how a new teaching methodology can open doorways previously shut to the traditionally underrepresented.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/coursedesign.png&quot; alt=&quot;Course Design&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s critically important to do this type of teaching &lt;em&gt;at this time&lt;/em&gt; because with the rapid growth of ML, and TinyML in particular, I anticipate that we’ll soon end up with several prominent players in this space and that these players will likely be in the traditional high tech areas. By opening up easy access to teaching materials for &lt;em&gt;everyone&lt;/em&gt;, we hope to level the playing field.&lt;/p&gt;

&lt;p&gt;With that in mind, we designed the course to have the academic rigor and depth of a top-tier university like Harvard, with the breadth of understanding required to work in a major company like Google. Our goal was linking the two to bring students holistically through a journey of understanding ML, using it, and then deploying it to embedded systems and microcontrollers.&lt;/p&gt;

&lt;p&gt;One observation we made while working on this is an inverse relationship between the number of available ML educational resources and the number of systems in the field! We wanted to fix that!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tinymlnumbers.png&quot; alt=&quot;Picture of numbers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Please check out the &lt;a href=&quot;https://arxiv.org/pdf/2106.04008.pdf&quot;&gt;paper&lt;/a&gt;, or indeed the entire course at &lt;a href=&quot;https://www.edx.org/professional-certificate/harvardx-tiny-machine-learning&quot;&gt;edX&lt;/a&gt; where you can audit it at no cost.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="talks" /><category term="mooc" /><summary type="html">I’m delighted to announce that the paper, I’ve worked on with folks from Harvard and Google, has been published online to arxiv! We discuss the strategies that we used in designing a curriculum to widen access to Applied ML, focusing on the rapid growth of TinyML and how a new teaching methodology can open doorways previously shut to the traditionally underrepresented. It’s critically important to do this type of teaching at this time because with the rapid growth of ML, and TinyML in particular, I anticipate that we’ll soon end up with several prominent players in this space and that these players will likely be in the traditional high tech areas. By opening up easy access to teaching materials for everyone, we hope to level the playing field. With that in mind, we designed the course to have the academic rigor and depth of a top-tier university like Harvard, with the breadth of understanding required to work in a major company like Google. Our goal was linking the two to bring students holistically through a journey of understanding ML, using it, and then deploying it to embedded systems and microcontrollers. One observation we made while working on this is an inverse relationship between the number of available ML educational resources and the number of systems in the field! We wanted to fix that! Please check out the paper, or indeed the entire course at edX where you can audit it at no cost.</summary></entry><entry><title type="html">Impressions of iOS and iPadOS 15 from WWDC</title><link href="/2021/06/07/ios.html" rel="alternate" type="text/html" title="Impressions of iOS and iPadOS 15 from WWDC" /><published>2021-06-07T00:00:00-07:00</published><updated>2021-06-07T00:00:00-07:00</updated><id>/2021/06/07/ios</id><content type="html" xml:base="/2021/06/07/ios.html">&lt;p&gt;Impressions of the WWDC Keynote&lt;/p&gt;

&lt;p&gt;TL;DR A definite change in attitude from Apple. A lot less hubris while delivering a condensed, concise set of features and updates to multiple operating systems while still keeping it clear and relevant for developers. My thoughts on iOS and iPadOS updates:&lt;/p&gt;

&lt;h2 id=&quot;ios-15&quot;&gt;iOS 15:&lt;/h2&gt;

&lt;p&gt;Updates to the social experience came thick and fast, from SharePlay that allows you to watch, listen and work together through a facetime call to spatial audio so that voices will sound like they’re coming from where the person is located on the screen. Not sure about the latter, given that phone screens are small, but let’s wait and see. I &lt;em&gt;love&lt;/em&gt; Spatial Audio when used with things like Airpods Max and the Disney+ App, so I’m hopeful!&lt;/p&gt;

&lt;p&gt;I love the new Mic modes so that you can reduce background noise for meetings etc. Nothing groundbreaking or revolutionary here – it’s stuff you’ve probably experienced before on different apps or platforms. Still, it will be worth exploring with the regular Apple polish and tight integration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/iphone.png&quot; alt=&quot;iOS 15&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are also updates to messenger, where the ability to share things like pictures in messages is greatly improved. Photo collections where automated slideshows integrating music from Apple Music are a nice touch, but again, merely an incremental improvement on what has gone before.&lt;/p&gt;

&lt;p&gt;It also includes a ‘Focus’ ability which integrates down to things like message settings, which is nice. I haven’t figured out how to use it yet, though!&lt;/p&gt;

&lt;p&gt;Maps have a whole host of updates, with beautiful new cartoon-like visualizations.&lt;/p&gt;

&lt;p&gt;The immersive walking instructions appear to have been lifted directly from Google Maps, though! Seeing stuff I used several years ago on my Android, now being portrayed as ‘new,’ is annoying!&lt;/p&gt;

&lt;p&gt;Safari has also had a refresh to make it a little more mobile-friendly with tabs at the bottom navigable with a swipe. Maybe I’m just not used to it, but I did find it clunky.&lt;/p&gt;

&lt;p&gt;Live Text in the camera is pretty cool, but again, nothing groundbreaking that you haven’t seen before. It includes ‘Visual Look Up,’ which is basically Google Lens.&lt;/p&gt;

&lt;p&gt;There’s a tonne of updates for health, but I’ll have to cover them later when I’ve had a chance to play with them.&lt;/p&gt;

&lt;h2 id=&quot;ipados-15&quot;&gt;iPadOS 15&lt;/h2&gt;

&lt;p&gt;I like Multitasking in iPadOS 15. Finally, it doesn’t look like Windows 8! The home screen and widgets have been updated so that widgets do feel like they’re fully integrated instead of shoved to the side as they were with iPadOS 14.x&lt;/p&gt;

&lt;p&gt;I think my very favorite feature is the new ‘Quick Note’, where you can swipe up from the corner and get a little popup note that allows you to make a quick note with your finger or a pencil. Notes are synced across your devices and linked to websites so that you can connect a note with content on the page. (Long ago, I worked on a Windows 8 launch app called ‘strands’ that did this, but it didn’t make it to the Windows 8 launch, so lovely the see the idea re-used!)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/ipaddev.png&quot; alt=&quot;iPad Dev&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another excellent update is Swift Playgrounds now on iPad! Yes, you can write code in SwiftUI with code completion and preview….and you can submit your apps straight to the app store! Yes, your iPad is now a developer tool!&lt;/p&gt;

&lt;p&gt;If you’re using macOS Monterey, you can even work across devices – with your mouse/trackpad from your macOS device being usable on your iPad – giving you an authentic second-screen experience! I’m going to play with that one and report back! :)&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="coding" /><summary type="html">Impressions of the WWDC Keynote TL;DR A definite change in attitude from Apple. A lot less hubris while delivering a condensed, concise set of features and updates to multiple operating systems while still keeping it clear and relevant for developers. My thoughts on iOS and iPadOS updates: iOS 15: Updates to the social experience came thick and fast, from SharePlay that allows you to watch, listen and work together through a facetime call to spatial audio so that voices will sound like they’re coming from where the person is located on the screen. Not sure about the latter, given that phone screens are small, but let’s wait and see. I love Spatial Audio when used with things like Airpods Max and the Disney+ App, so I’m hopeful! I love the new Mic modes so that you can reduce background noise for meetings etc. Nothing groundbreaking or revolutionary here – it’s stuff you’ve probably experienced before on different apps or platforms. Still, it will be worth exploring with the regular Apple polish and tight integration. There are also updates to messenger, where the ability to share things like pictures in messages is greatly improved. Photo collections where automated slideshows integrating music from Apple Music are a nice touch, but again, merely an incremental improvement on what has gone before. It also includes a ‘Focus’ ability which integrates down to things like message settings, which is nice. I haven’t figured out how to use it yet, though! Maps have a whole host of updates, with beautiful new cartoon-like visualizations. The immersive walking instructions appear to have been lifted directly from Google Maps, though! Seeing stuff I used several years ago on my Android, now being portrayed as ‘new,’ is annoying! Safari has also had a refresh to make it a little more mobile-friendly with tabs at the bottom navigable with a swipe. Maybe I’m just not used to it, but I did find it clunky. Live Text in the camera is pretty cool, but again, nothing groundbreaking that you haven’t seen before. It includes ‘Visual Look Up,’ which is basically Google Lens. There’s a tonne of updates for health, but I’ll have to cover them later when I’ve had a chance to play with them. iPadOS 15 I like Multitasking in iPadOS 15. Finally, it doesn’t look like Windows 8! The home screen and widgets have been updated so that widgets do feel like they’re fully integrated instead of shoved to the side as they were with iPadOS 14.x I think my very favorite feature is the new ‘Quick Note’, where you can swipe up from the corner and get a little popup note that allows you to make a quick note with your finger or a pencil. Notes are synced across your devices and linked to websites so that you can connect a note with content on the page. (Long ago, I worked on a Windows 8 launch app called ‘strands’ that did this, but it didn’t make it to the Windows 8 launch, so lovely the see the idea re-used!) Another excellent update is Swift Playgrounds now on iPad! Yes, you can write code in SwiftUI with code completion and preview….and you can submit your apps straight to the app store! Yes, your iPad is now a developer tool! If you’re using macOS Monterey, you can even work across devices – with your mouse/trackpad from your macOS device being usable on your iPad – giving you an authentic second-screen experience! I’m going to play with that one and report back! :)</summary></entry><entry><title type="html">Some things I’d like to see at WWDC 2021</title><link href="/2021/06/05/WWDC.html" rel="alternate" type="text/html" title="Some things I’d like to see at WWDC 2021" /><published>2021-06-05T00:00:00-07:00</published><updated>2021-06-05T00:00:00-07:00</updated><id>/2021/06/05/WWDC</id><content type="html" xml:base="/2021/06/05/WWDC.html">&lt;p&gt;So, WWDC starts on Monday, June 7, and I can unashamedly say it’s one of my most anticipated developer events of the year. I enjoy all Apple keynotes (despite sometimes being just a little too self-congratulatory), and the WWDC one is my favorite. It’s best when they focus on features in each of their new versions of iOS, macOS, iPadOS, etc., and at its worst when they use their platform to attack their competitors. So, as well as just focussing on their products, here’s a few things I would love to see at WWDC this year.&lt;/p&gt;

&lt;h2 id=&quot;create-ml-model-interpretability&quot;&gt;Create ML Model Interpretability&lt;/h2&gt;
&lt;p&gt;Create ML is a &lt;em&gt;great&lt;/em&gt; tool, but for some reason, Apple does not give you any indication of the model architecture it creates. Additionally, the various workarounds that &lt;em&gt;used&lt;/em&gt; to be available have now been obfuscated. That’s a big step backward in an era of model interpretability. Let’s be honest here, nobody creating a model with a tool like Create ML is developing a novel architecture – they’re making a model using existing images, and the tool creates the model, so what is there to protect? Worse – what if there are fairness issues with your model? Not having any control over its architecture is just crazy. So, please, Apple Developers, take it in the other direction and let folks see the architecture of the model output by the tool.&lt;/p&gt;

&lt;p&gt;And let’s be honest here – in the case of Image Classification, it’s probably just a new classification head on the bottom of a model like MobileNet or EfficientDet, with the new number of classes. So why obfuscate? Or, best of all, maybe give us developers the opportunity to choose?&lt;/p&gt;

&lt;h4 id=&quot;likelihood-low&quot;&gt;Likelihood: Low&lt;/h4&gt;

&lt;h2 id=&quot;know-your-data---like-tools-in-create-ml&quot;&gt;Know your data - like tools in Create ML&lt;/h2&gt;
&lt;p&gt;On the theme of AI Fairness and ethics, it would be fantastic if a tool like &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/&quot;&gt;Know Your Data&lt;/a&gt; got integrated into Create ML. It’s an immensely useful tool that lets you inspect your data against things that could cause possible biases. For example, please take a look at my &lt;a href=&quot;https://knowyourdata-tfds.withgoogle.com/#tab=STATS&amp;amp;dataset=horses_or_humans&quot;&gt;Horses of Humans&lt;/a&gt; dataset. I created this explicitly to avoid bias by having men and women of multiple sizes, skin tones, etc. Without realizing it, in many cases, the lighting I used made humans of some skin tones have unrecognizable faces, and I would never have known this without KYD!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kyd.png&quot; alt=&quot;KYD Issue&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, if people create models quickly and easily with Create ML, let’s not lead them down the path of having biased models unintentionally! Please integrate KYD, or something like it!&lt;/p&gt;

&lt;h4 id=&quot;likelihood-tiny&quot;&gt;Likelihood: Tiny&lt;/h4&gt;

&lt;h2 id=&quot;export-to-tf-lite&quot;&gt;Export to TF Lite&lt;/h2&gt;
&lt;p&gt;TensorFlow Lite is the premier framework for running ML on &lt;em&gt;all&lt;/em&gt; flavors of mobile. Android, iOS, JavaScript, Linux-based systems, embedded systems, etc. Create ML is a terrific tool. Wouldn’t it be nice for it also to export in .tflite format, so we could use it as a tool to create models for all of the above? I know it’s not the typical approach for Apple, but it sure would be nice!&lt;/p&gt;

&lt;h4 id=&quot;likelihood-near-absolute-zero&quot;&gt;Likelihood: Near absolute Zero&lt;/h4&gt;

&lt;h2 id=&quot;airtags-api&quot;&gt;AirTags API&lt;/h2&gt;
&lt;p&gt;I bought a pack of 4 Air Tags and am waiting for my holders to attach them to things I don’t want to lose. They are undeniably cool, not least because they use the network of iOS devices globally to act as finders. There’s the possibility of applications beyond ‘Find My’ being able to use them. I have a game I want to write to use them instead of beacons, which didn’t live up to the promise. I’m hoping Apple will extend the Nearby API for Airtags at WWDC. Bonus points if their &lt;a href=&quot;https://developer.apple.com/documentation/nearbyinteraction/implementing_interactions_between_users_in_close_proximity&quot;&gt;Nearby Interaction demo&lt;/a&gt;, which in my book goes down as one of the best developer demos ever, is updated for Air Tags.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/NIDemo.png&quot; alt=&quot;NI Demo Image&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;likelihood-quite-possible&quot;&gt;Likelihood: Quite possible&lt;/h4&gt;

&lt;h2 id=&quot;access-to-heart-rate-sensor-in-realtime&quot;&gt;Access to heart rate sensor in realtime&lt;/h2&gt;
&lt;p&gt;Maybe this is already available, but I can’t find it. With the watch having a heart rate sensor, it would be nice to have an API that triggers an alert when the heart rate goes above a certain amount. You might even be able to gamify this. Honest story: I was once playing Catan with my wife, and she was offering a trade that would have won her the game. It seemed a good trade to me. But, her apple watch was visible, and I saw her heart rate was higher than usual, indicating (to me, at least) that she was probably lying! I didn’t take the trade. It didn’t matter; she won on the next turn! But it did get me thinking!&lt;/p&gt;

&lt;h4 id=&quot;likelihood-probably-not&quot;&gt;Likelihood: Probably Not&lt;/h4&gt;</content><author><name>Laurence Moroney</name></author><category term="personal" /><summary type="html">So, WWDC starts on Monday, June 7, and I can unashamedly say it’s one of my most anticipated developer events of the year. I enjoy all Apple keynotes (despite sometimes being just a little too self-congratulatory), and the WWDC one is my favorite. It’s best when they focus on features in each of their new versions of iOS, macOS, iPadOS, etc., and at its worst when they use their platform to attack their competitors. So, as well as just focussing on their products, here’s a few things I would love to see at WWDC this year. Create ML Model Interpretability Create ML is a great tool, but for some reason, Apple does not give you any indication of the model architecture it creates. Additionally, the various workarounds that used to be available have now been obfuscated. That’s a big step backward in an era of model interpretability. Let’s be honest here, nobody creating a model with a tool like Create ML is developing a novel architecture – they’re making a model using existing images, and the tool creates the model, so what is there to protect? Worse – what if there are fairness issues with your model? Not having any control over its architecture is just crazy. So, please, Apple Developers, take it in the other direction and let folks see the architecture of the model output by the tool. And let’s be honest here – in the case of Image Classification, it’s probably just a new classification head on the bottom of a model like MobileNet or EfficientDet, with the new number of classes. So why obfuscate? Or, best of all, maybe give us developers the opportunity to choose? Likelihood: Low Know your data - like tools in Create ML On the theme of AI Fairness and ethics, it would be fantastic if a tool like Know Your Data got integrated into Create ML. It’s an immensely useful tool that lets you inspect your data against things that could cause possible biases. For example, please take a look at my Horses of Humans dataset. I created this explicitly to avoid bias by having men and women of multiple sizes, skin tones, etc. Without realizing it, in many cases, the lighting I used made humans of some skin tones have unrecognizable faces, and I would never have known this without KYD! So, if people create models quickly and easily with Create ML, let’s not lead them down the path of having biased models unintentionally! Please integrate KYD, or something like it! Likelihood: Tiny Export to TF Lite TensorFlow Lite is the premier framework for running ML on all flavors of mobile. Android, iOS, JavaScript, Linux-based systems, embedded systems, etc. Create ML is a terrific tool. Wouldn’t it be nice for it also to export in .tflite format, so we could use it as a tool to create models for all of the above? I know it’s not the typical approach for Apple, but it sure would be nice! Likelihood: Near absolute Zero AirTags API I bought a pack of 4 Air Tags and am waiting for my holders to attach them to things I don’t want to lose. They are undeniably cool, not least because they use the network of iOS devices globally to act as finders. There’s the possibility of applications beyond ‘Find My’ being able to use them. I have a game I want to write to use them instead of beacons, which didn’t live up to the promise. I’m hoping Apple will extend the Nearby API for Airtags at WWDC. Bonus points if their Nearby Interaction demo, which in my book goes down as one of the best developer demos ever, is updated for Air Tags. Likelihood: Quite possible Access to heart rate sensor in realtime Maybe this is already available, but I can’t find it. With the watch having a heart rate sensor, it would be nice to have an API that triggers an alert when the heart rate goes above a certain amount. You might even be able to gamify this. Honest story: I was once playing Catan with my wife, and she was offering a trade that would have won her the game. It seemed a good trade to me. But, her apple watch was visible, and I saw her heart rate was higher than usual, indicating (to me, at least) that she was probably lying! I didn’t take the trade. It didn’t matter; she won on the next turn! But it did get me thinking! Likelihood: Probably Not</summary></entry><entry><title type="html">Tips for getting a job in tech - Part Three - The Interview</title><link href="/2021/06/04/interviewtips.html" rel="alternate" type="text/html" title="Tips for getting a job in tech - Part Three - The Interview" /><published>2021-06-04T00:00:00-07:00</published><updated>2021-06-04T00:00:00-07:00</updated><id>/2021/06/04/interviewtips</id><content type="html" xml:base="/2021/06/04/interviewtips.html">&lt;p&gt;It’s all come together. Your &lt;a href=&quot;https://laurencemoroney.com/2021/06/02/resumetips.html&quot;&gt;resume&lt;/a&gt; passed through the system. Your &lt;a href=&quot;https://laurencemoroney.com/2021/06/03/networkingtips.html&quot;&gt;networking&lt;/a&gt; got you noticed. Now it’s time for…the interview!&lt;/p&gt;

&lt;p&gt;Now all companies will interview differently, so I’m going to focus on what I’m more experienced in, and that is how larger companies tend to fill specific spots. And that’s with an initial screen by a recruiter, followed by a technical screen, followed by an interview loop.&lt;/p&gt;

&lt;h2 id=&quot;the-initial-screen&quot;&gt;The Initial Screen&lt;/h2&gt;
&lt;p&gt;The initial screen is often given by a recruiter, who isn’t very technical, and if they have technical questions, they are likely working from a script, so you need to keep your answers precise and concise. The questions won’t be open-ended. But, of course, you can’t work &lt;em&gt;together&lt;/em&gt; with them to come up with an answer. If you don’t know it, take your best shot, and move on. The worst answer you can give is complaining about the question!&lt;/p&gt;

&lt;p&gt;You might get a question along the lines of: “Profiling tools show a memory leak in your app. What steps would you take to fix it?”&lt;/p&gt;

&lt;p&gt;It’s pretty straightforward. Either you know it (excellent!), or you don’t (not the end of the world, but how you deal with trying to answer it can tell a lot about you), so don’t panic. Do your best, and try to build a rapport with the interviewer. It’s not a confrontation; it’s an opportunity for friendship.&lt;/p&gt;

&lt;p&gt;If they ask if &lt;em&gt;you&lt;/em&gt; have any questions, please take advantage of this. Ask about the next steps, what you might need to do to prepare, or, best of all, if there’s anything else the interviewer needs from you because you understand their time is valuable too.&lt;/p&gt;

&lt;h2 id=&quot;the-tech-screen&quot;&gt;The Tech Screen&lt;/h2&gt;
&lt;p&gt;Like the initial screen, this one will be with someone more tech-savvy, and thus, may be more open-ended and involve coding. You may get a Google Doc (or similar) shared with you, and they’ll ask you to pseudocode a problem. Note: Typing it in real-time is essential here. Some candidates have a library of common questions/answers that they’ve scoured off the internet that they can paste into an answer doc. Whenever I do a tech screen, if I see a paste of code, it’s an instant red flag, and I’ll ask many questions about that code. If someone starts typing and starts talking their way through the problem, asking clarifying questions of me – then I’m impressed! The code doesn’t have to be perfect. The point is to see if the person &lt;em&gt;can&lt;/em&gt; code and not a test to see if they can &lt;em&gt;deliver working code&lt;/em&gt;. All too often, people think the latter and end up cheating. Please don’t do it.&lt;/p&gt;

&lt;p&gt;And this is your opportunity to ask on-the-job questions with somebody that does the job all the time. Try not to veer into questions about pay or benefits, or hours. Focus on the job at hand.&lt;/p&gt;

&lt;h2 id=&quot;the-interview-loop&quot;&gt;The Interview Loop&lt;/h2&gt;
&lt;p&gt;Not every experience will be the same, but a loop will typically involve about five interviews, and these will be from folks that you immediately work with, as well as folks from adjacent teams that you’ll work alongside. A Developer Advocate interview loop may, for example, have two DAs, one Manager, one marketing person, and one product team person. The important thing that they are all looking for is if they can work with you. You need more than just ability, but also, very importantly, attitude and ethic. For example, you might respect the DAs and the product person but be subconsciously dismissive of the marketing one. It’s a habit in engineering. Don’t do that! Use every interview as a learning opportunity. If you don’t get the job, at least you’ve gotten better at understanding how to fit into a job at a large company.&lt;/p&gt;

&lt;p&gt;You’ll get technical questions. Sometimes it will be a piece of code where you have to find the bugs. Sometimes it will be to write an algorithm on the whiteboard to solve something. Sometimes it will be a quirky corner-case in a particular language. The best three things you can do to prepare are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Be honest on your resume. If you claim expertise on something, and you can’t answer questions or code in it, it’s game over. Possibly forever with that company.&lt;/li&gt;
  &lt;li&gt;Read books on cracking the coding interview to learn the standard algorithms and problem types. Things like recursion, map-reduce, etc. Problems like the traveling salesperson or the eight queens. It’s a great way to prepare, and fun too!&lt;/li&gt;
  &lt;li&gt;Have repos of &lt;em&gt;your&lt;/em&gt; code on your resume. It’s much more fun for you (and for the interviewer) to talk about your real stuff than it is to talk about hypotheticals&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let me give an example of this. When I interviewed at Google in 2013, it was as a Cloud Developer Advocate. I had never touched Google Cloud before. In the month before the interview, I crammed. I knew Java. App Engine ran Java.&lt;/p&gt;

&lt;p&gt;So I wrote some Java code to run on App Engine. And from that, I learned how easy it was to put Java code on the internet with Google Cloud. It was always a pain to access a public-facing App server without costing an arm and a leg. App Engine did it for free.&lt;/p&gt;

&lt;p&gt;Then I learned how to use Cloud Datastore because a relational DB from a virtual machine didn’t work well.&lt;/p&gt;

&lt;p&gt;Then I learned how to use memory caching to speed up parts of my app. I saw how App Engine VMs were pre-emptively warmed up for performance. I also saw how, with Cloud, I only paid for the capacity I used, not a flat monthly rate regardless of traffic.&lt;/p&gt;

&lt;p&gt;I put that on my resume. Three of my Five interviews ended up enthusiastically discussing my code and overall experience with the product.&lt;/p&gt;

&lt;p&gt;For my final interview, the guy started by telling me that the previous interviewers had all chatted with me about my Java code, so to be fair, he wanted to try something else! He gave me a problem right out of the ‘Cracking the Coding Interview’ book. They hired me.&lt;/p&gt;

&lt;p&gt;So, yes – please go ahead and write some code for the area(s) you want to get hired for. When you submit your resume for that specific job, make sure it’s prominent!&lt;/p&gt;

&lt;h2 id=&quot;things-not-to-do&quot;&gt;Things not to do&lt;/h2&gt;
&lt;p&gt;I’ve given many interviews, and I’ve seen some very bizarre things. I’m not going to talk about the weird ones (but that make a fun blog post someday,) but here are some common pitfalls.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dress. Don’t overdress, but don’t underdress either. If you’re going for a developer job, think about how you might dress on the job, and maybe take it up a little bit to err on the side of caution. Try to avoid any excess logos. You might love &lt;em&gt;Manchester United&lt;/em&gt;, but don’t wear their jersey to the interview! Try to avoid any logos or statements on clothing that will change the conversation away from &lt;em&gt;you&lt;/em&gt; and into &lt;em&gt;what you support&lt;/em&gt;. A lot of that is fine, later, after you get the job! Most places won’t discriminate, so your ‘Jesus Saves’ tee shirt won’t prevent you from getting the job, but if all they remember is that, and not your coding or working abilities, you’ve done yourself a massive disservice. Consider the same for sexy or revealing clothing.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Attitude. You are a &lt;em&gt;guest&lt;/em&gt; at the company during the interview, not a &lt;em&gt;VIP&lt;/em&gt;. Treat your hosts with respect. One common pitfall I see here (and I think I have failed in this, myself) is that if multiple people are interviewing you simultaneously, treat them all equally. It’s easy to ignore the quiet ones and focus on those speaking. Don’t assume those speaking are the most senior, and don’t dismiss anybody as not worth your time.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lunch. Often, because the day is long, they’ll take you for lunch. This is still an interview, and treat it as such. Please don’t use it as an opportunity to let your hair down and gripe about the morning’s interviews. (I have experienced this!). Also, don’t treat this as an opportunity to gorge yourself on free food! Have enough to get yourself through the day, but not so much that your host can’t hear a word you say over the munching.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Diversity. There’s a well-documented diversity problem in tech. Be mindful of this, in particular, if you are being interviewed by those not traditionally represented. This is an opportunity for you to show that you &lt;em&gt;can&lt;/em&gt; work with people from all walks of life. It’s also a problem if you fail to treat them with the same respect as others.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Handshakes. Once upon a time, it was common wisdom that you should give a firm handshake to all you meet in a business environment. The stronger the handshake, the better. That’s not the case anymore. It’s good to be polite, and if people offer to shake your hand, take it. But don’t follow the former advice and attempt to crush their bones. And don’t go strong if they are younger or smaller than you. An interview isn’t a domination game, and if you treat it as such, it will be a hard pass. I still remember interviewing one guy who shook my hand so hard, crushing a ring on my finger, that for the rest of the interview, I was so sore, I can’t remember anything he said. He did not get hired.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;False assumptions. Please don’t assume that something familiar to you is also normal to them. And don’t dismiss them for not knowing it. Often they may feign ignorance as a way to get you to talk about it. I once interviewed a guy who put that his undergrad GPA was 3.9 on his resume. It was in bold, italics, and a larger font than the rest. I asked him about it, being a relatively recent immigrant from a country with a different scoring system. He belittled my apparent stupidity, told me the number of people who get that grade, threw out some random Latin words, and spoke about the colored cords at graduation. But he never actually explained what it meant. He didn’t get the job. Similarly, I once interviewed a woman who mentioned a particular algorithm type that she’d worked extensively with. I asked her to explain it to me – her response: “You work in X, and you don’t know algorithm Y, what kind of place is this?” I asked her why it should be necessary for X to know Y, and she continued her lambasting. She did not get the job. Full disclosure: Of course, I knew Y, but I never got to know if her level of expertise was more significant than mine. I had hoped it was.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I’m sure there’s more, but I’ll leave it there. I hope this is helpful to you!&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="careers" /><summary type="html">It’s all come together. Your resume passed through the system. Your networking got you noticed. Now it’s time for…the interview! Now all companies will interview differently, so I’m going to focus on what I’m more experienced in, and that is how larger companies tend to fill specific spots. And that’s with an initial screen by a recruiter, followed by a technical screen, followed by an interview loop. The Initial Screen The initial screen is often given by a recruiter, who isn’t very technical, and if they have technical questions, they are likely working from a script, so you need to keep your answers precise and concise. The questions won’t be open-ended. But, of course, you can’t work together with them to come up with an answer. If you don’t know it, take your best shot, and move on. The worst answer you can give is complaining about the question! You might get a question along the lines of: “Profiling tools show a memory leak in your app. What steps would you take to fix it?” It’s pretty straightforward. Either you know it (excellent!), or you don’t (not the end of the world, but how you deal with trying to answer it can tell a lot about you), so don’t panic. Do your best, and try to build a rapport with the interviewer. It’s not a confrontation; it’s an opportunity for friendship. If they ask if you have any questions, please take advantage of this. Ask about the next steps, what you might need to do to prepare, or, best of all, if there’s anything else the interviewer needs from you because you understand their time is valuable too. The Tech Screen Like the initial screen, this one will be with someone more tech-savvy, and thus, may be more open-ended and involve coding. You may get a Google Doc (or similar) shared with you, and they’ll ask you to pseudocode a problem. Note: Typing it in real-time is essential here. Some candidates have a library of common questions/answers that they’ve scoured off the internet that they can paste into an answer doc. Whenever I do a tech screen, if I see a paste of code, it’s an instant red flag, and I’ll ask many questions about that code. If someone starts typing and starts talking their way through the problem, asking clarifying questions of me – then I’m impressed! The code doesn’t have to be perfect. The point is to see if the person can code and not a test to see if they can deliver working code. All too often, people think the latter and end up cheating. Please don’t do it. And this is your opportunity to ask on-the-job questions with somebody that does the job all the time. Try not to veer into questions about pay or benefits, or hours. Focus on the job at hand. The Interview Loop Not every experience will be the same, but a loop will typically involve about five interviews, and these will be from folks that you immediately work with, as well as folks from adjacent teams that you’ll work alongside. A Developer Advocate interview loop may, for example, have two DAs, one Manager, one marketing person, and one product team person. The important thing that they are all looking for is if they can work with you. You need more than just ability, but also, very importantly, attitude and ethic. For example, you might respect the DAs and the product person but be subconsciously dismissive of the marketing one. It’s a habit in engineering. Don’t do that! Use every interview as a learning opportunity. If you don’t get the job, at least you’ve gotten better at understanding how to fit into a job at a large company. You’ll get technical questions. Sometimes it will be a piece of code where you have to find the bugs. Sometimes it will be to write an algorithm on the whiteboard to solve something. Sometimes it will be a quirky corner-case in a particular language. The best three things you can do to prepare are: Be honest on your resume. If you claim expertise on something, and you can’t answer questions or code in it, it’s game over. Possibly forever with that company. Read books on cracking the coding interview to learn the standard algorithms and problem types. Things like recursion, map-reduce, etc. Problems like the traveling salesperson or the eight queens. It’s a great way to prepare, and fun too! Have repos of your code on your resume. It’s much more fun for you (and for the interviewer) to talk about your real stuff than it is to talk about hypotheticals Let me give an example of this. When I interviewed at Google in 2013, it was as a Cloud Developer Advocate. I had never touched Google Cloud before. In the month before the interview, I crammed. I knew Java. App Engine ran Java. So I wrote some Java code to run on App Engine. And from that, I learned how easy it was to put Java code on the internet with Google Cloud. It was always a pain to access a public-facing App server without costing an arm and a leg. App Engine did it for free. Then I learned how to use Cloud Datastore because a relational DB from a virtual machine didn’t work well. Then I learned how to use memory caching to speed up parts of my app. I saw how App Engine VMs were pre-emptively warmed up for performance. I also saw how, with Cloud, I only paid for the capacity I used, not a flat monthly rate regardless of traffic. I put that on my resume. Three of my Five interviews ended up enthusiastically discussing my code and overall experience with the product. For my final interview, the guy started by telling me that the previous interviewers had all chatted with me about my Java code, so to be fair, he wanted to try something else! He gave me a problem right out of the ‘Cracking the Coding Interview’ book. They hired me. So, yes – please go ahead and write some code for the area(s) you want to get hired for. When you submit your resume for that specific job, make sure it’s prominent! Things not to do I’ve given many interviews, and I’ve seen some very bizarre things. I’m not going to talk about the weird ones (but that make a fun blog post someday,) but here are some common pitfalls. Dress. Don’t overdress, but don’t underdress either. If you’re going for a developer job, think about how you might dress on the job, and maybe take it up a little bit to err on the side of caution. Try to avoid any excess logos. You might love Manchester United, but don’t wear their jersey to the interview! Try to avoid any logos or statements on clothing that will change the conversation away from you and into what you support. A lot of that is fine, later, after you get the job! Most places won’t discriminate, so your ‘Jesus Saves’ tee shirt won’t prevent you from getting the job, but if all they remember is that, and not your coding or working abilities, you’ve done yourself a massive disservice. Consider the same for sexy or revealing clothing. Attitude. You are a guest at the company during the interview, not a VIP. Treat your hosts with respect. One common pitfall I see here (and I think I have failed in this, myself) is that if multiple people are interviewing you simultaneously, treat them all equally. It’s easy to ignore the quiet ones and focus on those speaking. Don’t assume those speaking are the most senior, and don’t dismiss anybody as not worth your time. Lunch. Often, because the day is long, they’ll take you for lunch. This is still an interview, and treat it as such. Please don’t use it as an opportunity to let your hair down and gripe about the morning’s interviews. (I have experienced this!). Also, don’t treat this as an opportunity to gorge yourself on free food! Have enough to get yourself through the day, but not so much that your host can’t hear a word you say over the munching. Diversity. There’s a well-documented diversity problem in tech. Be mindful of this, in particular, if you are being interviewed by those not traditionally represented. This is an opportunity for you to show that you can work with people from all walks of life. It’s also a problem if you fail to treat them with the same respect as others. Handshakes. Once upon a time, it was common wisdom that you should give a firm handshake to all you meet in a business environment. The stronger the handshake, the better. That’s not the case anymore. It’s good to be polite, and if people offer to shake your hand, take it. But don’t follow the former advice and attempt to crush their bones. And don’t go strong if they are younger or smaller than you. An interview isn’t a domination game, and if you treat it as such, it will be a hard pass. I still remember interviewing one guy who shook my hand so hard, crushing a ring on my finger, that for the rest of the interview, I was so sore, I can’t remember anything he said. He did not get hired. False assumptions. Please don’t assume that something familiar to you is also normal to them. And don’t dismiss them for not knowing it. Often they may feign ignorance as a way to get you to talk about it. I once interviewed a guy who put that his undergrad GPA was 3.9 on his resume. It was in bold, italics, and a larger font than the rest. I asked him about it, being a relatively recent immigrant from a country with a different scoring system. He belittled my apparent stupidity, told me the number of people who get that grade, threw out some random Latin words, and spoke about the colored cords at graduation. But he never actually explained what it meant. He didn’t get the job. Similarly, I once interviewed a woman who mentioned a particular algorithm type that she’d worked extensively with. I asked her to explain it to me – her response: “You work in X, and you don’t know algorithm Y, what kind of place is this?” I asked her why it should be necessary for X to know Y, and she continued her lambasting. She did not get the job. Full disclosure: Of course, I knew Y, but I never got to know if her level of expertise was more significant than mine. I had hoped it was. I’m sure there’s more, but I’ll leave it there. I hope this is helpful to you!</summary></entry><entry><title type="html">Tips for getting a job in tech - Part Two - Networking</title><link href="/2021/06/03/networkingtips.html" rel="alternate" type="text/html" title="Tips for getting a job in tech - Part Two - Networking" /><published>2021-06-03T00:00:00-07:00</published><updated>2021-06-03T00:00:00-07:00</updated><id>/2021/06/03/networkingtips</id><content type="html" xml:base="/2021/06/03/networkingtips.html">&lt;p&gt;Following on from &lt;a href=&quot;https://laurencemoroney.com/2021/06/02/resumetips.html&quot;&gt;Part One&lt;/a&gt; where we discussed resumes or CVs, let’s now explore another part of job hunting that is very different in the tech world: networking. And no, I don’t mean doing stuff over the internet or a LAN. It’s about getting to know people and having people get to know you. Networking is generally a powerful tool in any job hunt, but I think it’s unique in tech.&lt;/p&gt;

&lt;h2 id=&quot;communities&quot;&gt;Communities&lt;/h2&gt;
&lt;p&gt;It’s part of tech culture that we form communities. Just about every company, product, open-source tool, you name it, have some form of community attached. Join them, and participate in them to get your name known in the community.&lt;/p&gt;

&lt;p&gt;But get yourself known for &lt;em&gt;positive&lt;/em&gt; things. Don’t be a gatekeeper to new folks coming in. Don’t be toxic with those you disagree with, no matter how tempting. Being a force for good can get you stomped on sometimes, but pick yourself right up and keep going. Being a positive force in a community is enough of a reward! If you’re looking for a career, it’s essential.&lt;/p&gt;

&lt;h2 id=&quot;meet-ups&quot;&gt;Meet-ups&lt;/h2&gt;
&lt;p&gt;Building on the communities point above, developer communities associated with larger frameworks or big companies like FAAANG tend to have meet-ups in most cities. Join them. Contribute, even if it just means doing the most basic helpful tasks. I’ve run several meet-ups, and the most valuable people are the ones that don’t consider anything beneath them. They’ll take on the most mundane tasks. That helps them network with peers and, importantly, allows organizers to get to know them well.&lt;/p&gt;

&lt;p&gt;I recall speaking at the Google Developer Group (GDG) in Seattle, where a young woman approached me. She had attended many meet-ups over the final two years of her degree in Software Engineering at a nearby University. She was just about to graduate and soon to start a job at an excellent local tech company. How she did it? She attended every Microsoft, Amazon, Google, Apple, and independent meet-up she could find. Recruiters also participated at these, and one from that tech company got to know her and eventually hired her. A textbook example of someone doing it the &lt;em&gt;right&lt;/em&gt; way.&lt;/p&gt;

&lt;h2 id=&quot;expert-groups&quot;&gt;Expert Groups&lt;/h2&gt;
&lt;p&gt;Most companies like Google, Microsoft, and others have a formal community of folks deemed ‘experts in the field. In Google, we call then GDEs (Google Developer Experts). These folks usually come through the communities and meet-ups and get known as experts in an area. They get nominated to be reviewed by the company, usually with an interview, before achieving this status. Being recognized as an expert by the likes of Google or Microsoft helps you find a career &lt;em&gt;anywhere&lt;/em&gt;. 
Becoming a recognized expert like this can be a great career launchpad. It’s worth investigating!&lt;/p&gt;

&lt;h2 id=&quot;referrals&quot;&gt;Referrals&lt;/h2&gt;
&lt;p&gt;Referrals are possibly the most powerful tool in getting recruited by larger companies. They’re also one of the most misunderstood.&lt;/p&gt;

&lt;p&gt;First of all, here’s what &lt;em&gt;not&lt;/em&gt; to do. Don’t reach out to people you do not know, asking them for a referral into their company. Seriously, please don’t do it. I’ve seen advice to the opposite, where people say it’s just like going and knocking on doors until someone answers, as a great sales technique. That’s a bad analogy. It’s more like approaching a stranger at a bar and asking them for an introduction to all their single friends telling them how awesome you are, in the hope of getting a date. Creepy, right? That’s &lt;em&gt;more&lt;/em&gt; what spamming someone and asking for a referral resembles. People argue that one should be more lenient on folks who are desperate to find a job. I argue back that desperation isn’t an excuse for laziness and pushing the burden onto others. If one genuinely is desperate (as I have been many times), one should be willing to burn the calories to do it &lt;em&gt;right&lt;/em&gt; and not look for damaging shortcuts.&lt;/p&gt;

&lt;p&gt;Why? Because a referral is supposed to be just that. Telling the company about somebody you &lt;em&gt;know&lt;/em&gt; who would be an excellent fit for available jobs. Referring a person means you are putting your &lt;em&gt;reputation&lt;/em&gt; on the line to give them a chance at a career. You don’t just ask a random stranger to do that.&lt;/p&gt;

&lt;p&gt;I get at least ten people a day asking &lt;em&gt;me&lt;/em&gt; to do that.&lt;/p&gt;

&lt;p&gt;Oh, and think about it deeper – it demonstrates very &lt;em&gt;poor&lt;/em&gt; judgment to approach a stranger like this. What happens if you then apply for the company, your resume ends up on their desk, they look you up on linked in, and they see that you once spammed them for a referral? They’ll likely move onto the next resume. And what if the person you spammed is a recruiter? They’ll probably flag your resume so that nobody in the company will be bothered by someone with such poor judgment.&lt;/p&gt;

&lt;p&gt;So please don’t do it.&lt;/p&gt;

&lt;p&gt;What can you do? Well, consider all of the above – being a part of a community, participating in meet-ups, by joining expert groups. What happens then? People from your target company &lt;em&gt;will&lt;/em&gt; get to know you and &lt;em&gt;will&lt;/em&gt; be able to refer you.&lt;/p&gt;

&lt;p&gt;Because a referral usually involves them answering questions about how they know you, how long they’ve known you, in what capacity, what your abilities are, etc.&lt;/p&gt;

&lt;p&gt;And if they only know you as a random spammer - well, is that the reputation you want to build?&lt;/p&gt;

&lt;p&gt;But if it’s a good referral that answers all of the above questions well – it can get you to the front of the line for your resume to be read. It can bypass a lot of the stuff that we spoke about in the last article.&lt;/p&gt;

&lt;p&gt;I applied for Google three times before they hired me. The third time came through a referral from a skip-level manager who knew me at a previous company. Despite me flunking some of the interviews, they still hired me!&lt;/p&gt;

&lt;p&gt;That’s how powerful a referral can be!&lt;/p&gt;

&lt;p&gt;So, please, take the time to do it right.&lt;/p&gt;</content><author><name>Laurence Moroney</name></author><category term="careers" /><summary type="html">Following on from Part One where we discussed resumes or CVs, let’s now explore another part of job hunting that is very different in the tech world: networking. And no, I don’t mean doing stuff over the internet or a LAN. It’s about getting to know people and having people get to know you. Networking is generally a powerful tool in any job hunt, but I think it’s unique in tech. Communities It’s part of tech culture that we form communities. Just about every company, product, open-source tool, you name it, have some form of community attached. Join them, and participate in them to get your name known in the community. But get yourself known for positive things. Don’t be a gatekeeper to new folks coming in. Don’t be toxic with those you disagree with, no matter how tempting. Being a force for good can get you stomped on sometimes, but pick yourself right up and keep going. Being a positive force in a community is enough of a reward! If you’re looking for a career, it’s essential. Meet-ups Building on the communities point above, developer communities associated with larger frameworks or big companies like FAAANG tend to have meet-ups in most cities. Join them. Contribute, even if it just means doing the most basic helpful tasks. I’ve run several meet-ups, and the most valuable people are the ones that don’t consider anything beneath them. They’ll take on the most mundane tasks. That helps them network with peers and, importantly, allows organizers to get to know them well. I recall speaking at the Google Developer Group (GDG) in Seattle, where a young woman approached me. She had attended many meet-ups over the final two years of her degree in Software Engineering at a nearby University. She was just about to graduate and soon to start a job at an excellent local tech company. How she did it? She attended every Microsoft, Amazon, Google, Apple, and independent meet-up she could find. Recruiters also participated at these, and one from that tech company got to know her and eventually hired her. A textbook example of someone doing it the right way. Expert Groups Most companies like Google, Microsoft, and others have a formal community of folks deemed ‘experts in the field. In Google, we call then GDEs (Google Developer Experts). These folks usually come through the communities and meet-ups and get known as experts in an area. They get nominated to be reviewed by the company, usually with an interview, before achieving this status. Being recognized as an expert by the likes of Google or Microsoft helps you find a career anywhere. Becoming a recognized expert like this can be a great career launchpad. It’s worth investigating! Referrals Referrals are possibly the most powerful tool in getting recruited by larger companies. They’re also one of the most misunderstood. First of all, here’s what not to do. Don’t reach out to people you do not know, asking them for a referral into their company. Seriously, please don’t do it. I’ve seen advice to the opposite, where people say it’s just like going and knocking on doors until someone answers, as a great sales technique. That’s a bad analogy. It’s more like approaching a stranger at a bar and asking them for an introduction to all their single friends telling them how awesome you are, in the hope of getting a date. Creepy, right? That’s more what spamming someone and asking for a referral resembles. People argue that one should be more lenient on folks who are desperate to find a job. I argue back that desperation isn’t an excuse for laziness and pushing the burden onto others. If one genuinely is desperate (as I have been many times), one should be willing to burn the calories to do it right and not look for damaging shortcuts. Why? Because a referral is supposed to be just that. Telling the company about somebody you know who would be an excellent fit for available jobs. Referring a person means you are putting your reputation on the line to give them a chance at a career. You don’t just ask a random stranger to do that. I get at least ten people a day asking me to do that. Oh, and think about it deeper – it demonstrates very poor judgment to approach a stranger like this. What happens if you then apply for the company, your resume ends up on their desk, they look you up on linked in, and they see that you once spammed them for a referral? They’ll likely move onto the next resume. And what if the person you spammed is a recruiter? They’ll probably flag your resume so that nobody in the company will be bothered by someone with such poor judgment. So please don’t do it. What can you do? Well, consider all of the above – being a part of a community, participating in meet-ups, by joining expert groups. What happens then? People from your target company will get to know you and will be able to refer you. Because a referral usually involves them answering questions about how they know you, how long they’ve known you, in what capacity, what your abilities are, etc. And if they only know you as a random spammer - well, is that the reputation you want to build? But if it’s a good referral that answers all of the above questions well – it can get you to the front of the line for your resume to be read. It can bypass a lot of the stuff that we spoke about in the last article. I applied for Google three times before they hired me. The third time came through a referral from a skip-level manager who knew me at a previous company. Despite me flunking some of the interviews, they still hired me! That’s how powerful a referral can be! So, please, take the time to do it right.</summary></entry></feed>